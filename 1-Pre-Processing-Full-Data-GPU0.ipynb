{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook archives the code to process the full data set and save the encoded faces in `/data_processed/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install mmcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mmcv, cv2\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1, extract_face\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython import display\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6.2.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading parameters (1/2)\n",
      "Downloading parameters (2/2)\n"
     ]
    }
   ],
   "source": [
    "# Load face detector\n",
    "mtcnn = MTCNN(margin=14, keep_all=True, post_process=False, thresholds = [0.9, 0.9, 0.9], device=device).eval()\n",
    "\n",
    "# Load facial recognition model, but I didn't want to use it yet\n",
    "resnet = InceptionResnetV1(pretrained='vggface2', device=device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionPipeline:\n",
    "    \"\"\"Pipeline class for detecting faces in the frames of a video file.\"\"\"\n",
    "    \n",
    "    def __init__(self, detector, n_frames=None, batch_size=60, resize=None):\n",
    "        \"\"\"Constructor for DetectionPipeline class.\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            n_frames {int} -- Total number of frames to load. These will be evenly spaced\n",
    "                throughout the video. If not specified (i.e., None), all frames will be loaded.\n",
    "                (default: {None})\n",
    "            batch_size {int} -- Batch size to use with MTCNN face detector. (default: {32})\n",
    "            resize {float} -- Fraction by which to resize frames from original prior to face\n",
    "                detection. A value less than 1 results in downsampling and a value greater than\n",
    "                1 result in upsampling. (default: {None})\n",
    "        \"\"\"\n",
    "        self.detector = detector\n",
    "        self.n_frames = n_frames\n",
    "        self.batch_size = batch_size\n",
    "        self.resize = resize\n",
    "    \n",
    "    def __call__(self, filename):\n",
    "        \"\"\"Load frames from an MP4 video and detect faces.\n",
    "\n",
    "        Arguments:\n",
    "            filename {str} -- Path to video.\n",
    "        \"\"\"\n",
    "        # Create video reader and find length\n",
    "        v_cap = cv2.VideoCapture(filename)\n",
    "        v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Pick 'n_frames' evenly spaced frames to sample\n",
    "        if self.n_frames is None:\n",
    "            sample = np.arange(0, v_len)\n",
    "        else:\n",
    "            sample = np.linspace(0, v_len - 1, self.n_frames).astype(int)\n",
    "\n",
    "        # Loop through frames\n",
    "        faces = []\n",
    "        frames = []\n",
    "        for j in range(v_len):\n",
    "            success = v_cap.grab()\n",
    "            if j in sample:\n",
    "                # Load frame\n",
    "                success, frame = v_cap.retrieve()\n",
    "                if not success:\n",
    "                    continue\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = Image.fromarray(frame)\n",
    "                \n",
    "                # Resize frame to desired size\n",
    "                if self.resize is not None:\n",
    "                    frame = frame.resize([int(d * self.resize) for d in frame.size])\n",
    "                frames.append(frame)\n",
    "\n",
    "                # When batch is full, detect faces and reset frame list\n",
    "                if len(frames) % self.batch_size == 0 or j == sample[-1]:\n",
    "                    faces.extend(self.detector(frames))\n",
    "                    frames = []\n",
    "\n",
    "        v_cap.release()\n",
    "\n",
    "        return faces    \n",
    "\n",
    "\n",
    "def process_faces(faces, resnet):\n",
    "    # Filter out frames without faces\n",
    "    faces = [f for f in faces if f is not None]\n",
    "    faces = torch.cat(faces).to(device)\n",
    "\n",
    "    # Generate facial feature vectors using a pretrained model\n",
    "    embeddings = resnet(faces)\n",
    "\n",
    "    # Calculate centroid for video and distance of each face's feature vector from centroid\n",
    "#     centroid = embeddings.mean(dim=0)\n",
    "#     x = (embeddings - centroid).norm(dim=1).cpu().numpy()\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Videos for speed test\n",
    "    \n",
    "* I see 400 videos took minutes 18 minutes on a pair of P100s. There are 292/400 videos with 1 face only, 16 with 2 faces and none with three in every frame. The same dataset took about 13 minutes on a pair of V100s, given the prices differences I will use P100s to process the entire thing. \n",
    "    \n",
    "* An estimate of ETA is 2000 videos/folder * 50 folders * 18 minute/400videos = 4500 minutes or 75 hours. \n",
    "    \n",
    "* However, I noticed that MTCNN only uses one GPU. Therefore I duplicated this workbook so that we have both GPUs running at the same time. Ideally it should half the time and we are looking at 1 day and half. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/400 [00:02<19:20,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   10.3\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 2/400 [00:05<18:58,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   10.6\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 3/400 [00:08<18:45,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   10.7\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 4/400 [00:10<17:08,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   11.4\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▏         | 5/400 [00:13<17:07,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   11.5\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 6/400 [00:15<17:00,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   11.5\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 7/400 [00:18<18:20,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   11.1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 8/400 [00:21<18:06,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   11.1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 9/400 [00:25<19:16,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   10.8\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▎         | 10/400 [00:27<19:04,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   10.7\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 11/400 [00:29<17:15,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   11.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 12/400 [00:33<18:00,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   10.9\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 13/400 [00:35<17:37,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   10.9\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▎         | 14/400 [00:38<18:39,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   10.8\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 15/400 [00:41<17:57,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   10.8\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 16/400 [00:44<17:53,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   10.8\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 17/400 [00:46<17:36,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   10.9\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 18/400 [00:49<17:43,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   10.8\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▍         | 19/400 [00:52<17:53,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   10.8\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 20/400 [00:55<17:28,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   10.8\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 21/400 [00:58<17:49,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   10.8\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 22/400 [01:01<18:22,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   10.7\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 23/400 [01:04<18:08,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   10.7\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 24/400 [01:07<18:14,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   10.7\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▋         | 25/400 [01:10<18:57,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   10.6\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▋         | 26/400 [01:13<18:05,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   10.7\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 27/400 [01:15<17:24,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   10.7\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 28/400 [01:18<18:14,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   10.6\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 29/400 [01:21<17:17,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   10.7\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 30/400 [01:24<17:12,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   10.7\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 31/400 [01:26<17:07,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second (load+detect+embed):   10.7\n",
      "Stopped.\n"
     ]
    }
   ],
   "source": [
    "# Define face detection pipeline\n",
    "detection_pipeline = DetectionPipeline(detector=mtcnn, batch_size=60, resize=None, n_frames=30)\n",
    "\n",
    "# Get all test videos\n",
    "filenames = glob.glob('data/train_sample_videos/*.mp4')\n",
    "metadata = pd.read_json('data/train_sample_videos/metadata.json').T\n",
    "\n",
    "X1 = []\n",
    "X1_encoded = []\n",
    "Y1 = []\n",
    "X2 = []\n",
    "X2_encoded = []\n",
    "Y2 = []\n",
    "X3 = []\n",
    "X3_encoded = []\n",
    "Y3 = []\n",
    "start = time.time()\n",
    "n_processed = 0\n",
    "with torch.no_grad():\n",
    "    for i, filename in tqdm(enumerate(filenames), total=len(filenames)):\n",
    "        try:\n",
    "            # Load frames and find faces\n",
    "            faces = detection_pipeline(filename)\n",
    "            y = int((metadata.label['data/train_sample_videos/' + metadata.index == filename] == 'REAL') * 1)\n",
    "            # 1 faces ----------\n",
    "            if [x.shape[0] for x in faces if x is not None] == [1] * 30:\n",
    "                # Calculate embeddings\n",
    "                X1.append(faces)\n",
    "                X1_encoded.append(process_faces(faces, resnet))\n",
    "                Y1.append(y)\n",
    "            # 2 faces ----------   \n",
    "            if [x.shape[0] for x in faces if x is not None] == [2] * 30:\n",
    "                # Calculate embeddings\n",
    "                X2.append(faces)\n",
    "                X2_encoded.append(process_faces(faces, resnet))\n",
    "                Y2.append(y)\n",
    "            # 3 faces ----------   \n",
    "            if [x.shape[0] for x in faces if x is not None] == [3] * 30:\n",
    "                # Calculate embeddings\n",
    "                X3.append(faces)\n",
    "                X3_encoded.append(process_faces(faces, resnet))\n",
    "                Y3.append(y)\n",
    "        except KeyboardInterrupt:\n",
    "            print('\\nStopped.')\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "        n_processed += len(faces)\n",
    "        print(f'Frames per second (load+detect+embed): {n_processed / (time.time() - start):6.3}\\r', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This GPU processes odd folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/dfdc_train_part_1/*.mp4 | 1699 files\n"
     ]
    }
   ],
   "source": [
    "# Define face detection pipeline\n",
    "detection_pipeline = DetectionPipeline(detector=mtcnn, batch_size=60, resize=None, n_frames=30)\n",
    "start = time.time()\n",
    "n_processed = 0\n",
    "with torch.no_grad():\n",
    "    for f in tqdm(np.arange(1, 50, 2), total = 25):\n",
    "        # Get all videos\n",
    "        filenames = glob.glob('data/dfdc_train_part_' + str(f) + '/*.mp4')\n",
    "        metadata = pd.read_json('data/dfdc_train_part_' + str(f) + '/metadata.json').T\n",
    "        print('data/dfdc_train_part_' + str(f) + '/*.mp4 | '+ str(len(filenames)) + ' files')\n",
    "        X1 = []\n",
    "        X1_encoded = []\n",
    "        Y1 = []\n",
    "        X2 = []\n",
    "        X2_encoded = []\n",
    "        Y2 = []\n",
    "        X3 = []\n",
    "        X3_encoded = []\n",
    "        Y3 = []\n",
    "        start = time.time()\n",
    "        n_processed = 0\n",
    "        for i, filename in enumerate(filenames):\n",
    "            try:\n",
    "                # Load frames and find faces\n",
    "                faces = detection_pipeline(filename)\n",
    "                y = int((metadata.label[i] == 'REAL') * 1)\n",
    "                # 1 faces ----------\n",
    "                if [x.shape[0] for x in faces if x is not None] == [1] * 30:\n",
    "                    # Calculate embeddings\n",
    "#                     X1.append(faces)\n",
    "                    X1_encoded.append(process_faces(faces, resnet))\n",
    "                    Y1.append(y)\n",
    "                # 2 faces ----------   \n",
    "                if [x.shape[0] for x in faces if x is not None] == [2] * 30:\n",
    "                    # Calculate embeddings\n",
    "#                     X2.append(faces)\n",
    "                    X2_encoded.append(process_faces(faces, resnet))\n",
    "                    Y2.append(y)\n",
    "                # 3 faces ----------   \n",
    "                if [x.shape[0] for x in faces if x is not None] == [3] * 30:\n",
    "                    # Calculate embeddings\n",
    "#                     X3.append(faces)\n",
    "                    X3_encoded.append(process_faces(faces, resnet))\n",
    "                    Y3.append(y)\n",
    "            except KeyboardInterrupt:\n",
    "                print('\\nStopped.')\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        n_processed += len(faces)\n",
    "        print(f'Frames per second (load+detect+embed): {n_processed / (time.time() - start):6.3}\\r', end='')\n",
    "        torch.save(X1_encoded, 'data_processed/1face_X_part' + str(f) + '.pt')\n",
    "        torch.save(Y1, 'data_processed/1face_Y_part' + str(f) + '.pt')\n",
    "        torch.save(X2_encoded, 'data_processed/2face_X_part' + str(f) + '.pt')\n",
    "        torch.save(Y2, 'data_processed/2face_Y_part' + str(f) + '.pt')\n",
    "        torch.save(X3_encoded, 'data_processed/3face_X_part' + str(f) + '.pt')\n",
    "        torch.save(Y3, 'data_processed/3face_Y_part' + str(f) + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
