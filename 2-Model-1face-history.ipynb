{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efeae20f970>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:26<00:00,  1.88s/it]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for p in tqdm(np.arange(0, 50), total = 50):\n",
    "    X_p = torch.load('data_processed/1face_X_part' + str(p) + '.pt', map_location = device)\n",
    "    Y_p = torch.load('data_processed/1face_Y_part' + str(p) + '.pt', map_location = device)\n",
    "    X = X + X_p\n",
    "    Y = Y + Y_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104343"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104343"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.stack(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([104343, 30, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data_tomodel/X.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-89d9b861827d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./data_tomodel/X.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./data_tomodel/Y.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data_tomodel/X.pt'"
     ]
    }
   ],
   "source": [
    "torch.save(X, './data_tomodel/X.pt')\n",
    "torch.save(Y, './data_tomodel/Y.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "dataset = TensorDataset(X[0:104300], torch.from_numpy(np.array(Y[0:104300])))\n",
    "train_data, val_data = torch.utils.data.random_split(dataset, [100000, 4300])\n",
    "train_batch_size = 1000\n",
    "val_batch_size = 100\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=val_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DFDCNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(DFDCNet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.75)\n",
    "        self.batchnorm = nn.BatchNorm1d(hidden_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.fc1 = nn.Linear(hidden_dim, 32)\n",
    "#         self.fc2 = nn.Linear(64, 32)\n",
    "#         self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(32, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.float()\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "#         out = self.batchnorm(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.elu(out)\n",
    "#         out = self.fc2(out)\n",
    "#         out = self.elu(out)\n",
    "#         out = self.fc3(out)\n",
    "#         out = self.elu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFDCNet(\n",
      "  (lstm): LSTM(512, 64, num_layers=5, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.75, inplace=False)\n",
      "  (batchnorm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (elu): ELU(alpha=1.0)\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 512\n",
    "output_size = 1\n",
    "hidden_dim = 128\n",
    "n_layers = 5\n",
    "\n",
    "model = DFDCNet(input_size, output_size, hidden_dim, n_layers)\n",
    "model.to(device)\n",
    "train_criterion = nn.BCELoss()\n",
    "val_criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=500, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/10000... Step: 1000... Loss: 0.531787... Val Loss: 0.431874\n",
      "Validation loss decreased (inf --> 0.431874).  Saving model ...\n",
      "Epoch: 20/10000... Step: 2000... Loss: 0.472329... Val Loss: 0.431393\n",
      "Validation loss decreased (0.431874 --> 0.431393).  Saving model ...\n",
      "Epoch: 30/10000... Step: 3000... Loss: 0.449641... Val Loss: 0.431291\n",
      "Validation loss decreased (0.431393 --> 0.431291).  Saving model ...\n",
      "Epoch: 40/10000... Step: 4000... Loss: 0.437786... Val Loss: 0.430938\n",
      "Validation loss decreased (0.431291 --> 0.430938).  Saving model ...\n",
      "Epoch: 50/10000... Step: 5000... Loss: 0.443349... Val Loss: 0.431201\n",
      "Epoch: 60/10000... Step: 6000... Loss: 0.466028... Val Loss: 0.430974\n",
      "Epoch: 70/10000... Step: 7000... Loss: 0.462511... Val Loss: 0.430990\n",
      "Epoch: 80/10000... Step: 8000... Loss: 0.465358... Val Loss: 0.431100\n",
      "Epoch: 90/10000... Step: 9000... Loss: 0.467207... Val Loss: 0.431350\n",
      "Epoch: 100/10000... Step: 10000... Loss: 0.455970... Val Loss: 0.431349\n",
      "Epoch: 110/10000... Step: 11000... Loss: 0.457632... Val Loss: 0.431261\n",
      "Epoch: 120/10000... Step: 12000... Loss: 0.454191... Val Loss: 0.431336\n",
      "Epoch: 130/10000... Step: 13000... Loss: 0.457278... Val Loss: 0.431589\n",
      "Epoch: 140/10000... Step: 14000... Loss: 0.441668... Val Loss: 0.431142\n",
      "Epoch: 150/10000... Step: 15000... Loss: 0.452922... Val Loss: 0.431018\n",
      "Epoch: 160/10000... Step: 16000... Loss: 0.501180... Val Loss: 0.430780\n",
      "Validation loss decreased (0.430938 --> 0.430780).  Saving model ...\n",
      "Epoch: 170/10000... Step: 17000... Loss: 0.452665... Val Loss: 0.431126\n",
      "Epoch: 180/10000... Step: 18000... Loss: 0.435249... Val Loss: 0.431215\n",
      "Epoch: 190/10000... Step: 19000... Loss: 0.435412... Val Loss: 0.431546\n",
      "Epoch: 200/10000... Step: 20000... Loss: 0.455950... Val Loss: 0.431593\n",
      "Epoch: 210/10000... Step: 21000... Loss: 0.490311... Val Loss: 0.430924\n",
      "Epoch: 220/10000... Step: 22000... Loss: 0.465992... Val Loss: 0.431115\n",
      "Epoch: 230/10000... Step: 23000... Loss: 0.436535... Val Loss: 0.430913\n",
      "Epoch: 240/10000... Step: 24000... Loss: 0.492195... Val Loss: 0.431761\n",
      "Epoch: 250/10000... Step: 25000... Loss: 0.426555... Val Loss: 0.425136\n",
      "Validation loss decreased (0.430780 --> 0.425136).  Saving model ...\n",
      "Epoch: 260/10000... Step: 26000... Loss: 0.412694... Val Loss: 0.407532\n",
      "Validation loss decreased (0.425136 --> 0.407532).  Saving model ...\n",
      "Epoch: 270/10000... Step: 27000... Loss: 0.394946... Val Loss: 0.401915\n",
      "Validation loss decreased (0.407532 --> 0.401915).  Saving model ...\n",
      "Epoch: 280/10000... Step: 28000... Loss: 0.395257... Val Loss: 0.391669\n",
      "Validation loss decreased (0.401915 --> 0.391669).  Saving model ...\n",
      "Epoch: 290/10000... Step: 29000... Loss: 0.387796... Val Loss: 0.381405\n",
      "Validation loss decreased (0.391669 --> 0.381405).  Saving model ...\n",
      "Epoch: 300/10000... Step: 30000... Loss: 0.394108... Val Loss: 0.379386\n",
      "Validation loss decreased (0.381405 --> 0.379386).  Saving model ...\n",
      "Epoch: 310/10000... Step: 31000... Loss: 0.394702... Val Loss: 0.369911\n",
      "Validation loss decreased (0.379386 --> 0.369911).  Saving model ...\n",
      "Epoch: 320/10000... Step: 32000... Loss: 0.366856... Val Loss: 0.364915\n",
      "Validation loss decreased (0.369911 --> 0.364915).  Saving model ...\n",
      "Epoch: 330/10000... Step: 33000... Loss: 0.368662... Val Loss: 0.363449\n",
      "Validation loss decreased (0.364915 --> 0.363449).  Saving model ...\n",
      "Epoch: 340/10000... Step: 34000... Loss: 0.348882... Val Loss: 0.356082\n",
      "Validation loss decreased (0.363449 --> 0.356082).  Saving model ...\n",
      "Epoch: 350/10000... Step: 35000... Loss: 0.353864... Val Loss: 0.353985\n",
      "Validation loss decreased (0.356082 --> 0.353985).  Saving model ...\n",
      "Epoch: 360/10000... Step: 36000... Loss: 0.331982... Val Loss: 0.354707\n",
      "Epoch: 370/10000... Step: 37000... Loss: 0.307452... Val Loss: 0.340589\n",
      "Validation loss decreased (0.353985 --> 0.340589).  Saving model ...\n",
      "Epoch: 380/10000... Step: 38000... Loss: 0.306288... Val Loss: 0.336129\n",
      "Validation loss decreased (0.340589 --> 0.336129).  Saving model ...\n",
      "Epoch: 390/10000... Step: 39000... Loss: 0.318658... Val Loss: 0.338284\n",
      "Epoch: 400/10000... Step: 40000... Loss: 0.313316... Val Loss: 0.334955\n",
      "Validation loss decreased (0.336129 --> 0.334955).  Saving model ...\n",
      "Epoch: 410/10000... Step: 41000... Loss: 0.335133... Val Loss: 0.334931\n",
      "Validation loss decreased (0.334955 --> 0.334931).  Saving model ...\n",
      "Epoch: 420/10000... Step: 42000... Loss: 0.270853... Val Loss: 0.332716\n",
      "Validation loss decreased (0.334931 --> 0.332716).  Saving model ...\n",
      "Epoch: 430/10000... Step: 43000... Loss: 0.284182... Val Loss: 0.333650\n",
      "Epoch: 440/10000... Step: 44000... Loss: 0.287038... Val Loss: 0.338938\n",
      "Epoch: 450/10000... Step: 45000... Loss: 0.270831... Val Loss: 0.333599\n",
      "Epoch: 460/10000... Step: 46000... Loss: 0.262602... Val Loss: 0.338987\n",
      "Epoch: 470/10000... Step: 47000... Loss: 0.293131... Val Loss: 0.331634\n",
      "Validation loss decreased (0.332716 --> 0.331634).  Saving model ...\n",
      "Epoch: 480/10000... Step: 48000... Loss: 0.301701... Val Loss: 0.324947\n",
      "Validation loss decreased (0.331634 --> 0.324947).  Saving model ...\n",
      "Epoch: 490/10000... Step: 49000... Loss: 0.252277... Val Loss: 0.332261\n",
      "Epoch: 500/10000... Step: 50000... Loss: 0.272591... Val Loss: 0.322272\n",
      "Validation loss decreased (0.324947 --> 0.322272).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "counter = 0\n",
    "print_every = 1000\n",
    "clip = .5\n",
    "valid_loss_min = np.Inf\n",
    "val_loss = torch.tensor(np.Inf)\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(train_batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = train_criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            val_h = model.init_hidden(val_batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in val_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp, val_h)\n",
    "                val_loss = val_criterion(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './model_1face.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)\n",
    "    scheduler.step(val_loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "* https://github.com/gabrielloye/LSTM_Sentiment-Analysis/blob/master/main.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doubling hidden dim to 128, 2nd fc with (64, 32) all activated with ReLU\n",
    "0.85 dropout\n",
    "0.003 LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DFDCNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(DFDCNet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.85)\n",
    "        self.batchnorm = nn.BatchNorm1d(hidden_dim)\n",
    "        self.elu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "#         self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(32, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.float()\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "#         out = self.batchnorm(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.elu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.elu(out)\n",
    "#         out = self.fc3(out)\n",
    "#         out = self.elu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFDCNet(\n",
      "  (lstm): LSTM(512, 128, num_layers=5, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.85, inplace=False)\n",
      "  (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (elu): ReLU()\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 512\n",
    "output_size = 1\n",
    "hidden_dim = 128\n",
    "n_layers = 5\n",
    "\n",
    "model = DFDCNet(input_size, output_size, hidden_dim, n_layers)\n",
    "model.to(device)\n",
    "train_criterion = nn.BCELoss()\n",
    "val_criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=500, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/10000... Step: 1000... Loss: 0.428404... Val Loss: 0.447665\n",
      "Validation loss decreased (inf --> 0.447665).  Saving model ...\n",
      "Epoch: 20/10000... Step: 2000... Loss: 0.440405... Val Loss: 0.447356\n",
      "Validation loss decreased (0.447665 --> 0.447356).  Saving model ...\n",
      "Epoch: 30/10000... Step: 3000... Loss: 0.431659... Val Loss: 0.447322\n",
      "Validation loss decreased (0.447356 --> 0.447322).  Saving model ...\n",
      "Epoch: 40/10000... Step: 4000... Loss: 0.426973... Val Loss: 0.447308\n",
      "Validation loss decreased (0.447322 --> 0.447308).  Saving model ...\n",
      "Epoch: 50/10000... Step: 5000... Loss: 0.457830... Val Loss: 0.447308\n",
      "Epoch: 60/10000... Step: 6000... Loss: 0.432147... Val Loss: 0.447435\n",
      "Epoch: 70/10000... Step: 7000... Loss: 0.453326... Val Loss: 0.447384\n",
      "Epoch: 80/10000... Step: 8000... Loss: 0.455806... Val Loss: 0.447306\n",
      "Validation loss decreased (0.447308 --> 0.447306).  Saving model ...\n",
      "Epoch: 90/10000... Step: 9000... Loss: 0.459213... Val Loss: 0.447306\n",
      "Epoch: 100/10000... Step: 10000... Loss: 0.442712... Val Loss: 0.447411\n",
      "Epoch: 110/10000... Step: 11000... Loss: 0.480807... Val Loss: 0.447344\n",
      "Epoch: 120/10000... Step: 12000... Loss: 0.446500... Val Loss: 0.447372\n",
      "Epoch: 130/10000... Step: 13000... Loss: 0.427279... Val Loss: 0.447347\n",
      "Epoch: 140/10000... Step: 14000... Loss: 0.433445... Val Loss: 0.447303\n",
      "Validation loss decreased (0.447306 --> 0.447303).  Saving model ...\n",
      "Epoch: 150/10000... Step: 15000... Loss: 0.482884... Val Loss: 0.447451\n",
      "Epoch: 160/10000... Step: 16000... Loss: 0.446161... Val Loss: 0.447360\n",
      "Epoch: 170/10000... Step: 17000... Loss: 0.441551... Val Loss: 0.447304\n",
      "Epoch: 180/10000... Step: 18000... Loss: 0.466913... Val Loss: 0.447486\n",
      "Epoch: 190/10000... Step: 19000... Loss: 0.423807... Val Loss: 0.447315\n",
      "Epoch: 200/10000... Step: 20000... Loss: 0.444834... Val Loss: 0.447469\n",
      "Epoch: 210/10000... Step: 21000... Loss: 0.455885... Val Loss: 0.447324\n",
      "Epoch: 220/10000... Step: 22000... Loss: 0.446396... Val Loss: 0.447322\n",
      "Epoch: 230/10000... Step: 23000... Loss: 0.446227... Val Loss: 0.447303\n",
      "Epoch: 240/10000... Step: 24000... Loss: 0.448028... Val Loss: 0.447364\n",
      "Epoch: 250/10000... Step: 25000... Loss: 0.434614... Val Loss: 0.447349\n",
      "Epoch: 260/10000... Step: 26000... Loss: 0.452758... Val Loss: 0.447307\n",
      "Epoch: 270/10000... Step: 27000... Loss: 0.491460... Val Loss: 0.447305\n",
      "Epoch: 280/10000... Step: 28000... Loss: 0.482044... Val Loss: 0.447305\n",
      "Epoch: 290/10000... Step: 29000... Loss: 0.465867... Val Loss: 0.447331\n",
      "Epoch: 300/10000... Step: 30000... Loss: 0.424105... Val Loss: 0.447389\n",
      "Epoch: 310/10000... Step: 31000... Loss: 0.457622... Val Loss: 0.447304\n",
      "Epoch: 320/10000... Step: 32000... Loss: 0.429773... Val Loss: 0.447325\n",
      "Epoch: 330/10000... Step: 33000... Loss: 0.454348... Val Loss: 0.447336\n",
      "Epoch: 340/10000... Step: 34000... Loss: 0.468364... Val Loss: 0.447648\n",
      "Epoch: 350/10000... Step: 35000... Loss: 0.433094... Val Loss: 0.447367\n",
      "Epoch: 360/10000... Step: 36000... Loss: 0.473508... Val Loss: 0.447418\n",
      "Epoch: 370/10000... Step: 37000... Loss: 0.428705... Val Loss: 0.447329\n",
      "Epoch: 380/10000... Step: 38000... Loss: 0.479626... Val Loss: 0.447493\n",
      "Epoch: 390/10000... Step: 39000... Loss: 0.449730... Val Loss: 0.447303\n",
      "Epoch: 400/10000... Step: 40000... Loss: 0.429397... Val Loss: 0.447354\n",
      "Epoch: 410/10000... Step: 41000... Loss: 0.408146... Val Loss: 0.447391\n",
      "Epoch: 420/10000... Step: 42000... Loss: 0.443001... Val Loss: 0.447303\n",
      "Validation loss decreased (0.447303 --> 0.447303).  Saving model ...\n",
      "Epoch: 430/10000... Step: 43000... Loss: 0.439725... Val Loss: 0.447313\n",
      "Epoch: 440/10000... Step: 44000... Loss: 0.459364... Val Loss: 0.447332\n",
      "Epoch: 450/10000... Step: 45000... Loss: 0.411091... Val Loss: 0.447325\n",
      "Epoch: 460/10000... Step: 46000... Loss: 0.459619... Val Loss: 0.447351\n",
      "Epoch: 470/10000... Step: 47000... Loss: 0.452600... Val Loss: 0.447305\n",
      "Epoch: 480/10000... Step: 48000... Loss: 0.450865... Val Loss: 0.447316\n",
      "Epoch: 490/10000... Step: 49000... Loss: 0.438444... Val Loss: 0.447392\n",
      "Epoch: 500/10000... Step: 50000... Loss: 0.426591... Val Loss: 0.447308\n",
      "Epoch: 510/10000... Step: 51000... Loss: 0.462283... Val Loss: 0.447364\n",
      "Epoch: 520/10000... Step: 52000... Loss: 0.433414... Val Loss: 0.447319\n",
      "Epoch: 530/10000... Step: 53000... Loss: 0.431620... Val Loss: 0.447303\n",
      "Epoch: 540/10000... Step: 54000... Loss: 0.481831... Val Loss: 0.447330\n",
      "Epoch: 550/10000... Step: 55000... Loss: 0.444672... Val Loss: 0.447328\n",
      "Epoch: 560/10000... Step: 56000... Loss: 0.457503... Val Loss: 0.447303\n",
      "Epoch: 570/10000... Step: 57000... Loss: 0.422002... Val Loss: 0.447303\n",
      "Epoch: 580/10000... Step: 58000... Loss: 0.438333... Val Loss: 0.447358\n",
      "Epoch: 590/10000... Step: 59000... Loss: 0.456085... Val Loss: 0.447307\n",
      "Epoch: 600/10000... Step: 60000... Loss: 0.451139... Val Loss: 0.447306\n",
      "Epoch: 610/10000... Step: 61000... Loss: 0.454290... Val Loss: 0.447383\n",
      "Epoch   611: reducing learning rate of group 0 to 1.5000e-03.\n",
      "Epoch: 620/10000... Step: 62000... Loss: 0.449508... Val Loss: 0.447333\n",
      "Epoch: 630/10000... Step: 63000... Loss: 0.430094... Val Loss: 0.447311\n",
      "Epoch: 640/10000... Step: 64000... Loss: 0.459294... Val Loss: 0.447303\n",
      "Validation loss decreased (0.447303 --> 0.447303).  Saving model ...\n",
      "Epoch: 650/10000... Step: 65000... Loss: 0.452726... Val Loss: 0.447303\n",
      "Epoch: 660/10000... Step: 66000... Loss: 0.452749... Val Loss: 0.447303\n",
      "Epoch: 670/10000... Step: 67000... Loss: 0.428387... Val Loss: 0.447303\n",
      "Epoch: 680/10000... Step: 68000... Loss: 0.444690... Val Loss: 0.447310\n",
      "Epoch: 690/10000... Step: 69000... Loss: 0.443018... Val Loss: 0.447333\n",
      "Epoch: 720/10000... Step: 72000... Loss: 0.436379... Val Loss: 0.447308\n",
      "Epoch: 730/10000... Step: 73000... Loss: 0.435296... Val Loss: 0.447414\n",
      "Epoch: 740/10000... Step: 74000... Loss: 0.456007... Val Loss: 0.447303\n",
      "Epoch: 750/10000... Step: 75000... Loss: 0.417051... Val Loss: 0.447303\n",
      "Epoch: 760/10000... Step: 76000... Loss: 0.468977... Val Loss: 0.447305\n",
      "Epoch: 770/10000... Step: 77000... Loss: 0.473884... Val Loss: 0.447307\n",
      "Epoch: 780/10000... Step: 78000... Loss: 0.426803... Val Loss: 0.447309\n",
      "Epoch: 790/10000... Step: 79000... Loss: 0.436659... Val Loss: 0.447309\n",
      "Epoch: 800/10000... Step: 80000... Loss: 0.452667... Val Loss: 0.447371\n",
      "Epoch: 810/10000... Step: 81000... Loss: 0.441381... Val Loss: 0.447303\n",
      "Validation loss decreased (0.447303 --> 0.447303).  Saving model ...\n",
      "Epoch: 820/10000... Step: 82000... Loss: 0.452628... Val Loss: 0.447303\n",
      "Epoch: 830/10000... Step: 83000... Loss: 0.436564... Val Loss: 0.447317\n",
      "Epoch: 840/10000... Step: 84000... Loss: 0.504704... Val Loss: 0.447303\n",
      "Epoch: 850/10000... Step: 85000... Loss: 0.467322... Val Loss: 0.447309\n",
      "Epoch: 860/10000... Step: 86000... Loss: 0.442883... Val Loss: 0.447304\n",
      "Epoch: 870/10000... Step: 87000... Loss: 0.468927... Val Loss: 0.447303\n",
      "Epoch: 900/10000... Step: 90000... Loss: 0.467292... Val Loss: 0.447303\n",
      "Epoch: 910/10000... Step: 91000... Loss: 0.449540... Val Loss: 0.447313\n",
      "Epoch: 920/10000... Step: 92000... Loss: 0.444651... Val Loss: 0.447306\n",
      "Epoch: 930/10000... Step: 93000... Loss: 0.459409... Val Loss: 0.447323\n",
      "Epoch: 940/10000... Step: 94000... Loss: 0.464002... Val Loss: 0.447304\n",
      "Epoch: 950/10000... Step: 95000... Loss: 0.493801... Val Loss: 0.447311\n",
      "Epoch: 960/10000... Step: 96000... Loss: 0.439803... Val Loss: 0.447307\n",
      "Epoch: 970/10000... Step: 97000... Loss: 0.425202... Val Loss: 0.447305\n",
      "Epoch: 980/10000... Step: 98000... Loss: 0.460826... Val Loss: 0.447314\n",
      "Epoch: 990/10000... Step: 99000... Loss: 0.419079... Val Loss: 0.447311\n",
      "Epoch: 1000/10000... Step: 100000... Loss: 0.457550... Val Loss: 0.447322\n",
      "Epoch: 1010/10000... Step: 101000... Loss: 0.475740... Val Loss: 0.447314\n",
      "Epoch: 1020/10000... Step: 102000... Loss: 0.444628... Val Loss: 0.447318\n",
      "Epoch: 1030/10000... Step: 103000... Loss: 0.428597... Val Loss: 0.447320\n",
      "Epoch: 1040/10000... Step: 104000... Loss: 0.464199... Val Loss: 0.447303\n",
      "Epoch: 1050/10000... Step: 105000... Loss: 0.486994... Val Loss: 0.447304\n",
      "Epoch: 1060/10000... Step: 106000... Loss: 0.446220... Val Loss: 0.447311\n",
      "Epoch: 1070/10000... Step: 107000... Loss: 0.449461... Val Loss: 0.447305\n",
      "Epoch: 1080/10000... Step: 108000... Loss: 0.474091... Val Loss: 0.447303\n",
      "Epoch: 1090/10000... Step: 109000... Loss: 0.469151... Val Loss: 0.447303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1100/10000... Step: 110000... Loss: 0.439683... Val Loss: 0.447303\n",
      "Epoch: 1110/10000... Step: 111000... Loss: 0.451094... Val Loss: 0.447335\n",
      "Epoch: 1120/10000... Step: 112000... Loss: 0.459152... Val Loss: 0.447320\n",
      "Epoch: 1130/10000... Step: 113000... Loss: 0.452772... Val Loss: 0.447305\n",
      "Epoch: 1140/10000... Step: 114000... Loss: 0.455921... Val Loss: 0.447304\n",
      "Epoch: 1150/10000... Step: 115000... Loss: 0.426947... Val Loss: 0.447307\n",
      "Epoch: 1160/10000... Step: 116000... Loss: 0.460788... Val Loss: 0.447309\n",
      "Epoch: 1170/10000... Step: 117000... Loss: 0.430038... Val Loss: 0.447303\n",
      "Validation loss decreased (0.447303 --> 0.447303).  Saving model ...\n",
      "Epoch: 1180/10000... Step: 118000... Loss: 0.431411... Val Loss: 0.447303\n",
      "Epoch: 1190/10000... Step: 119000... Loss: 0.430052... Val Loss: 0.447306\n",
      "Epoch: 1200/10000... Step: 120000... Loss: 0.436672... Val Loss: 0.447315\n",
      "Epoch: 1210/10000... Step: 121000... Loss: 0.467124... Val Loss: 0.447334\n",
      "Epoch: 1220/10000... Step: 122000... Loss: 0.447587... Val Loss: 0.447334\n",
      "Epoch: 1230/10000... Step: 123000... Loss: 0.446271... Val Loss: 0.447306\n",
      "Epoch: 1240/10000... Step: 124000... Loss: 0.446306... Val Loss: 0.447303\n",
      "Epoch: 1250/10000... Step: 125000... Loss: 0.462386... Val Loss: 0.447319\n",
      "Epoch: 1260/10000... Step: 126000... Loss: 0.404128... Val Loss: 0.447303\n",
      "Epoch: 1270/10000... Step: 127000... Loss: 0.467372... Val Loss: 0.447312\n",
      "Epoch: 1280/10000... Step: 128000... Loss: 0.439739... Val Loss: 0.447303\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f3cd4e146227>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2076\u001b[0m     return torch._C._nn.binary_cross_entropy(\n\u001b[0;32m-> 2077\u001b[0;31m         input, target, weight, reduction_enum)\n\u001b[0m\u001b[1;32m   2078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "counter = 0\n",
    "print_every = 1000\n",
    "clip = .5\n",
    "valid_loss_min = np.Inf\n",
    "val_loss = torch.tensor(np.Inf)\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(train_batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = train_criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            val_h = model.init_hidden(val_batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in val_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp, val_h)\n",
    "                val_loss = val_criterion(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './model_1face.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)\n",
    "    scheduler.step(val_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adding 2nd fc with (64, 32) all activated with ReLU\n",
    "0.75 dropout\n",
    "0.001 LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DFDCNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(DFDCNet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.75)\n",
    "        self.batchnorm = nn.BatchNorm1d(hidden_dim)\n",
    "        self.elu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(hidden_dim, 32)\n",
    "#         self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.float()\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "#         out = self.batchnorm(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.elu(out)\n",
    "#         out = self.fc2(out)\n",
    "#         out = self.elu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.elu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFDCNet(\n",
      "  (lstm): LSTM(512, 64, num_layers=5, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.75, inplace=False)\n",
      "  (batchnorm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (elu): ReLU()\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 512\n",
    "output_size = 1\n",
    "hidden_dim = 64\n",
    "n_layers = 5\n",
    "\n",
    "model = DFDCNet(input_size, output_size, hidden_dim, n_layers)\n",
    "model.to(device)\n",
    "train_criterion = nn.BCELoss()\n",
    "val_criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=500, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/10000... Step: 1000... Loss: 0.452133... Val Loss: 0.464245\n",
      "Validation loss decreased (inf --> 0.464245).  Saving model ...\n",
      "Epoch: 20/10000... Step: 2000... Loss: 0.462666... Val Loss: 0.456296\n",
      "Validation loss decreased (0.464245 --> 0.456296).  Saving model ...\n",
      "Epoch: 30/10000... Step: 3000... Loss: 0.417840... Val Loss: 0.449044\n",
      "Validation loss decreased (0.456296 --> 0.449044).  Saving model ...\n",
      "Epoch: 40/10000... Step: 4000... Loss: 0.452251... Val Loss: 0.448030\n",
      "Validation loss decreased (0.449044 --> 0.448030).  Saving model ...\n",
      "Epoch: 50/10000... Step: 5000... Loss: 0.431644... Val Loss: 0.447310\n",
      "Validation loss decreased (0.448030 --> 0.447310).  Saving model ...\n",
      "Epoch: 60/10000... Step: 6000... Loss: 0.473961... Val Loss: 0.447324\n",
      "Epoch: 70/10000... Step: 7000... Loss: 0.461031... Val Loss: 0.447313\n",
      "Epoch: 80/10000... Step: 8000... Loss: 0.436875... Val Loss: 0.447465\n",
      "Epoch: 90/10000... Step: 9000... Loss: 0.451956... Val Loss: 0.447679\n",
      "Epoch: 100/10000... Step: 10000... Loss: 0.454414... Val Loss: 0.447312\n",
      "Epoch: 110/10000... Step: 11000... Loss: 0.454349... Val Loss: 0.447303\n",
      "Validation loss decreased (0.447310 --> 0.447303).  Saving model ...\n",
      "Epoch: 120/10000... Step: 12000... Loss: 0.457420... Val Loss: 0.447303\n",
      "Validation loss decreased (0.447303 --> 0.447303).  Saving model ...\n",
      "Epoch: 130/10000... Step: 13000... Loss: 0.472053... Val Loss: 0.447321\n",
      "Epoch: 140/10000... Step: 14000... Loss: 0.462297... Val Loss: 0.447316\n",
      "Epoch: 150/10000... Step: 15000... Loss: 0.446242... Val Loss: 0.447334\n",
      "Epoch: 160/10000... Step: 16000... Loss: 0.455949... Val Loss: 0.447303\n",
      "Epoch: 170/10000... Step: 17000... Loss: 0.470588... Val Loss: 0.447303\n",
      "Epoch: 180/10000... Step: 18000... Loss: 0.441636... Val Loss: 0.447385\n",
      "Epoch: 190/10000... Step: 19000... Loss: 0.446069... Val Loss: 0.447313\n",
      "Epoch: 200/10000... Step: 20000... Loss: 0.403244... Val Loss: 0.447347\n",
      "Epoch: 210/10000... Step: 21000... Loss: 0.464173... Val Loss: 0.447307\n",
      "Epoch: 220/10000... Step: 22000... Loss: 0.436480... Val Loss: 0.447348\n",
      "Epoch: 230/10000... Step: 23000... Loss: 0.460830... Val Loss: 0.447306\n",
      "Epoch: 240/10000... Step: 24000... Loss: 0.443172... Val Loss: 0.447314\n",
      "Epoch: 250/10000... Step: 25000... Loss: 0.443108... Val Loss: 0.447306\n",
      "Epoch: 260/10000... Step: 26000... Loss: 0.433307... Val Loss: 0.447304\n",
      "Epoch: 270/10000... Step: 27000... Loss: 0.451145... Val Loss: 0.447303\n",
      "Epoch: 280/10000... Step: 28000... Loss: 0.459244... Val Loss: 0.447304\n",
      "Epoch: 290/10000... Step: 29000... Loss: 0.467164... Val Loss: 0.447319\n",
      "Epoch: 300/10000... Step: 30000... Loss: 0.423761... Val Loss: 0.447329\n",
      "Epoch: 310/10000... Step: 31000... Loss: 0.431477... Val Loss: 0.447329\n",
      "Epoch: 320/10000... Step: 32000... Loss: 0.439882... Val Loss: 0.447312\n",
      "Epoch: 330/10000... Step: 33000... Loss: 0.441385... Val Loss: 0.447314\n",
      "Epoch: 340/10000... Step: 34000... Loss: 0.438207... Val Loss: 0.447309\n",
      "Epoch: 350/10000... Step: 35000... Loss: 0.468995... Val Loss: 0.447303\n",
      "Validation loss decreased (0.447303 --> 0.447303).  Saving model ...\n",
      "Epoch: 360/10000... Step: 36000... Loss: 0.439791... Val Loss: 0.447303\n",
      "Epoch: 370/10000... Step: 37000... Loss: 0.469308... Val Loss: 0.447334\n",
      "Epoch: 380/10000... Step: 38000... Loss: 0.440123... Val Loss: 0.447425\n",
      "Epoch: 390/10000... Step: 39000... Loss: 0.462701... Val Loss: 0.447329\n",
      "Epoch: 400/10000... Step: 40000... Loss: 0.435008... Val Loss: 0.447313\n",
      "Epoch: 410/10000... Step: 41000... Loss: 0.480076... Val Loss: 0.447325\n",
      "Epoch: 420/10000... Step: 42000... Loss: 0.462437... Val Loss: 0.447303\n",
      "Epoch: 430/10000... Step: 43000... Loss: 0.438225... Val Loss: 0.447310\n",
      "Epoch: 440/10000... Step: 44000... Loss: 0.452680... Val Loss: 0.447313\n",
      "Epoch: 450/10000... Step: 45000... Loss: 0.428238... Val Loss: 0.447320\n",
      "Epoch: 460/10000... Step: 46000... Loss: 0.462365... Val Loss: 0.447335\n",
      "Epoch: 470/10000... Step: 47000... Loss: 0.455882... Val Loss: 0.447327\n",
      "Epoch: 480/10000... Step: 48000... Loss: 0.457583... Val Loss: 0.447303\n",
      "Epoch: 490/10000... Step: 49000... Loss: 0.464255... Val Loss: 0.447311\n",
      "Epoch: 500/10000... Step: 50000... Loss: 0.436709... Val Loss: 0.447352\n",
      "Epoch: 510/10000... Step: 51000... Loss: 0.431690... Val Loss: 0.447309\n",
      "Epoch: 520/10000... Step: 52000... Loss: 0.462408... Val Loss: 0.447311\n",
      "Epoch: 530/10000... Step: 53000... Loss: 0.433284... Val Loss: 0.447303\n",
      "Epoch: 540/10000... Step: 54000... Loss: 0.460768... Val Loss: 0.447308\n",
      "Epoch: 550/10000... Step: 55000... Loss: 0.417118... Val Loss: 0.447304\n",
      "Epoch: 560/10000... Step: 56000... Loss: 0.476631... Val Loss: 0.447359\n",
      "Epoch: 570/10000... Step: 57000... Loss: 0.465515... Val Loss: 0.447356\n",
      "Epoch: 580/10000... Step: 58000... Loss: 0.449487... Val Loss: 0.447308\n",
      "Epoch: 590/10000... Step: 59000... Loss: 0.451115... Val Loss: 0.447319\n",
      "Epoch: 600/10000... Step: 60000... Loss: 0.464178... Val Loss: 0.447305\n",
      "Epoch: 610/10000... Step: 61000... Loss: 0.455919... Val Loss: 0.447308\n",
      "Epoch: 620/10000... Step: 62000... Loss: 0.447885... Val Loss: 0.447317\n",
      "Epoch: 630/10000... Step: 63000... Loss: 0.441262... Val Loss: 0.447314\n",
      "Epoch: 640/10000... Step: 64000... Loss: 0.443010... Val Loss: 0.447314\n",
      "Epoch: 650/10000... Step: 65000... Loss: 0.423789... Val Loss: 0.447310\n",
      "Epoch: 660/10000... Step: 66000... Loss: 0.425519... Val Loss: 0.447340\n",
      "Epoch: 670/10000... Step: 67000... Loss: 0.444635... Val Loss: 0.447331\n",
      "Epoch: 680/10000... Step: 68000... Loss: 0.446252... Val Loss: 0.447303\n",
      "Epoch: 690/10000... Step: 69000... Loss: 0.405917... Val Loss: 0.447306\n",
      "Epoch   691: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch: 700/10000... Step: 70000... Loss: 0.465654... Val Loss: 0.447305\n",
      "Epoch: 710/10000... Step: 71000... Loss: 0.431817... Val Loss: 0.447335\n",
      "Epoch: 720/10000... Step: 72000... Loss: 0.425319... Val Loss: 0.447317\n",
      "Epoch: 730/10000... Step: 73000... Loss: 0.454330... Val Loss: 0.447321\n",
      "Epoch: 740/10000... Step: 74000... Loss: 0.462361... Val Loss: 0.447315\n",
      "Epoch: 750/10000... Step: 75000... Loss: 0.462398... Val Loss: 0.447308\n",
      "Epoch: 760/10000... Step: 76000... Loss: 0.426795... Val Loss: 0.447305\n",
      "Epoch: 770/10000... Step: 77000... Loss: 0.449519... Val Loss: 0.447305\n",
      "Epoch: 780/10000... Step: 78000... Loss: 0.451079... Val Loss: 0.447325\n",
      "Epoch: 790/10000... Step: 79000... Loss: 0.459259... Val Loss: 0.447304\n",
      "Epoch: 800/10000... Step: 80000... Loss: 0.441400... Val Loss: 0.447307\n",
      "Epoch: 810/10000... Step: 81000... Loss: 0.457585... Val Loss: 0.447313\n",
      "Epoch: 820/10000... Step: 82000... Loss: 0.454324... Val Loss: 0.447325\n",
      "Epoch: 830/10000... Step: 83000... Loss: 0.439805... Val Loss: 0.447314\n",
      "Epoch: 840/10000... Step: 84000... Loss: 0.462429... Val Loss: 0.447303\n",
      "Epoch: 850/10000... Step: 85000... Loss: 0.460746... Val Loss: 0.447319\n",
      "Epoch: 860/10000... Step: 86000... Loss: 0.431704... Val Loss: 0.447309\n",
      "Epoch: 870/10000... Step: 87000... Loss: 0.464053... Val Loss: 0.447305\n",
      "Epoch: 880/10000... Step: 88000... Loss: 0.439703... Val Loss: 0.447305\n",
      "Epoch: 890/10000... Step: 89000... Loss: 0.434884... Val Loss: 0.447303\n",
      "Epoch: 900/10000... Step: 90000... Loss: 0.438146... Val Loss: 0.447303\n",
      "Epoch: 910/10000... Step: 91000... Loss: 0.478418... Val Loss: 0.447322\n",
      "Epoch: 950/10000... Step: 95000... Loss: 0.438253... Val Loss: 0.447307\n",
      "Epoch: 960/10000... Step: 96000... Loss: 0.452748... Val Loss: 0.447309\n",
      "Epoch: 970/10000... Step: 97000... Loss: 0.433424... Val Loss: 0.447324\n",
      "Epoch: 980/10000... Step: 98000... Loss: 0.430145... Val Loss: 0.447311\n",
      "Epoch: 990/10000... Step: 99000... Loss: 0.460781... Val Loss: 0.447304\n",
      "Epoch: 1000/10000... Step: 100000... Loss: 0.462593... Val Loss: 0.447303\n",
      "Epoch: 1010/10000... Step: 101000... Loss: 0.435080... Val Loss: 0.447305\n",
      "Epoch: 1020/10000... Step: 102000... Loss: 0.430256... Val Loss: 0.447321\n",
      "Epoch: 1030/10000... Step: 103000... Loss: 0.457582... Val Loss: 0.447312\n",
      "Epoch: 1040/10000... Step: 104000... Loss: 0.488409... Val Loss: 0.447304\n",
      "Epoch: 1050/10000... Step: 105000... Loss: 0.420341... Val Loss: 0.447303\n",
      "Epoch: 1060/10000... Step: 106000... Loss: 0.473803... Val Loss: 0.447303\n",
      "Epoch: 1070/10000... Step: 107000... Loss: 0.476802... Val Loss: 0.447324\n",
      "Epoch: 1080/10000... Step: 108000... Loss: 0.452771... Val Loss: 0.447303\n",
      "Epoch: 1090/10000... Step: 109000... Loss: 0.426612... Val Loss: 0.447309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1100/10000... Step: 110000... Loss: 0.413692... Val Loss: 0.447305\n",
      "Epoch: 1110/10000... Step: 111000... Loss: 0.467235... Val Loss: 0.447312\n",
      "Epoch: 1120/10000... Step: 112000... Loss: 0.444538... Val Loss: 0.447303\n",
      "Epoch: 1130/10000... Step: 113000... Loss: 0.446272... Val Loss: 0.447329\n",
      "Epoch: 1140/10000... Step: 114000... Loss: 0.480350... Val Loss: 0.447303\n",
      "Epoch: 1150/10000... Step: 115000... Loss: 0.452690... Val Loss: 0.447341\n",
      "Epoch: 1160/10000... Step: 116000... Loss: 0.462508... Val Loss: 0.447303\n",
      "Epoch: 1170/10000... Step: 117000... Loss: 0.439838... Val Loss: 0.447306\n",
      "Epoch: 1180/10000... Step: 118000... Loss: 0.418754... Val Loss: 0.447306\n",
      "Epoch: 1190/10000... Step: 119000... Loss: 0.444606... Val Loss: 0.447303\n",
      "Epoch: 1200/10000... Step: 120000... Loss: 0.443066... Val Loss: 0.447332\n",
      "Epoch: 1210/10000... Step: 121000... Loss: 0.443047... Val Loss: 0.447304\n",
      "Epoch: 1220/10000... Step: 122000... Loss: 0.467334... Val Loss: 0.447304\n",
      "Epoch: 1230/10000... Step: 123000... Loss: 0.441433... Val Loss: 0.447309\n",
      "Epoch: 1240/10000... Step: 124000... Loss: 0.444609... Val Loss: 0.447311\n",
      "Epoch: 1250/10000... Step: 125000... Loss: 0.447994... Val Loss: 0.447303\n",
      "Epoch: 1260/10000... Step: 126000... Loss: 0.426886... Val Loss: 0.447308\n",
      "Epoch: 1270/10000... Step: 127000... Loss: 0.446244... Val Loss: 0.447309\n",
      "Epoch: 1280/10000... Step: 128000... Loss: 0.431799... Val Loss: 0.447317\n",
      "Epoch: 1290/10000... Step: 129000... Loss: 0.475159... Val Loss: 0.447329\n",
      "Epoch: 1300/10000... Step: 130000... Loss: 0.475266... Val Loss: 0.447316\n",
      "Epoch: 1310/10000... Step: 131000... Loss: 0.406137... Val Loss: 0.447322\n",
      "Epoch: 1320/10000... Step: 132000... Loss: 0.446334... Val Loss: 0.447303\n",
      "Epoch: 1330/10000... Step: 133000... Loss: 0.452743... Val Loss: 0.447303\n",
      "Epoch: 1340/10000... Step: 134000... Loss: 0.457580... Val Loss: 0.447304\n",
      "Epoch: 1350/10000... Step: 135000... Loss: 0.418742... Val Loss: 0.447304\n",
      "Epoch: 1360/10000... Step: 136000... Loss: 0.439893... Val Loss: 0.447337\n",
      "Epoch: 1370/10000... Step: 137000... Loss: 0.451141... Val Loss: 0.447304\n",
      "Epoch: 1380/10000... Step: 138000... Loss: 0.471973... Val Loss: 0.447313\n",
      "Epoch: 1390/10000... Step: 139000... Loss: 0.447886... Val Loss: 0.447303\n",
      "Epoch: 1400/10000... Step: 140000... Loss: 0.460720... Val Loss: 0.447311\n",
      "Epoch: 1440/10000... Step: 144000... Loss: 0.428511... Val Loss: 0.447311\n",
      "Epoch: 1450/10000... Step: 145000... Loss: 0.460810... Val Loss: 0.447314\n",
      "Epoch: 1460/10000... Step: 146000... Loss: 0.449479... Val Loss: 0.447308\n",
      "Epoch: 1470/10000... Step: 147000... Loss: 0.455890... Val Loss: 0.447318\n",
      "Epoch: 1480/10000... Step: 148000... Loss: 0.450990... Val Loss: 0.447311\n",
      "Epoch: 1490/10000... Step: 149000... Loss: 0.485060... Val Loss: 0.447307\n",
      "Epoch: 1500/10000... Step: 150000... Loss: 0.436555... Val Loss: 0.447310\n",
      "Epoch: 1510/10000... Step: 151000... Loss: 0.441382... Val Loss: 0.447306\n",
      "Epoch: 1520/10000... Step: 152000... Loss: 0.425192... Val Loss: 0.447304\n",
      "Epoch: 1530/10000... Step: 153000... Loss: 0.431753... Val Loss: 0.447312\n",
      "Epoch: 1540/10000... Step: 154000... Loss: 0.444599... Val Loss: 0.447305\n",
      "Epoch: 1550/10000... Step: 155000... Loss: 0.459180... Val Loss: 0.447309\n",
      "Epoch: 1560/10000... Step: 156000... Loss: 0.444677... Val Loss: 0.447314\n",
      "Epoch: 1570/10000... Step: 157000... Loss: 0.491432... Val Loss: 0.447309\n",
      "Epoch: 1580/10000... Step: 158000... Loss: 0.436634... Val Loss: 0.447325\n",
      "Epoch: 1590/10000... Step: 159000... Loss: 0.443057... Val Loss: 0.447311\n",
      "Epoch: 1600/10000... Step: 160000... Loss: 0.478640... Val Loss: 0.447304\n",
      "Epoch: 1610/10000... Step: 161000... Loss: 0.425226... Val Loss: 0.447306\n",
      "Epoch: 1620/10000... Step: 162000... Loss: 0.462403... Val Loss: 0.447307\n",
      "Epoch: 1630/10000... Step: 163000... Loss: 0.438235... Val Loss: 0.447318\n",
      "Epoch: 1640/10000... Step: 164000... Loss: 0.417240... Val Loss: 0.447313\n",
      "Epoch: 1650/10000... Step: 165000... Loss: 0.462373... Val Loss: 0.447310\n",
      "Epoch: 1660/10000... Step: 166000... Loss: 0.441398... Val Loss: 0.447311\n",
      "Epoch: 1670/10000... Step: 167000... Loss: 0.452715... Val Loss: 0.447308\n",
      "Epoch: 1680/10000... Step: 168000... Loss: 0.504557... Val Loss: 0.447306\n",
      "Epoch: 1690/10000... Step: 169000... Loss: 0.454318... Val Loss: 0.447309\n",
      "Epoch: 1700/10000... Step: 170000... Loss: 0.439779... Val Loss: 0.447307\n",
      "Epoch: 1710/10000... Step: 171000... Loss: 0.426942... Val Loss: 0.447318\n",
      "Epoch: 1720/10000... Step: 172000... Loss: 0.468843... Val Loss: 0.447307\n",
      "Epoch: 1730/10000... Step: 173000... Loss: 0.447875... Val Loss: 0.447319\n",
      "Epoch: 1740/10000... Step: 174000... Loss: 0.449512... Val Loss: 0.447306\n",
      "Epoch: 1750/10000... Step: 175000... Loss: 0.444663... Val Loss: 0.447307\n",
      "Epoch: 1760/10000... Step: 176000... Loss: 0.443064... Val Loss: 0.447311\n",
      "Epoch: 1770/10000... Step: 177000... Loss: 0.420427... Val Loss: 0.447305\n",
      "Epoch: 1780/10000... Step: 178000... Loss: 0.462460... Val Loss: 0.447306\n",
      "Epoch: 1790/10000... Step: 179000... Loss: 0.422051... Val Loss: 0.447316\n",
      "Epoch: 1800/10000... Step: 180000... Loss: 0.465574... Val Loss: 0.447315\n",
      "Epoch: 1810/10000... Step: 181000... Loss: 0.431732... Val Loss: 0.447309\n",
      "Epoch: 1820/10000... Step: 182000... Loss: 0.457547... Val Loss: 0.447310\n",
      "Epoch: 1830/10000... Step: 183000... Loss: 0.459220... Val Loss: 0.447305\n",
      "Epoch: 1840/10000... Step: 184000... Loss: 0.464082... Val Loss: 0.447309\n",
      "Epoch: 1850/10000... Step: 185000... Loss: 0.480204... Val Loss: 0.447306\n",
      "Epoch: 1860/10000... Step: 186000... Loss: 0.455925... Val Loss: 0.447313\n",
      "Epoch: 1870/10000... Step: 187000... Loss: 0.417227... Val Loss: 0.447309\n",
      "Epoch: 1880/10000... Step: 188000... Loss: 0.447838... Val Loss: 0.447308\n",
      "Epoch: 1890/10000... Step: 189000... Loss: 0.439800... Val Loss: 0.447314\n",
      "Epoch: 1900/10000... Step: 190000... Loss: 0.433327... Val Loss: 0.447307\n",
      "Epoch: 1910/10000... Step: 191000... Loss: 0.446229... Val Loss: 0.447307\n",
      "Epoch: 1920/10000... Step: 192000... Loss: 0.431807... Val Loss: 0.447315\n",
      "Epoch  1922: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch: 1930/10000... Step: 193000... Loss: 0.433342... Val Loss: 0.447309\n",
      "Epoch: 1940/10000... Step: 194000... Loss: 0.462465... Val Loss: 0.447306\n",
      "Epoch: 1950/10000... Step: 195000... Loss: 0.441398... Val Loss: 0.447307\n",
      "Epoch: 1960/10000... Step: 196000... Loss: 0.434932... Val Loss: 0.447308\n",
      "Epoch: 1970/10000... Step: 197000... Loss: 0.441366... Val Loss: 0.447308\n",
      "Epoch: 1980/10000... Step: 198000... Loss: 0.415662... Val Loss: 0.447311\n",
      "Epoch: 1990/10000... Step: 199000... Loss: 0.460790... Val Loss: 0.447307\n",
      "Epoch: 2000/10000... Step: 200000... Loss: 0.483497... Val Loss: 0.447305\n",
      "Epoch: 2010/10000... Step: 201000... Loss: 0.467318... Val Loss: 0.447306\n",
      "Epoch: 2020/10000... Step: 202000... Loss: 0.489885... Val Loss: 0.447307\n",
      "Epoch: 2030/10000... Step: 203000... Loss: 0.486619... Val Loss: 0.447309\n",
      "Epoch: 2040/10000... Step: 204000... Loss: 0.439804... Val Loss: 0.447312\n",
      "Epoch: 2050/10000... Step: 205000... Loss: 0.459207... Val Loss: 0.447308\n",
      "Epoch: 2060/10000... Step: 206000... Loss: 0.438162... Val Loss: 0.447307\n",
      "Epoch: 2070/10000... Step: 207000... Loss: 0.457594... Val Loss: 0.447314\n",
      "Epoch: 2080/10000... Step: 208000... Loss: 0.459146... Val Loss: 0.447309\n",
      "Epoch: 2090/10000... Step: 209000... Loss: 0.441426... Val Loss: 0.447306\n",
      "Epoch: 2100/10000... Step: 210000... Loss: 0.439831... Val Loss: 0.447308\n",
      "Epoch: 2110/10000... Step: 211000... Loss: 0.422036... Val Loss: 0.447308\n",
      "Epoch: 2120/10000... Step: 212000... Loss: 0.434944... Val Loss: 0.447307\n",
      "Epoch: 2130/10000... Step: 213000... Loss: 0.467321... Val Loss: 0.447305\n",
      "Epoch: 2140/10000... Step: 214000... Loss: 0.431702... Val Loss: 0.447307\n",
      "Epoch: 2150/10000... Step: 215000... Loss: 0.449532... Val Loss: 0.447310\n",
      "Epoch: 2160/10000... Step: 216000... Loss: 0.447838... Val Loss: 0.447307\n",
      "Epoch: 2170/10000... Step: 217000... Loss: 0.439780... Val Loss: 0.447308\n",
      "Epoch: 2180/10000... Step: 218000... Loss: 0.457587... Val Loss: 0.447309\n",
      "Epoch: 2190/10000... Step: 219000... Loss: 0.468886... Val Loss: 0.447305\n",
      "Epoch: 2200/10000... Step: 220000... Loss: 0.452689... Val Loss: 0.447306\n",
      "Epoch: 2210/10000... Step: 221000... Loss: 0.478583... Val Loss: 0.447307\n",
      "Epoch: 2220/10000... Step: 222000... Loss: 0.434914... Val Loss: 0.447305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2230/10000... Step: 223000... Loss: 0.473766... Val Loss: 0.447305\n",
      "Epoch: 2240/10000... Step: 224000... Loss: 0.486651... Val Loss: 0.447308\n",
      "Epoch: 2250/10000... Step: 225000... Loss: 0.457565... Val Loss: 0.447307\n",
      "Epoch: 2260/10000... Step: 226000... Loss: 0.444632... Val Loss: 0.447305\n",
      "Epoch: 2270/10000... Step: 227000... Loss: 0.436559... Val Loss: 0.447308\n",
      "Epoch: 2280/10000... Step: 228000... Loss: 0.499576... Val Loss: 0.447307\n",
      "Epoch: 2290/10000... Step: 229000... Loss: 0.478566... Val Loss: 0.447308\n",
      "Epoch: 2300/10000... Step: 230000... Loss: 0.488252... Val Loss: 0.447306\n",
      "Epoch: 2310/10000... Step: 231000... Loss: 0.462457... Val Loss: 0.447305\n",
      "Epoch: 2320/10000... Step: 232000... Loss: 0.396279... Val Loss: 0.447312\n",
      "Epoch: 2330/10000... Step: 233000... Loss: 0.446233... Val Loss: 0.447310\n",
      "Epoch: 2340/10000... Step: 234000... Loss: 0.467215... Val Loss: 0.447310\n",
      "Epoch: 2350/10000... Step: 235000... Loss: 0.462374... Val Loss: 0.447311\n",
      "Epoch: 2360/10000... Step: 236000... Loss: 0.447875... Val Loss: 0.447307\n",
      "Epoch: 2370/10000... Step: 237000... Loss: 0.454307... Val Loss: 0.447307\n",
      "Epoch: 2380/10000... Step: 238000... Loss: 0.480181... Val Loss: 0.447305\n",
      "Epoch: 2390/10000... Step: 239000... Loss: 0.438149... Val Loss: 0.447306\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f3cd4e146227>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-50d824cdd86e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mlstm_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 559\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "counter = 0\n",
    "print_every = 1000\n",
    "clip = .5\n",
    "valid_loss_min = np.Inf\n",
    "val_loss = torch.tensor(np.Inf)\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(train_batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = train_criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            val_h = model.init_hidden(val_batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in val_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp, val_h)\n",
    "                val_loss = val_criterion(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './model_1face.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)\n",
    "    scheduler.step(val_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adding 2nd fc with (64, 32) all activated with ELU\n",
    "0.9 dropout\n",
    "0.0001 LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DFDCNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(DFDCNet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.9)\n",
    "        self.batchnorm = nn.BatchNorm1d(hidden_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.fc1 = nn.Linear(hidden_dim, 32)\n",
    "#         self.fc2 = nn.Linear(64, 32)\n",
    "#         self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(32, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.float()\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "#         out = self.batchnorm(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.elu(out)\n",
    "#         out = self.fc2(out)\n",
    "#         out = self.elu(out)\n",
    "#         out = self.fc3(out)\n",
    "#         out = self.elu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFDCNet(\n",
      "  (lstm): LSTM(512, 64, num_layers=5, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.9, inplace=False)\n",
      "  (batchnorm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (elu): ELU(alpha=1.0)\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 512\n",
    "output_size = 1\n",
    "hidden_dim = 64\n",
    "n_layers = 5\n",
    "\n",
    "model = DFDCNet(input_size, output_size, hidden_dim, n_layers)\n",
    "model.to(device)\n",
    "train_criterion = nn.BCELoss()\n",
    "val_criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=500, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/10000... Step: 1000... Loss: 0.432577... Val Loss: 0.447698\n",
      "Validation loss decreased (inf --> 0.447698).  Saving model ...\n",
      "Epoch: 20/10000... Step: 2000... Loss: 0.456634... Val Loss: 0.447488\n",
      "Validation loss decreased (0.447698 --> 0.447488).  Saving model ...\n",
      "Epoch: 30/10000... Step: 3000... Loss: 0.456329... Val Loss: 0.447308\n",
      "Validation loss decreased (0.447488 --> 0.447308).  Saving model ...\n",
      "Epoch: 40/10000... Step: 4000... Loss: 0.459240... Val Loss: 0.447363\n",
      "Epoch: 50/10000... Step: 5000... Loss: 0.424112... Val Loss: 0.447420\n",
      "Epoch: 60/10000... Step: 6000... Loss: 0.427462... Val Loss: 0.447305\n",
      "Validation loss decreased (0.447308 --> 0.447305).  Saving model ...\n",
      "Epoch: 70/10000... Step: 7000... Loss: 0.469035... Val Loss: 0.447304\n",
      "Validation loss decreased (0.447305 --> 0.447304).  Saving model ...\n",
      "Epoch: 80/10000... Step: 8000... Loss: 0.449686... Val Loss: 0.447332\n",
      "Epoch: 90/10000... Step: 9000... Loss: 0.450217... Val Loss: 0.447412\n",
      "Epoch: 100/10000... Step: 10000... Loss: 0.455992... Val Loss: 0.447311\n",
      "Epoch: 110/10000... Step: 11000... Loss: 0.439633... Val Loss: 0.447455\n",
      "Epoch: 120/10000... Step: 12000... Loss: 0.463530... Val Loss: 0.447582\n",
      "Epoch: 130/10000... Step: 13000... Loss: 0.471880... Val Loss: 0.447308\n",
      "Epoch: 140/10000... Step: 14000... Loss: 0.424086... Val Loss: 0.447401\n",
      "Epoch: 150/10000... Step: 15000... Loss: 0.477739... Val Loss: 0.447303\n",
      "Validation loss decreased (0.447304 --> 0.447303).  Saving model ...\n",
      "Epoch: 160/10000... Step: 16000... Loss: 0.449541... Val Loss: 0.447321\n",
      "Epoch: 170/10000... Step: 17000... Loss: 0.487979... Val Loss: 0.447402\n",
      "Epoch: 180/10000... Step: 18000... Loss: 0.462636... Val Loss: 0.447341\n",
      "Epoch: 190/10000... Step: 19000... Loss: 0.423940... Val Loss: 0.447415\n",
      "Epoch: 200/10000... Step: 20000... Loss: 0.438007... Val Loss: 0.447362\n",
      "Epoch: 210/10000... Step: 21000... Loss: 0.436480... Val Loss: 0.447369\n",
      "Epoch: 220/10000... Step: 22000... Loss: 0.461841... Val Loss: 0.447492\n",
      "Epoch: 230/10000... Step: 23000... Loss: 0.454243... Val Loss: 0.447343\n",
      "Epoch: 240/10000... Step: 24000... Loss: 0.416651... Val Loss: 0.447306\n",
      "Epoch: 250/10000... Step: 25000... Loss: 0.444920... Val Loss: 0.447310\n",
      "Epoch: 260/10000... Step: 26000... Loss: 0.454501... Val Loss: 0.447534\n",
      "Epoch: 270/10000... Step: 27000... Loss: 0.432928... Val Loss: 0.447347\n",
      "Epoch: 280/10000... Step: 28000... Loss: 0.453300... Val Loss: 0.447680\n",
      "Epoch: 290/10000... Step: 29000... Loss: 0.434874... Val Loss: 0.447305\n",
      "Epoch: 300/10000... Step: 30000... Loss: 0.435285... Val Loss: 0.447317\n",
      "Epoch: 310/10000... Step: 31000... Loss: 0.430163... Val Loss: 0.447340\n",
      "Epoch: 320/10000... Step: 32000... Loss: 0.459147... Val Loss: 0.447310\n",
      "Epoch: 330/10000... Step: 33000... Loss: 0.457758... Val Loss: 0.447350\n",
      "Epoch: 340/10000... Step: 34000... Loss: 0.436483... Val Loss: 0.447303\n",
      "Epoch: 350/10000... Step: 35000... Loss: 0.449584... Val Loss: 0.447320\n",
      "Epoch: 360/10000... Step: 36000... Loss: 0.428425... Val Loss: 0.447335\n",
      "Epoch: 370/10000... Step: 37000... Loss: 0.452787... Val Loss: 0.447303\n",
      "Epoch: 380/10000... Step: 38000... Loss: 0.455873... Val Loss: 0.447316\n",
      "Epoch: 390/10000... Step: 39000... Loss: 0.421766... Val Loss: 0.447314\n",
      "Epoch: 400/10000... Step: 40000... Loss: 0.391895... Val Loss: 0.447319\n",
      "Epoch: 410/10000... Step: 41000... Loss: 0.459437... Val Loss: 0.447314\n",
      "Epoch: 420/10000... Step: 42000... Loss: 0.480164... Val Loss: 0.447303\n",
      "Validation loss decreased (0.447303 --> 0.447303).  Saving model ...\n",
      "Epoch: 430/10000... Step: 43000... Loss: 0.464128... Val Loss: 0.447303\n",
      "Epoch: 440/10000... Step: 44000... Loss: 0.448015... Val Loss: 0.447380\n",
      "Epoch: 450/10000... Step: 45000... Loss: 0.454415... Val Loss: 0.447307\n",
      "Epoch: 460/10000... Step: 46000... Loss: 0.436751... Val Loss: 0.447313\n",
      "Epoch: 470/10000... Step: 47000... Loss: 0.450927... Val Loss: 0.447321\n",
      "Epoch: 480/10000... Step: 48000... Loss: 0.443064... Val Loss: 0.447309\n",
      "Epoch: 490/10000... Step: 49000... Loss: 0.449605... Val Loss: 0.447372\n",
      "Epoch: 500/10000... Step: 50000... Loss: 0.439649... Val Loss: 0.447303\n",
      "Epoch: 540/10000... Step: 54000... Loss: 0.486693... Val Loss: 0.447322\n",
      "Epoch: 550/10000... Step: 55000... Loss: 0.468991... Val Loss: 0.447303\n",
      "Epoch: 560/10000... Step: 56000... Loss: 0.473283... Val Loss: 0.447473\n",
      "Epoch: 570/10000... Step: 57000... Loss: 0.457573... Val Loss: 0.447303\n",
      "Validation loss decreased (0.447303 --> 0.447303).  Saving model ...\n",
      "Epoch: 580/10000... Step: 58000... Loss: 0.428516... Val Loss: 0.447318\n",
      "Epoch: 590/10000... Step: 59000... Loss: 0.409166... Val Loss: 0.447303\n",
      "Epoch: 600/10000... Step: 60000... Loss: 0.452646... Val Loss: 0.447303\n",
      "Epoch: 610/10000... Step: 61000... Loss: 0.462538... Val Loss: 0.447304\n",
      "Epoch: 620/10000... Step: 62000... Loss: 0.455968... Val Loss: 0.447303\n",
      "Epoch: 630/10000... Step: 63000... Loss: 0.427279... Val Loss: 0.447375\n",
      "Epoch: 640/10000... Step: 64000... Loss: 0.451050... Val Loss: 0.447307\n",
      "Epoch: 650/10000... Step: 65000... Loss: 0.468980... Val Loss: 0.447305\n",
      "Epoch: 660/10000... Step: 66000... Loss: 0.444683... Val Loss: 0.447317\n",
      "Epoch: 670/10000... Step: 67000... Loss: 0.461249... Val Loss: 0.447417\n",
      "Epoch: 680/10000... Step: 68000... Loss: 0.447933... Val Loss: 0.447303\n",
      "Epoch: 690/10000... Step: 69000... Loss: 0.467255... Val Loss: 0.447307\n",
      "Epoch: 700/10000... Step: 70000... Loss: 0.486951... Val Loss: 0.447303\n",
      "Epoch: 710/10000... Step: 71000... Loss: 0.443076... Val Loss: 0.447307\n",
      "Epoch: 720/10000... Step: 72000... Loss: 0.486347... Val Loss: 0.447347\n",
      "Epoch: 730/10000... Step: 73000... Loss: 0.465372... Val Loss: 0.447507\n",
      "Epoch: 740/10000... Step: 74000... Loss: 0.460639... Val Loss: 0.447357\n",
      "Epoch: 750/10000... Step: 75000... Loss: 0.430029... Val Loss: 0.447303\n",
      "Epoch: 760/10000... Step: 76000... Loss: 0.431747... Val Loss: 0.447312\n",
      "Epoch: 770/10000... Step: 77000... Loss: 0.479899... Val Loss: 0.447350\n",
      "Epoch: 780/10000... Step: 78000... Loss: 0.434943... Val Loss: 0.447306\n",
      "Epoch: 790/10000... Step: 79000... Loss: 0.436513... Val Loss: 0.447310\n",
      "Epoch: 800/10000... Step: 80000... Loss: 0.457546... Val Loss: 0.447314\n",
      "Epoch: 810/10000... Step: 81000... Loss: 0.451150... Val Loss: 0.447317\n",
      "Epoch: 820/10000... Step: 82000... Loss: 0.433156... Val Loss: 0.447325\n",
      "Epoch: 830/10000... Step: 83000... Loss: 0.439714... Val Loss: 0.447303\n",
      "Epoch: 840/10000... Step: 84000... Loss: 0.446306... Val Loss: 0.447317\n",
      "Epoch: 850/10000... Step: 85000... Loss: 0.468852... Val Loss: 0.447315\n",
      "Epoch: 860/10000... Step: 86000... Loss: 0.446351... Val Loss: 0.447374\n",
      "Epoch: 870/10000... Step: 87000... Loss: 0.430295... Val Loss: 0.447308\n",
      "Epoch: 880/10000... Step: 88000... Loss: 0.468677... Val Loss: 0.447325\n",
      "Epoch: 890/10000... Step: 89000... Loss: 0.474359... Val Loss: 0.447350\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f3cd4e146227>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2076\u001b[0m     return torch._C._nn.binary_cross_entropy(\n\u001b[0;32m-> 2077\u001b[0;31m         input, target, weight, reduction_enum)\n\u001b[0m\u001b[1;32m   2078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "counter = 0\n",
    "print_every = 1000\n",
    "clip = .5\n",
    "valid_loss_min = np.Inf\n",
    "val_loss = torch.tensor(np.Inf)\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(train_batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = train_criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            val_h = model.init_hidden(val_batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in val_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp, val_h)\n",
    "                val_loss = val_criterion(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './model_1face.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)\n",
    "    scheduler.step(val_loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing original with history = history\n",
    "adding all previous fc with (64, 32) and (32,16) all activated with ELU\n",
    "0.75 dropout\n",
    "0.0001 LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DFDCNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, drop_prob=0.75):\n",
    "        super(DFDCNet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.75)\n",
    "        self.batchnorm = nn.BatchNorm1d(hidden_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.float()\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "#         out = self.batchnorm(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.elu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.elu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.elu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFDCNet(\n",
      "  (lstm): LSTM(512, 128, num_layers=5, batch_first=True, dropout=0.75)\n",
      "  (dropout): Dropout(p=0.75, inplace=False)\n",
      "  (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (elu): ELU(alpha=1.0)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 512\n",
    "output_size = 1\n",
    "hidden_dim = 128\n",
    "n_layers = 5\n",
    "\n",
    "model = DFDCNet(input_size, output_size, hidden_dim, n_layers)\n",
    "model.to(device)\n",
    "train_criterion = nn.BCELoss()\n",
    "val_criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=500, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/10000... Step: 1000... Loss: 0.430252... Val Loss: 0.447322\n",
      "Validation loss decreased (inf --> 0.447322).  Saving model ...\n",
      "Epoch: 20/10000... Step: 2000... Loss: 0.439899... Val Loss: 0.447304\n",
      "Validation loss decreased (0.447322 --> 0.447304).  Saving model ...\n",
      "Epoch: 30/10000... Step: 3000... Loss: 0.458890... Val Loss: 0.436724\n",
      "Validation loss decreased (0.447304 --> 0.436724).  Saving model ...\n",
      "Epoch: 40/10000... Step: 4000... Loss: 0.462353... Val Loss: 0.427261\n",
      "Validation loss decreased (0.436724 --> 0.427261).  Saving model ...\n",
      "Epoch: 50/10000... Step: 5000... Loss: 0.407788... Val Loss: 0.426447\n",
      "Validation loss decreased (0.427261 --> 0.426447).  Saving model ...\n",
      "Epoch: 60/10000... Step: 6000... Loss: 0.452861... Val Loss: 0.422623\n",
      "Validation loss decreased (0.426447 --> 0.422623).  Saving model ...\n",
      "Epoch: 70/10000... Step: 7000... Loss: 0.443603... Val Loss: 0.420585\n",
      "Validation loss decreased (0.422623 --> 0.420585).  Saving model ...\n",
      "Epoch: 80/10000... Step: 8000... Loss: 0.392853... Val Loss: 0.421309\n",
      "Epoch: 90/10000... Step: 9000... Loss: 0.421360... Val Loss: 0.417611\n",
      "Validation loss decreased (0.420585 --> 0.417611).  Saving model ...\n",
      "Epoch: 100/10000... Step: 10000... Loss: 0.410814... Val Loss: 0.413896\n",
      "Validation loss decreased (0.417611 --> 0.413896).  Saving model ...\n",
      "Epoch: 110/10000... Step: 11000... Loss: 0.411617... Val Loss: 0.411233\n",
      "Validation loss decreased (0.413896 --> 0.411233).  Saving model ...\n",
      "Epoch: 120/10000... Step: 12000... Loss: 0.374878... Val Loss: 0.411252\n",
      "Epoch: 130/10000... Step: 13000... Loss: 0.413377... Val Loss: 0.406159\n",
      "Validation loss decreased (0.411233 --> 0.406159).  Saving model ...\n",
      "Epoch: 140/10000... Step: 14000... Loss: 0.422499... Val Loss: 0.405852\n",
      "Validation loss decreased (0.406159 --> 0.405852).  Saving model ...\n",
      "Epoch: 150/10000... Step: 15000... Loss: 0.385006... Val Loss: 0.398419\n",
      "Validation loss decreased (0.405852 --> 0.398419).  Saving model ...\n",
      "Epoch: 160/10000... Step: 16000... Loss: 0.389149... Val Loss: 0.398135\n",
      "Validation loss decreased (0.398419 --> 0.398135).  Saving model ...\n",
      "Epoch: 170/10000... Step: 17000... Loss: 0.383724... Val Loss: 0.392541\n",
      "Validation loss decreased (0.398135 --> 0.392541).  Saving model ...\n",
      "Epoch: 200/10000... Step: 20000... Loss: 0.421625... Val Loss: 0.391716\n",
      "Validation loss decreased (0.392408 --> 0.391716).  Saving model ...\n",
      "Epoch: 210/10000... Step: 21000... Loss: 0.389688... Val Loss: 0.388590\n",
      "Validation loss decreased (0.391716 --> 0.388590).  Saving model ...\n",
      "Epoch: 220/10000... Step: 22000... Loss: 0.353296... Val Loss: 0.385726\n",
      "Validation loss decreased (0.388590 --> 0.385726).  Saving model ...\n",
      "Epoch: 230/10000... Step: 23000... Loss: 0.396122... Val Loss: 0.393916\n",
      "Epoch: 240/10000... Step: 24000... Loss: 0.346041... Val Loss: 0.387280\n",
      "Epoch: 250/10000... Step: 25000... Loss: 0.369064... Val Loss: 0.381113\n",
      "Validation loss decreased (0.385726 --> 0.381113).  Saving model ...\n",
      "Epoch: 260/10000... Step: 26000... Loss: 0.359813... Val Loss: 0.385577\n",
      "Epoch: 270/10000... Step: 27000... Loss: 0.386962... Val Loss: 0.383931\n",
      "Epoch: 280/10000... Step: 28000... Loss: 0.381243... Val Loss: 0.379984\n",
      "Validation loss decreased (0.381113 --> 0.379984).  Saving model ...\n",
      "Epoch: 290/10000... Step: 29000... Loss: 0.367495... Val Loss: 0.380091\n",
      "Epoch: 300/10000... Step: 30000... Loss: 0.359858... Val Loss: 0.378366\n",
      "Validation loss decreased (0.379984 --> 0.378366).  Saving model ...\n",
      "Epoch: 310/10000... Step: 31000... Loss: 0.380727... Val Loss: 0.379586\n",
      "Epoch: 320/10000... Step: 32000... Loss: 0.403720... Val Loss: 0.386060\n",
      "Epoch: 330/10000... Step: 33000... Loss: 0.390766... Val Loss: 0.374331\n",
      "Validation loss decreased (0.378366 --> 0.374331).  Saving model ...\n",
      "Epoch: 340/10000... Step: 34000... Loss: 0.338803... Val Loss: 0.376569\n",
      "Epoch: 350/10000... Step: 35000... Loss: 0.333109... Val Loss: 0.385256\n",
      "Epoch: 360/10000... Step: 36000... Loss: 0.362142... Val Loss: 0.375361\n",
      "Epoch: 370/10000... Step: 37000... Loss: 0.387759... Val Loss: 0.376811\n",
      "Epoch: 380/10000... Step: 38000... Loss: 0.326428... Val Loss: 0.366557\n",
      "Validation loss decreased (0.374331 --> 0.366557).  Saving model ...\n",
      "Epoch: 390/10000... Step: 39000... Loss: 0.331318... Val Loss: 0.370577\n",
      "Epoch: 400/10000... Step: 40000... Loss: 0.359859... Val Loss: 0.368399\n",
      "Epoch: 410/10000... Step: 41000... Loss: 0.319218... Val Loss: 0.371910\n",
      "Epoch: 420/10000... Step: 42000... Loss: 0.352386... Val Loss: 0.369807\n",
      "Epoch: 430/10000... Step: 43000... Loss: 0.362258... Val Loss: 0.369479\n",
      "Epoch: 440/10000... Step: 44000... Loss: 0.336641... Val Loss: 0.368016\n",
      "Epoch: 450/10000... Step: 45000... Loss: 0.346039... Val Loss: 0.372568\n",
      "Epoch: 460/10000... Step: 46000... Loss: 0.345641... Val Loss: 0.362686\n",
      "Validation loss decreased (0.366557 --> 0.362686).  Saving model ...\n",
      "Epoch: 470/10000... Step: 47000... Loss: 0.331676... Val Loss: 0.363561\n",
      "Epoch: 480/10000... Step: 48000... Loss: 0.331632... Val Loss: 0.362772\n",
      "Epoch: 500/10000... Step: 50000... Loss: 0.341402... Val Loss: 0.365771\n",
      "Epoch: 510/10000... Step: 51000... Loss: 0.346051... Val Loss: 0.353659\n",
      "Validation loss decreased (0.359133 --> 0.353659).  Saving model ...\n",
      "Epoch: 520/10000... Step: 52000... Loss: 0.338369... Val Loss: 0.363178\n",
      "Epoch: 530/10000... Step: 53000... Loss: 0.345020... Val Loss: 0.359517\n",
      "Epoch: 540/10000... Step: 54000... Loss: 0.347343... Val Loss: 0.360685\n",
      "Epoch: 550/10000... Step: 55000... Loss: 0.330390... Val Loss: 0.356400\n",
      "Epoch: 560/10000... Step: 56000... Loss: 0.368111... Val Loss: 0.360639\n",
      "Epoch: 570/10000... Step: 57000... Loss: 0.311756... Val Loss: 0.361951\n",
      "Epoch: 580/10000... Step: 58000... Loss: 0.308261... Val Loss: 0.349107\n",
      "Validation loss decreased (0.353659 --> 0.349107).  Saving model ...\n",
      "Epoch: 590/10000... Step: 59000... Loss: 0.300661... Val Loss: 0.358703\n",
      "Epoch: 600/10000... Step: 60000... Loss: 0.304035... Val Loss: 0.356015\n",
      "Epoch: 610/10000... Step: 61000... Loss: 0.301104... Val Loss: 0.356651\n",
      "Epoch: 620/10000... Step: 62000... Loss: 0.282801... Val Loss: 0.358723\n",
      "Epoch: 630/10000... Step: 63000... Loss: 0.315117... Val Loss: 0.361255\n",
      "Epoch: 640/10000... Step: 64000... Loss: 0.317151... Val Loss: 0.354133\n",
      "Epoch: 650/10000... Step: 65000... Loss: 0.279014... Val Loss: 0.356902\n",
      "Epoch: 660/10000... Step: 66000... Loss: 0.312321... Val Loss: 0.360736\n",
      "Epoch: 670/10000... Step: 67000... Loss: 0.318014... Val Loss: 0.356707\n",
      "Epoch: 680/10000... Step: 68000... Loss: 0.310343... Val Loss: 0.360240\n",
      "Epoch: 690/10000... Step: 69000... Loss: 0.278476... Val Loss: 0.359868\n",
      "Epoch: 700/10000... Step: 70000... Loss: 0.278672... Val Loss: 0.359174\n",
      "Epoch: 710/10000... Step: 71000... Loss: 0.281625... Val Loss: 0.366466\n",
      "Epoch: 720/10000... Step: 72000... Loss: 0.282939... Val Loss: 0.355945\n",
      "Epoch: 730/10000... Step: 73000... Loss: 0.274019... Val Loss: 0.355361\n",
      "Epoch: 740/10000... Step: 74000... Loss: 0.267954... Val Loss: 0.351315\n",
      "Epoch: 750/10000... Step: 75000... Loss: 0.258982... Val Loss: 0.357746\n",
      "Epoch: 760/10000... Step: 76000... Loss: 0.287863... Val Loss: 0.358236\n",
      "Epoch: 770/10000... Step: 77000... Loss: 0.277343... Val Loss: 0.356846\n",
      "Epoch: 780/10000... Step: 78000... Loss: 0.264853... Val Loss: 0.355712\n",
      "Epoch: 790/10000... Step: 79000... Loss: 0.270935... Val Loss: 0.367894\n",
      "Epoch: 800/10000... Step: 80000... Loss: 0.264953... Val Loss: 0.350637\n",
      "Epoch: 810/10000... Step: 81000... Loss: 0.257604... Val Loss: 0.356413\n",
      "Epoch: 820/10000... Step: 82000... Loss: 0.286523... Val Loss: 0.349546\n",
      "Epoch: 830/10000... Step: 83000... Loss: 0.248689... Val Loss: 0.351673\n",
      "Epoch: 840/10000... Step: 84000... Loss: 0.260393... Val Loss: 0.362290\n",
      "Epoch: 870/10000... Step: 87000... Loss: 0.264232... Val Loss: 0.360160\n",
      "Epoch: 880/10000... Step: 88000... Loss: 0.303077... Val Loss: 0.347878\n",
      "Validation loss decreased (0.349107 --> 0.347878).  Saving model ...\n",
      "Epoch: 890/10000... Step: 89000... Loss: 0.262185... Val Loss: 0.357131\n",
      "Epoch: 900/10000... Step: 90000... Loss: 0.279036... Val Loss: 0.359178\n",
      "Epoch: 910/10000... Step: 91000... Loss: 0.295051... Val Loss: 0.365510\n",
      "Epoch: 920/10000... Step: 92000... Loss: 0.257733... Val Loss: 0.357037\n",
      "Epoch: 930/10000... Step: 93000... Loss: 0.270839... Val Loss: 0.344297\n",
      "Validation loss decreased (0.347878 --> 0.344297).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 940/10000... Step: 94000... Loss: 0.289788... Val Loss: 0.353235\n",
      "Epoch: 950/10000... Step: 95000... Loss: 0.299812... Val Loss: 0.353058\n",
      "Epoch: 960/10000... Step: 96000... Loss: 0.253406... Val Loss: 0.361294\n",
      "Epoch: 970/10000... Step: 97000... Loss: 0.277938... Val Loss: 0.354397\n",
      "Epoch: 980/10000... Step: 98000... Loss: 0.235850... Val Loss: 0.355755\n",
      "Epoch: 990/10000... Step: 99000... Loss: 0.306208... Val Loss: 0.362413\n",
      "Epoch: 1000/10000... Step: 100000... Loss: 0.265116... Val Loss: 0.353987\n",
      "Epoch: 1010/10000... Step: 101000... Loss: 0.294690... Val Loss: 0.352512\n",
      "Epoch: 1020/10000... Step: 102000... Loss: 0.261689... Val Loss: 0.361977\n",
      "Epoch: 1030/10000... Step: 103000... Loss: 0.243460... Val Loss: 0.359183\n",
      "Epoch: 1040/10000... Step: 104000... Loss: 0.251935... Val Loss: 0.345764\n",
      "Epoch: 1050/10000... Step: 105000... Loss: 0.198813... Val Loss: 0.358961\n",
      "Epoch: 1060/10000... Step: 106000... Loss: 0.253325... Val Loss: 0.363961\n",
      "Epoch: 1070/10000... Step: 107000... Loss: 0.215702... Val Loss: 0.373101\n",
      "Epoch: 1080/10000... Step: 108000... Loss: 0.266191... Val Loss: 0.355577\n",
      "Epoch: 1090/10000... Step: 109000... Loss: 0.244610... Val Loss: 0.358098\n",
      "Epoch: 1100/10000... Step: 110000... Loss: 0.226952... Val Loss: 0.354635\n",
      "Epoch: 1110/10000... Step: 111000... Loss: 0.222277... Val Loss: 0.355435\n",
      "Epoch: 1120/10000... Step: 112000... Loss: 0.239245... Val Loss: 0.364193\n",
      "Epoch: 1130/10000... Step: 113000... Loss: 0.203203... Val Loss: 0.345143\n",
      "Epoch: 1140/10000... Step: 114000... Loss: 0.233072... Val Loss: 0.353367\n",
      "Epoch: 1150/10000... Step: 115000... Loss: 0.222312... Val Loss: 0.360926\n",
      "Epoch: 1160/10000... Step: 116000... Loss: 0.214646... Val Loss: 0.360550\n",
      "Epoch: 1170/10000... Step: 117000... Loss: 0.258490... Val Loss: 0.375185\n",
      "Epoch: 1200/10000... Step: 120000... Loss: 0.253469... Val Loss: 0.345230\n",
      "Epoch: 1210/10000... Step: 121000... Loss: 0.251022... Val Loss: 0.363712\n",
      "Epoch: 1220/10000... Step: 122000... Loss: 0.215112... Val Loss: 0.371840\n",
      "Epoch: 1230/10000... Step: 123000... Loss: 0.254935... Val Loss: 0.352870\n",
      "Epoch: 1260/10000... Step: 126000... Loss: 0.242327... Val Loss: 0.352952\n",
      "Epoch: 1270/10000... Step: 127000... Loss: 0.236349... Val Loss: 0.356125\n",
      "Epoch: 1280/10000... Step: 128000... Loss: 0.242607... Val Loss: 0.360164\n",
      "Epoch: 1290/10000... Step: 129000... Loss: 0.273546... Val Loss: 0.357867\n",
      "Epoch: 1300/10000... Step: 130000... Loss: 0.265950... Val Loss: 0.371801\n",
      "Epoch: 1310/10000... Step: 131000... Loss: 0.204137... Val Loss: 0.355794\n",
      "Epoch: 1320/10000... Step: 132000... Loss: 0.222497... Val Loss: 0.361866\n",
      "Epoch: 1330/10000... Step: 133000... Loss: 0.217711... Val Loss: 0.358902\n",
      "Epoch: 1340/10000... Step: 134000... Loss: 0.225580... Val Loss: 0.362817\n",
      "Epoch: 1350/10000... Step: 135000... Loss: 0.197097... Val Loss: 0.354942\n",
      "Epoch: 1360/10000... Step: 136000... Loss: 0.222523... Val Loss: 0.367329\n",
      "Epoch: 1370/10000... Step: 137000... Loss: 0.262256... Val Loss: 0.356175\n",
      "Epoch: 1380/10000... Step: 138000... Loss: 0.241620... Val Loss: 0.364855\n",
      "Epoch: 1390/10000... Step: 139000... Loss: 0.226180... Val Loss: 0.359263\n",
      "Epoch: 1400/10000... Step: 140000... Loss: 0.223947... Val Loss: 0.342773\n",
      "Epoch: 1410/10000... Step: 141000... Loss: 0.223562... Val Loss: 0.367958\n",
      "Epoch: 1420/10000... Step: 142000... Loss: 0.225962... Val Loss: 0.353042\n",
      "Epoch: 1430/10000... Step: 143000... Loss: 0.222073... Val Loss: 0.371093\n",
      "Epoch: 1440/10000... Step: 144000... Loss: 0.216155... Val Loss: 0.345537\n",
      "Epoch: 1450/10000... Step: 145000... Loss: 0.227437... Val Loss: 0.353536\n",
      "Epoch: 1460/10000... Step: 146000... Loss: 0.204095... Val Loss: 0.363081\n",
      "Epoch: 1470/10000... Step: 147000... Loss: 0.261070... Val Loss: 0.386190\n",
      "Epoch: 1480/10000... Step: 148000... Loss: 0.207704... Val Loss: 0.371257\n",
      "Epoch: 1490/10000... Step: 149000... Loss: 0.226797... Val Loss: 0.358721\n",
      "Epoch: 1500/10000... Step: 150000... Loss: 0.255467... Val Loss: 0.359932\n",
      "Epoch: 1510/10000... Step: 151000... Loss: 0.201246... Val Loss: 0.355572\n",
      "Epoch: 1540/10000... Step: 154000... Loss: 0.220546... Val Loss: 0.361991\n",
      "Epoch: 1550/10000... Step: 155000... Loss: 0.217198... Val Loss: 0.362230\n",
      "Epoch: 1560/10000... Step: 156000... Loss: 0.214310... Val Loss: 0.354782\n",
      "Epoch: 1570/10000... Step: 157000... Loss: 0.182328... Val Loss: 0.366668\n",
      "Epoch: 1580/10000... Step: 158000... Loss: 0.215694... Val Loss: 0.361932\n",
      "Epoch: 1590/10000... Step: 159000... Loss: 0.195745... Val Loss: 0.375019\n",
      "Epoch: 1600/10000... Step: 160000... Loss: 0.169710... Val Loss: 0.365049\n",
      "Epoch: 1610/10000... Step: 161000... Loss: 0.223100... Val Loss: 0.367448\n",
      "Epoch: 1620/10000... Step: 162000... Loss: 0.199755... Val Loss: 0.355616\n",
      "Epoch: 1630/10000... Step: 163000... Loss: 0.192852... Val Loss: 0.363241\n",
      "Epoch: 1640/10000... Step: 164000... Loss: 0.183285... Val Loss: 0.364585\n",
      "Epoch: 1650/10000... Step: 165000... Loss: 0.201584... Val Loss: 0.369127\n",
      "Epoch: 1660/10000... Step: 166000... Loss: 0.215985... Val Loss: 0.366878\n",
      "Epoch: 1670/10000... Step: 167000... Loss: 0.207996... Val Loss: 0.364344\n",
      "Epoch: 1680/10000... Step: 168000... Loss: 0.200433... Val Loss: 0.371341\n",
      "Epoch: 1690/10000... Step: 169000... Loss: 0.140130... Val Loss: 0.367289\n",
      "Epoch: 1700/10000... Step: 170000... Loss: 0.200774... Val Loss: 0.366960\n",
      "Epoch: 1710/10000... Step: 171000... Loss: 0.191918... Val Loss: 0.379764\n",
      "Epoch: 1720/10000... Step: 172000... Loss: 0.156150... Val Loss: 0.378982\n",
      "Epoch: 1730/10000... Step: 173000... Loss: 0.208051... Val Loss: 0.379367\n",
      "Epoch: 1740/10000... Step: 174000... Loss: 0.191955... Val Loss: 0.377488\n",
      "Epoch: 1750/10000... Step: 175000... Loss: 0.182122... Val Loss: 0.371004\n",
      "Epoch: 1760/10000... Step: 176000... Loss: 0.204418... Val Loss: 0.378469\n",
      "Epoch: 1770/10000... Step: 177000... Loss: 0.216253... Val Loss: 0.369011\n",
      "Epoch: 1780/10000... Step: 178000... Loss: 0.168240... Val Loss: 0.360893\n",
      "Epoch: 1790/10000... Step: 179000... Loss: 0.196095... Val Loss: 0.374721\n",
      "Epoch: 1800/10000... Step: 180000... Loss: 0.166927... Val Loss: 0.368562\n",
      "Epoch: 1810/10000... Step: 181000... Loss: 0.207559... Val Loss: 0.356086\n",
      "Epoch: 1820/10000... Step: 182000... Loss: 0.169485... Val Loss: 0.374109\n",
      "Epoch: 1830/10000... Step: 183000... Loss: 0.199287... Val Loss: 0.369985\n",
      "Epoch: 1840/10000... Step: 184000... Loss: 0.221441... Val Loss: 0.381014\n",
      "Epoch: 1850/10000... Step: 185000... Loss: 0.195392... Val Loss: 0.374698\n",
      "Epoch: 1860/10000... Step: 186000... Loss: 0.177547... Val Loss: 0.375001\n",
      "Epoch: 1870/10000... Step: 187000... Loss: 0.185024... Val Loss: 0.375484\n",
      "Epoch: 1880/10000... Step: 188000... Loss: 0.172701... Val Loss: 0.364377\n",
      "Epoch: 1890/10000... Step: 189000... Loss: 0.168467... Val Loss: 0.374281\n",
      "Epoch: 1900/10000... Step: 190000... Loss: 0.173699... Val Loss: 0.390490\n",
      "Epoch: 1910/10000... Step: 191000... Loss: 0.194793... Val Loss: 0.384339\n",
      "Epoch: 1940/10000... Step: 194000... Loss: 0.205069... Val Loss: 0.375934\n",
      "Epoch: 1950/10000... Step: 195000... Loss: 0.163575... Val Loss: 0.364735\n",
      "Epoch: 1960/10000... Step: 196000... Loss: 0.176032... Val Loss: 0.373127\n",
      "Epoch: 1970/10000... Step: 197000... Loss: 0.189342... Val Loss: 0.379388\n",
      "Epoch: 1980/10000... Step: 198000... Loss: 0.181961... Val Loss: 0.392624\n",
      "Epoch: 1990/10000... Step: 199000... Loss: 0.193950... Val Loss: 0.388638\n",
      "Epoch: 2000/10000... Step: 200000... Loss: 0.183204... Val Loss: 0.391084\n",
      "Epoch: 2010/10000... Step: 201000... Loss: 0.166967... Val Loss: 0.381672\n",
      "Epoch: 2020/10000... Step: 202000... Loss: 0.157635... Val Loss: 0.384113\n",
      "Epoch: 2030/10000... Step: 203000... Loss: 0.155750... Val Loss: 0.378295\n",
      "Epoch  2031: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch: 2040/10000... Step: 204000... Loss: 0.192196... Val Loss: 0.401847\n",
      "Epoch: 2050/10000... Step: 205000... Loss: 0.146695... Val Loss: 0.389671\n",
      "Epoch: 2060/10000... Step: 206000... Loss: 0.219354... Val Loss: 0.380152\n",
      "Epoch: 2070/10000... Step: 207000... Loss: 0.132896... Val Loss: 0.386062\n",
      "Epoch: 2080/10000... Step: 208000... Loss: 0.143881... Val Loss: 0.393828\n",
      "Epoch: 2110/10000... Step: 211000... Loss: 0.162712... Val Loss: 0.386000\n",
      "Epoch: 2120/10000... Step: 212000... Loss: 0.177114... Val Loss: 0.410697\n",
      "Epoch: 2130/10000... Step: 213000... Loss: 0.151094... Val Loss: 0.396078\n",
      "Epoch: 2140/10000... Step: 214000... Loss: 0.162211... Val Loss: 0.401983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2150/10000... Step: 215000... Loss: 0.168736... Val Loss: 0.399681\n",
      "Epoch: 2160/10000... Step: 216000... Loss: 0.186802... Val Loss: 0.394652\n",
      "Epoch: 2170/10000... Step: 217000... Loss: 0.143316... Val Loss: 0.391868\n",
      "Epoch: 2180/10000... Step: 218000... Loss: 0.162085... Val Loss: 0.397076\n",
      "Epoch: 2190/10000... Step: 219000... Loss: 0.161891... Val Loss: 0.405701\n",
      "Epoch: 2200/10000... Step: 220000... Loss: 0.162330... Val Loss: 0.392737\n",
      "Epoch: 2210/10000... Step: 221000... Loss: 0.175962... Val Loss: 0.403038\n",
      "Epoch: 2220/10000... Step: 222000... Loss: 0.180720... Val Loss: 0.400249\n",
      "Epoch: 2230/10000... Step: 223000... Loss: 0.186848... Val Loss: 0.408999\n",
      "Epoch: 2240/10000... Step: 224000... Loss: 0.140682... Val Loss: 0.394735\n",
      "Epoch: 2250/10000... Step: 225000... Loss: 0.179507... Val Loss: 0.389893\n",
      "Epoch: 2260/10000... Step: 226000... Loss: 0.177151... Val Loss: 0.399124\n",
      "Epoch: 2270/10000... Step: 227000... Loss: 0.166680... Val Loss: 0.404541\n",
      "Epoch: 2280/10000... Step: 228000... Loss: 0.183300... Val Loss: 0.418638\n",
      "Epoch: 2290/10000... Step: 229000... Loss: 0.166337... Val Loss: 0.397389\n",
      "Epoch: 2300/10000... Step: 230000... Loss: 0.136995... Val Loss: 0.390861\n",
      "Epoch: 2310/10000... Step: 231000... Loss: 0.168099... Val Loss: 0.407304\n",
      "Epoch: 2320/10000... Step: 232000... Loss: 0.164248... Val Loss: 0.419403\n",
      "Epoch: 2330/10000... Step: 233000... Loss: 0.155052... Val Loss: 0.397671\n",
      "Epoch: 2340/10000... Step: 234000... Loss: 0.167739... Val Loss: 0.403410\n",
      "Epoch: 2350/10000... Step: 235000... Loss: 0.165343... Val Loss: 0.393189\n",
      "Epoch: 2360/10000... Step: 236000... Loss: 0.158867... Val Loss: 0.403530\n",
      "Epoch: 2370/10000... Step: 237000... Loss: 0.170385... Val Loss: 0.393393\n",
      "Epoch: 2380/10000... Step: 238000... Loss: 0.147797... Val Loss: 0.397878\n",
      "Epoch: 2390/10000... Step: 239000... Loss: 0.179322... Val Loss: 0.415023\n",
      "Epoch: 2400/10000... Step: 240000... Loss: 0.170769... Val Loss: 0.396200\n",
      "Epoch: 2410/10000... Step: 241000... Loss: 0.163451... Val Loss: 0.391188\n",
      "Epoch: 2420/10000... Step: 242000... Loss: 0.161320... Val Loss: 0.412020\n",
      "Epoch: 2440/10000... Step: 244000... Loss: 0.189173... Val Loss: 0.404885\n",
      "Epoch: 2450/10000... Step: 245000... Loss: 0.152026... Val Loss: 0.400192\n",
      "Epoch: 2460/10000... Step: 246000... Loss: 0.151317... Val Loss: 0.404562\n",
      "Epoch: 2470/10000... Step: 247000... Loss: 0.166919... Val Loss: 0.406520\n",
      "Epoch: 2480/10000... Step: 248000... Loss: 0.144843... Val Loss: 0.402624\n",
      "Epoch: 2490/10000... Step: 249000... Loss: 0.138534... Val Loss: 0.398063\n",
      "Epoch: 2500/10000... Step: 250000... Loss: 0.157268... Val Loss: 0.414995\n",
      "Epoch: 2510/10000... Step: 251000... Loss: 0.140394... Val Loss: 0.402989\n",
      "Epoch: 2520/10000... Step: 252000... Loss: 0.151375... Val Loss: 0.411858\n",
      "Epoch: 2530/10000... Step: 253000... Loss: 0.118124... Val Loss: 0.410053\n",
      "Epoch  2532: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch: 2540/10000... Step: 254000... Loss: 0.160427... Val Loss: 0.413927\n",
      "Epoch: 2550/10000... Step: 255000... Loss: 0.146459... Val Loss: 0.401863\n",
      "Epoch: 2560/10000... Step: 256000... Loss: 0.147211... Val Loss: 0.402179\n",
      "Epoch: 2570/10000... Step: 257000... Loss: 0.128979... Val Loss: 0.417975\n",
      "Epoch: 2580/10000... Step: 258000... Loss: 0.143952... Val Loss: 0.410781\n",
      "Epoch: 2590/10000... Step: 259000... Loss: 0.155177... Val Loss: 0.410827\n",
      "Epoch: 2600/10000... Step: 260000... Loss: 0.158166... Val Loss: 0.430116\n",
      "Epoch: 2610/10000... Step: 261000... Loss: 0.181897... Val Loss: 0.413013\n",
      "Epoch: 2620/10000... Step: 262000... Loss: 0.137502... Val Loss: 0.413508\n",
      "Epoch: 2630/10000... Step: 263000... Loss: 0.168872... Val Loss: 0.436740\n",
      "Epoch: 2640/10000... Step: 264000... Loss: 0.179850... Val Loss: 0.401709\n",
      "Epoch: 2650/10000... Step: 265000... Loss: 0.133253... Val Loss: 0.403380\n",
      "Epoch: 2660/10000... Step: 266000... Loss: 0.166178... Val Loss: 0.413250\n",
      "Epoch: 2670/10000... Step: 267000... Loss: 0.133181... Val Loss: 0.427004\n",
      "Epoch: 2680/10000... Step: 268000... Loss: 0.183603... Val Loss: 0.409343\n",
      "Epoch: 2690/10000... Step: 269000... Loss: 0.134621... Val Loss: 0.412833\n",
      "Epoch: 2700/10000... Step: 270000... Loss: 0.148942... Val Loss: 0.420300\n",
      "Epoch: 2710/10000... Step: 271000... Loss: 0.114516... Val Loss: 0.418548\n",
      "Epoch: 2720/10000... Step: 272000... Loss: 0.148159... Val Loss: 0.418019\n",
      "Epoch: 2730/10000... Step: 273000... Loss: 0.134762... Val Loss: 0.419927\n",
      "Epoch: 2740/10000... Step: 274000... Loss: 0.144254... Val Loss: 0.425251\n",
      "Epoch: 2750/10000... Step: 275000... Loss: 0.158065... Val Loss: 0.427277\n",
      "Epoch: 2760/10000... Step: 276000... Loss: 0.147513... Val Loss: 0.408327\n",
      "Epoch: 2770/10000... Step: 277000... Loss: 0.171193... Val Loss: 0.425577\n",
      "Epoch: 2780/10000... Step: 278000... Loss: 0.122563... Val Loss: 0.429449\n",
      "Epoch: 2790/10000... Step: 279000... Loss: 0.166896... Val Loss: 0.430353\n",
      "Epoch: 2800/10000... Step: 280000... Loss: 0.158112... Val Loss: 0.417766\n",
      "Epoch: 2810/10000... Step: 281000... Loss: 0.137927... Val Loss: 0.430260\n",
      "Epoch: 2820/10000... Step: 282000... Loss: 0.149044... Val Loss: 0.402889\n",
      "Epoch: 2830/10000... Step: 283000... Loss: 0.145275... Val Loss: 0.408711\n",
      "Epoch: 2840/10000... Step: 284000... Loss: 0.166102... Val Loss: 0.423234\n",
      "Epoch: 2850/10000... Step: 285000... Loss: 0.146017... Val Loss: 0.427001\n",
      "Epoch: 2860/10000... Step: 286000... Loss: 0.143655... Val Loss: 0.423917\n",
      "Epoch: 2870/10000... Step: 287000... Loss: 0.139185... Val Loss: 0.427582\n",
      "Epoch: 2880/10000... Step: 288000... Loss: 0.112481... Val Loss: 0.411430\n",
      "Epoch: 2890/10000... Step: 289000... Loss: 0.130414... Val Loss: 0.426938\n",
      "Epoch: 2900/10000... Step: 290000... Loss: 0.177134... Val Loss: 0.431044\n",
      "Epoch: 2910/10000... Step: 291000... Loss: 0.141793... Val Loss: 0.421690\n",
      "Epoch: 2920/10000... Step: 292000... Loss: 0.131163... Val Loss: 0.426468\n",
      "Epoch: 2930/10000... Step: 293000... Loss: 0.158457... Val Loss: 0.431541\n",
      "Epoch: 2940/10000... Step: 294000... Loss: 0.134256... Val Loss: 0.420046\n",
      "Epoch: 2950/10000... Step: 295000... Loss: 0.176187... Val Loss: 0.416552\n",
      "Epoch: 2960/10000... Step: 296000... Loss: 0.137726... Val Loss: 0.420407\n",
      "Epoch: 2970/10000... Step: 297000... Loss: 0.123169... Val Loss: 0.429121\n",
      "Epoch: 2980/10000... Step: 298000... Loss: 0.147142... Val Loss: 0.416077\n",
      "Epoch: 2990/10000... Step: 299000... Loss: 0.167520... Val Loss: 0.414242\n",
      "Epoch: 3000/10000... Step: 300000... Loss: 0.137052... Val Loss: 0.408707\n",
      "Epoch: 3030/10000... Step: 303000... Loss: 0.108646... Val Loss: 0.421638\n",
      "Epoch  3033: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch: 3040/10000... Step: 304000... Loss: 0.127176... Val Loss: 0.424228\n",
      "Epoch: 3060/10000... Step: 306000... Loss: 0.122250... Val Loss: 0.416590\n",
      "Epoch: 3070/10000... Step: 307000... Loss: 0.141995... Val Loss: 0.418276\n",
      "Epoch: 3080/10000... Step: 308000... Loss: 0.137015... Val Loss: 0.412972\n",
      "Epoch: 3090/10000... Step: 309000... Loss: 0.128862... Val Loss: 0.421701\n",
      "Epoch: 3100/10000... Step: 310000... Loss: 0.156752... Val Loss: 0.426173\n",
      "Epoch: 3110/10000... Step: 311000... Loss: 0.158041... Val Loss: 0.421612\n",
      "Epoch: 3120/10000... Step: 312000... Loss: 0.145705... Val Loss: 0.429407\n",
      "Epoch: 3130/10000... Step: 313000... Loss: 0.145534... Val Loss: 0.411051\n",
      "Epoch: 3140/10000... Step: 314000... Loss: 0.172373... Val Loss: 0.434818\n",
      "Epoch: 3150/10000... Step: 315000... Loss: 0.147870... Val Loss: 0.418770\n",
      "Epoch: 3160/10000... Step: 316000... Loss: 0.149521... Val Loss: 0.413522\n",
      "Epoch: 3170/10000... Step: 317000... Loss: 0.124591... Val Loss: 0.427733\n",
      "Epoch: 3180/10000... Step: 318000... Loss: 0.126051... Val Loss: 0.414206\n",
      "Epoch: 3190/10000... Step: 319000... Loss: 0.144595... Val Loss: 0.440301\n",
      "Epoch: 3200/10000... Step: 320000... Loss: 0.136862... Val Loss: 0.424661\n",
      "Epoch: 3210/10000... Step: 321000... Loss: 0.144671... Val Loss: 0.419637\n",
      "Epoch: 3220/10000... Step: 322000... Loss: 0.165919... Val Loss: 0.418280\n",
      "Epoch: 3230/10000... Step: 323000... Loss: 0.145942... Val Loss: 0.430910\n",
      "Epoch: 3240/10000... Step: 324000... Loss: 0.144792... Val Loss: 0.417261\n",
      "Epoch: 3250/10000... Step: 325000... Loss: 0.135508... Val Loss: 0.442262\n",
      "Epoch: 3260/10000... Step: 326000... Loss: 0.156192... Val Loss: 0.427645\n",
      "Epoch: 3270/10000... Step: 327000... Loss: 0.170849... Val Loss: 0.440018\n",
      "Epoch: 3280/10000... Step: 328000... Loss: 0.148452... Val Loss: 0.431357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3290/10000... Step: 329000... Loss: 0.160725... Val Loss: 0.435807\n",
      "Epoch: 3300/10000... Step: 330000... Loss: 0.150361... Val Loss: 0.431902\n",
      "Epoch: 3310/10000... Step: 331000... Loss: 0.110287... Val Loss: 0.444844\n",
      "Epoch: 3320/10000... Step: 332000... Loss: 0.136482... Val Loss: 0.430710\n",
      "Epoch: 3330/10000... Step: 333000... Loss: 0.154617... Val Loss: 0.432321\n",
      "Epoch: 3340/10000... Step: 334000... Loss: 0.133757... Val Loss: 0.427717\n",
      "Epoch: 3350/10000... Step: 335000... Loss: 0.147863... Val Loss: 0.428174\n",
      "Epoch: 3360/10000... Step: 336000... Loss: 0.115559... Val Loss: 0.426285\n",
      "Epoch: 3370/10000... Step: 337000... Loss: 0.147682... Val Loss: 0.424912\n",
      "Epoch: 3380/10000... Step: 338000... Loss: 0.139736... Val Loss: 0.428556\n",
      "Epoch: 3390/10000... Step: 339000... Loss: 0.154030... Val Loss: 0.433013\n",
      "Epoch: 3400/10000... Step: 340000... Loss: 0.180531... Val Loss: 0.425783\n",
      "Epoch: 3410/10000... Step: 341000... Loss: 0.148194... Val Loss: 0.433288\n",
      "Epoch: 3420/10000... Step: 342000... Loss: 0.168018... Val Loss: 0.414277\n",
      "Epoch: 3430/10000... Step: 343000... Loss: 0.110404... Val Loss: 0.425684\n",
      "Epoch: 3460/10000... Step: 346000... Loss: 0.155448... Val Loss: 0.425620\n",
      "Epoch: 3470/10000... Step: 347000... Loss: 0.110503... Val Loss: 0.432209\n",
      "Epoch: 3480/10000... Step: 348000... Loss: 0.137856... Val Loss: 0.431794\n",
      "Epoch: 3490/10000... Step: 349000... Loss: 0.155285... Val Loss: 0.428441\n",
      "Epoch: 3500/10000... Step: 350000... Loss: 0.136070... Val Loss: 0.439998\n",
      "Epoch: 3510/10000... Step: 351000... Loss: 0.145541... Val Loss: 0.424237\n",
      "Epoch: 3520/10000... Step: 352000... Loss: 0.135849... Val Loss: 0.431197\n",
      "Epoch: 3530/10000... Step: 353000... Loss: 0.136448... Val Loss: 0.432456\n",
      "Epoch: 3540/10000... Step: 354000... Loss: 0.167948... Val Loss: 0.431406\n",
      "Epoch: 3550/10000... Step: 355000... Loss: 0.135154... Val Loss: 0.435015\n",
      "Epoch: 3560/10000... Step: 356000... Loss: 0.140254... Val Loss: 0.440674\n",
      "Epoch: 3570/10000... Step: 357000... Loss: 0.125878... Val Loss: 0.438747\n",
      "Epoch: 3580/10000... Step: 358000... Loss: 0.135303... Val Loss: 0.425880\n",
      "Epoch: 3590/10000... Step: 359000... Loss: 0.153408... Val Loss: 0.424289\n",
      "Epoch: 3600/10000... Step: 360000... Loss: 0.132853... Val Loss: 0.438914\n",
      "Epoch: 3610/10000... Step: 361000... Loss: 0.134232... Val Loss: 0.426903\n",
      "Epoch: 3620/10000... Step: 362000... Loss: 0.145934... Val Loss: 0.423657\n",
      "Epoch: 3630/10000... Step: 363000... Loss: 0.128876... Val Loss: 0.430860\n",
      "Epoch: 3640/10000... Step: 364000... Loss: 0.132455... Val Loss: 0.420441\n",
      "Epoch: 3650/10000... Step: 365000... Loss: 0.147342... Val Loss: 0.436891\n",
      "Epoch: 3660/10000... Step: 366000... Loss: 0.138168... Val Loss: 0.430425\n",
      "Epoch: 3670/10000... Step: 367000... Loss: 0.147230... Val Loss: 0.433599\n",
      "Epoch: 3680/10000... Step: 368000... Loss: 0.137264... Val Loss: 0.444118\n",
      "Epoch: 3690/10000... Step: 369000... Loss: 0.134752... Val Loss: 0.424756\n",
      "Epoch: 3700/10000... Step: 370000... Loss: 0.146889... Val Loss: 0.422408\n",
      "Epoch: 3710/10000... Step: 371000... Loss: 0.145850... Val Loss: 0.431624\n",
      "Epoch: 3720/10000... Step: 372000... Loss: 0.120865... Val Loss: 0.428353\n",
      "Epoch: 3730/10000... Step: 373000... Loss: 0.147451... Val Loss: 0.419829\n",
      "Epoch: 3740/10000... Step: 374000... Loss: 0.145577... Val Loss: 0.452213\n",
      "Epoch: 3750/10000... Step: 375000... Loss: 0.126252... Val Loss: 0.442027\n",
      "Epoch: 3760/10000... Step: 376000... Loss: 0.119437... Val Loss: 0.420891\n",
      "Epoch: 3770/10000... Step: 377000... Loss: 0.126327... Val Loss: 0.421679\n",
      "Epoch: 3780/10000... Step: 378000... Loss: 0.107737... Val Loss: 0.437722\n",
      "Epoch: 3790/10000... Step: 379000... Loss: 0.143600... Val Loss: 0.420383\n",
      "Epoch: 3800/10000... Step: 380000... Loss: 0.127879... Val Loss: 0.428003\n",
      "Epoch: 3810/10000... Step: 381000... Loss: 0.149852... Val Loss: 0.431343\n",
      "Epoch: 3820/10000... Step: 382000... Loss: 0.125968... Val Loss: 0.438625\n",
      "Epoch  3821: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch: 3830/10000... Step: 383000... Loss: 0.128653... Val Loss: 0.434006\n",
      "Epoch: 3840/10000... Step: 384000... Loss: 0.134100... Val Loss: 0.434632\n",
      "Epoch: 3850/10000... Step: 385000... Loss: 0.132004... Val Loss: 0.428224\n",
      "Epoch: 3860/10000... Step: 386000... Loss: 0.122087... Val Loss: 0.428342\n",
      "Epoch: 3870/10000... Step: 387000... Loss: 0.117537... Val Loss: 0.444029\n",
      "Epoch: 3880/10000... Step: 388000... Loss: 0.177116... Val Loss: 0.428002\n",
      "Epoch: 3890/10000... Step: 389000... Loss: 0.116562... Val Loss: 0.440101\n",
      "Epoch: 3900/10000... Step: 390000... Loss: 0.119232... Val Loss: 0.437748\n",
      "Epoch: 3910/10000... Step: 391000... Loss: 0.113443... Val Loss: 0.452808\n",
      "Epoch: 3930/10000... Step: 393000... Loss: 0.118610... Val Loss: 0.434414\n",
      "Epoch: 3940/10000... Step: 394000... Loss: 0.120278... Val Loss: 0.427589\n",
      "Epoch: 3950/10000... Step: 395000... Loss: 0.134363... Val Loss: 0.430540\n",
      "Epoch: 3960/10000... Step: 396000... Loss: 0.152101... Val Loss: 0.429604\n",
      "Epoch: 3970/10000... Step: 397000... Loss: 0.139586... Val Loss: 0.431184\n",
      "Epoch: 3980/10000... Step: 398000... Loss: 0.145284... Val Loss: 0.433604\n",
      "Epoch: 3990/10000... Step: 399000... Loss: 0.168275... Val Loss: 0.432596\n",
      "Epoch: 4000/10000... Step: 400000... Loss: 0.137187... Val Loss: 0.431917\n",
      "Epoch: 4010/10000... Step: 401000... Loss: 0.129724... Val Loss: 0.437389\n",
      "Epoch: 4020/10000... Step: 402000... Loss: 0.141128... Val Loss: 0.440560\n",
      "Epoch: 4030/10000... Step: 403000... Loss: 0.129749... Val Loss: 0.433283\n",
      "Epoch: 4040/10000... Step: 404000... Loss: 0.151921... Val Loss: 0.432296\n",
      "Epoch: 4050/10000... Step: 405000... Loss: 0.193163... Val Loss: 0.420033\n",
      "Epoch: 4060/10000... Step: 406000... Loss: 0.134582... Val Loss: 0.434097\n",
      "Epoch: 4070/10000... Step: 407000... Loss: 0.131907... Val Loss: 0.444138\n",
      "Epoch: 4080/10000... Step: 408000... Loss: 0.111471... Val Loss: 0.441503\n",
      "Epoch: 4090/10000... Step: 409000... Loss: 0.120818... Val Loss: 0.425298\n",
      "Epoch: 4100/10000... Step: 410000... Loss: 0.139580... Val Loss: 0.431029\n",
      "Epoch: 4110/10000... Step: 411000... Loss: 0.132128... Val Loss: 0.437140\n",
      "Epoch: 4120/10000... Step: 412000... Loss: 0.132597... Val Loss: 0.428959\n",
      "Epoch: 4130/10000... Step: 413000... Loss: 0.144043... Val Loss: 0.432530\n",
      "Epoch: 4140/10000... Step: 414000... Loss: 0.139965... Val Loss: 0.435938\n",
      "Epoch: 4150/10000... Step: 415000... Loss: 0.127824... Val Loss: 0.443460\n",
      "Epoch: 4160/10000... Step: 416000... Loss: 0.134131... Val Loss: 0.436637\n",
      "Epoch: 4170/10000... Step: 417000... Loss: 0.132089... Val Loss: 0.444449\n",
      "Epoch: 4180/10000... Step: 418000... Loss: 0.140744... Val Loss: 0.433656\n",
      "Epoch: 4190/10000... Step: 419000... Loss: 0.116023... Val Loss: 0.434570\n",
      "Epoch: 4200/10000... Step: 420000... Loss: 0.136961... Val Loss: 0.436982\n",
      "Epoch: 4210/10000... Step: 421000... Loss: 0.134355... Val Loss: 0.439155\n",
      "Epoch: 4220/10000... Step: 422000... Loss: 0.135108... Val Loss: 0.446444\n",
      "Epoch: 4230/10000... Step: 423000... Loss: 0.176336... Val Loss: 0.429899\n",
      "Epoch: 4240/10000... Step: 424000... Loss: 0.131802... Val Loss: 0.433688\n",
      "Epoch: 4250/10000... Step: 425000... Loss: 0.141710... Val Loss: 0.419679\n",
      "Epoch: 4260/10000... Step: 426000... Loss: 0.111999... Val Loss: 0.432511\n",
      "Epoch: 4270/10000... Step: 427000... Loss: 0.121797... Val Loss: 0.433548\n",
      "Epoch: 4280/10000... Step: 428000... Loss: 0.157984... Val Loss: 0.441102\n",
      "Epoch: 4290/10000... Step: 429000... Loss: 0.112188... Val Loss: 0.429320\n",
      "Epoch: 4300/10000... Step: 430000... Loss: 0.124800... Val Loss: 0.429514\n",
      "Epoch: 4310/10000... Step: 431000... Loss: 0.161346... Val Loss: 0.431546\n",
      "Epoch: 4320/10000... Step: 432000... Loss: 0.134557... Val Loss: 0.428209\n",
      "Epoch  4322: reducing learning rate of group 0 to 3.1250e-06.\n",
      "Epoch: 4330/10000... Step: 433000... Loss: 0.126490... Val Loss: 0.440325\n",
      "Epoch: 4340/10000... Step: 434000... Loss: 0.135711... Val Loss: 0.448367\n",
      "Epoch: 4350/10000... Step: 435000... Loss: 0.121554... Val Loss: 0.440542\n",
      "Epoch: 4360/10000... Step: 436000... Loss: 0.119033... Val Loss: 0.435095\n",
      "Epoch: 4370/10000... Step: 437000... Loss: 0.111775... Val Loss: 0.440621\n",
      "Epoch: 4380/10000... Step: 438000... Loss: 0.143330... Val Loss: 0.446898\n",
      "Epoch: 4390/10000... Step: 439000... Loss: 0.125806... Val Loss: 0.441891\n",
      "Epoch: 4400/10000... Step: 440000... Loss: 0.150798... Val Loss: 0.437161\n",
      "Epoch: 4410/10000... Step: 441000... Loss: 0.130315... Val Loss: 0.447479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4420/10000... Step: 442000... Loss: 0.130694... Val Loss: 0.443877\n",
      "Epoch: 4430/10000... Step: 443000... Loss: 0.126056... Val Loss: 0.451048\n",
      "Epoch: 4440/10000... Step: 444000... Loss: 0.130782... Val Loss: 0.443154\n",
      "Epoch: 4450/10000... Step: 445000... Loss: 0.148772... Val Loss: 0.447705\n",
      "Epoch: 4460/10000... Step: 446000... Loss: 0.156762... Val Loss: 0.445241\n",
      "Epoch: 4470/10000... Step: 447000... Loss: 0.145147... Val Loss: 0.436648\n",
      "Epoch: 4480/10000... Step: 448000... Loss: 0.171561... Val Loss: 0.442056\n",
      "Epoch: 4490/10000... Step: 449000... Loss: 0.147614... Val Loss: 0.440539\n",
      "Epoch: 4500/10000... Step: 450000... Loss: 0.135337... Val Loss: 0.443540\n",
      "Epoch: 4510/10000... Step: 451000... Loss: 0.138702... Val Loss: 0.426058\n",
      "Epoch: 4520/10000... Step: 452000... Loss: 0.117202... Val Loss: 0.439614\n",
      "Epoch: 4530/10000... Step: 453000... Loss: 0.152687... Val Loss: 0.437090\n",
      "Epoch: 4540/10000... Step: 454000... Loss: 0.143674... Val Loss: 0.447561\n",
      "Epoch: 4550/10000... Step: 455000... Loss: 0.146092... Val Loss: 0.435952\n",
      "Epoch: 4560/10000... Step: 456000... Loss: 0.130158... Val Loss: 0.447912\n",
      "Epoch: 4570/10000... Step: 457000... Loss: 0.116212... Val Loss: 0.442452\n",
      "Epoch: 4580/10000... Step: 458000... Loss: 0.131776... Val Loss: 0.445498\n",
      "Epoch: 4590/10000... Step: 459000... Loss: 0.138108... Val Loss: 0.440342\n",
      "Epoch: 4600/10000... Step: 460000... Loss: 0.143175... Val Loss: 0.444365\n",
      "Epoch: 4610/10000... Step: 461000... Loss: 0.109553... Val Loss: 0.435646\n",
      "Epoch: 4620/10000... Step: 462000... Loss: 0.137147... Val Loss: 0.449111\n",
      "Epoch: 4630/10000... Step: 463000... Loss: 0.129674... Val Loss: 0.434801\n",
      "Epoch: 4640/10000... Step: 464000... Loss: 0.116739... Val Loss: 0.451419\n",
      "Epoch: 4650/10000... Step: 465000... Loss: 0.148119... Val Loss: 0.435423\n",
      "Epoch: 4660/10000... Step: 466000... Loss: 0.141109... Val Loss: 0.437954\n",
      "Epoch: 4670/10000... Step: 467000... Loss: 0.128455... Val Loss: 0.441895\n",
      "Epoch: 4680/10000... Step: 468000... Loss: 0.158707... Val Loss: 0.437713\n",
      "Epoch: 4690/10000... Step: 469000... Loss: 0.147138... Val Loss: 0.437775\n",
      "Epoch: 4700/10000... Step: 470000... Loss: 0.120078... Val Loss: 0.455922\n",
      "Epoch: 4710/10000... Step: 471000... Loss: 0.105863... Val Loss: 0.428940\n",
      "Epoch: 4720/10000... Step: 472000... Loss: 0.138665... Val Loss: 0.435741\n",
      "Epoch: 4730/10000... Step: 473000... Loss: 0.135173... Val Loss: 0.443874\n",
      "Epoch: 4740/10000... Step: 474000... Loss: 0.134364... Val Loss: 0.442914\n",
      "Epoch: 4750/10000... Step: 475000... Loss: 0.136627... Val Loss: 0.438366\n",
      "Epoch: 4760/10000... Step: 476000... Loss: 0.140067... Val Loss: 0.440781\n",
      "Epoch: 4770/10000... Step: 477000... Loss: 0.149185... Val Loss: 0.436171\n",
      "Epoch: 4780/10000... Step: 478000... Loss: 0.140437... Val Loss: 0.445615\n",
      "Epoch: 4790/10000... Step: 479000... Loss: 0.133085... Val Loss: 0.430573\n",
      "Epoch: 4800/10000... Step: 480000... Loss: 0.119192... Val Loss: 0.435798\n",
      "Epoch: 4810/10000... Step: 481000... Loss: 0.149106... Val Loss: 0.456424\n",
      "Epoch: 4820/10000... Step: 482000... Loss: 0.141268... Val Loss: 0.437748\n",
      "Epoch  4823: reducing learning rate of group 0 to 1.5625e-06.\n",
      "Epoch: 4830/10000... Step: 483000... Loss: 0.122195... Val Loss: 0.438046\n",
      "Epoch: 4840/10000... Step: 484000... Loss: 0.126003... Val Loss: 0.452211\n",
      "Epoch: 4850/10000... Step: 485000... Loss: 0.132801... Val Loss: 0.435712\n",
      "Epoch: 4860/10000... Step: 486000... Loss: 0.171727... Val Loss: 0.429116\n",
      "Epoch: 4870/10000... Step: 487000... Loss: 0.151027... Val Loss: 0.444656\n",
      "Epoch: 4880/10000... Step: 488000... Loss: 0.155042... Val Loss: 0.444214\n",
      "Epoch: 4890/10000... Step: 489000... Loss: 0.150269... Val Loss: 0.431976\n",
      "Epoch: 4900/10000... Step: 490000... Loss: 0.097824... Val Loss: 0.428549\n",
      "Epoch: 4910/10000... Step: 491000... Loss: 0.131209... Val Loss: 0.444563\n",
      "Epoch: 4920/10000... Step: 492000... Loss: 0.135600... Val Loss: 0.434611\n",
      "Epoch: 4930/10000... Step: 493000... Loss: 0.124072... Val Loss: 0.439955\n",
      "Epoch: 4940/10000... Step: 494000... Loss: 0.183191... Val Loss: 0.433968\n",
      "Epoch: 4950/10000... Step: 495000... Loss: 0.128950... Val Loss: 0.440295\n",
      "Epoch: 4960/10000... Step: 496000... Loss: 0.145427... Val Loss: 0.432616\n",
      "Epoch: 4970/10000... Step: 497000... Loss: 0.145590... Val Loss: 0.453220\n",
      "Epoch: 4980/10000... Step: 498000... Loss: 0.141286... Val Loss: 0.441991\n",
      "Epoch: 4990/10000... Step: 499000... Loss: 0.124255... Val Loss: 0.442580\n",
      "Epoch: 5000/10000... Step: 500000... Loss: 0.148774... Val Loss: 0.446906\n",
      "Epoch: 5010/10000... Step: 501000... Loss: 0.132942... Val Loss: 0.438170\n",
      "Epoch: 5020/10000... Step: 502000... Loss: 0.123465... Val Loss: 0.441737\n",
      "Epoch: 5030/10000... Step: 503000... Loss: 0.104098... Val Loss: 0.444389\n",
      "Epoch: 5040/10000... Step: 504000... Loss: 0.140349... Val Loss: 0.433207\n",
      "Epoch: 5050/10000... Step: 505000... Loss: 0.137830... Val Loss: 0.453103\n",
      "Epoch: 5060/10000... Step: 506000... Loss: 0.139679... Val Loss: 0.449033\n",
      "Epoch: 5070/10000... Step: 507000... Loss: 0.124549... Val Loss: 0.433712\n",
      "Epoch: 5080/10000... Step: 508000... Loss: 0.143096... Val Loss: 0.452212\n",
      "Epoch: 5090/10000... Step: 509000... Loss: 0.129953... Val Loss: 0.429047\n",
      "Epoch: 5100/10000... Step: 510000... Loss: 0.121574... Val Loss: 0.446426\n",
      "Epoch: 5110/10000... Step: 511000... Loss: 0.120025... Val Loss: 0.444026\n",
      "Epoch: 5120/10000... Step: 512000... Loss: 0.124016... Val Loss: 0.433103\n",
      "Epoch: 5130/10000... Step: 513000... Loss: 0.124124... Val Loss: 0.437848\n",
      "Epoch: 5140/10000... Step: 514000... Loss: 0.129804... Val Loss: 0.443475\n",
      "Epoch: 5170/10000... Step: 517000... Loss: 0.106889... Val Loss: 0.433968\n",
      "Epoch: 5180/10000... Step: 518000... Loss: 0.107823... Val Loss: 0.437816\n",
      "Epoch: 5190/10000... Step: 519000... Loss: 0.122282... Val Loss: 0.436926\n",
      "Epoch: 5200/10000... Step: 520000... Loss: 0.135396... Val Loss: 0.447164\n",
      "Epoch: 5210/10000... Step: 521000... Loss: 0.134228... Val Loss: 0.438024\n",
      "Epoch: 5220/10000... Step: 522000... Loss: 0.119177... Val Loss: 0.448996\n",
      "Epoch: 5230/10000... Step: 523000... Loss: 0.138466... Val Loss: 0.432018\n",
      "Epoch: 5240/10000... Step: 524000... Loss: 0.126455... Val Loss: 0.443625\n",
      "Epoch: 5250/10000... Step: 525000... Loss: 0.145832... Val Loss: 0.431937\n",
      "Epoch: 5260/10000... Step: 526000... Loss: 0.138076... Val Loss: 0.437383\n",
      "Epoch: 5270/10000... Step: 527000... Loss: 0.151077... Val Loss: 0.445705\n",
      "Epoch: 5280/10000... Step: 528000... Loss: 0.120609... Val Loss: 0.451452\n",
      "Epoch: 5290/10000... Step: 529000... Loss: 0.146001... Val Loss: 0.435210\n",
      "Epoch: 5300/10000... Step: 530000... Loss: 0.162053... Val Loss: 0.447314\n",
      "Epoch: 5310/10000... Step: 531000... Loss: 0.127020... Val Loss: 0.428061\n",
      "Epoch: 5320/10000... Step: 532000... Loss: 0.108844... Val Loss: 0.429873\n",
      "Epoch  5324: reducing learning rate of group 0 to 7.8125e-07.\n",
      "Epoch: 5330/10000... Step: 533000... Loss: 0.153716... Val Loss: 0.441679\n",
      "Epoch: 5340/10000... Step: 534000... Loss: 0.125862... Val Loss: 0.447047\n",
      "Epoch: 5350/10000... Step: 535000... Loss: 0.157806... Val Loss: 0.445113\n",
      "Epoch: 5360/10000... Step: 536000... Loss: 0.132442... Val Loss: 0.431321\n",
      "Epoch: 5370/10000... Step: 537000... Loss: 0.136696... Val Loss: 0.453696\n",
      "Epoch: 5380/10000... Step: 538000... Loss: 0.125182... Val Loss: 0.442984\n",
      "Epoch: 5390/10000... Step: 539000... Loss: 0.141321... Val Loss: 0.442523\n",
      "Epoch: 5400/10000... Step: 540000... Loss: 0.132735... Val Loss: 0.437755\n",
      "Epoch: 5410/10000... Step: 541000... Loss: 0.112322... Val Loss: 0.435491\n",
      "Epoch: 5420/10000... Step: 542000... Loss: 0.127619... Val Loss: 0.450963\n",
      "Epoch: 5430/10000... Step: 543000... Loss: 0.137358... Val Loss: 0.444486\n",
      "Epoch: 5440/10000... Step: 544000... Loss: 0.128770... Val Loss: 0.442839\n",
      "Epoch: 5450/10000... Step: 545000... Loss: 0.129335... Val Loss: 0.451189\n",
      "Epoch: 5460/10000... Step: 546000... Loss: 0.136461... Val Loss: 0.451711\n",
      "Epoch: 5470/10000... Step: 547000... Loss: 0.110718... Val Loss: 0.447068\n",
      "Epoch: 5480/10000... Step: 548000... Loss: 0.151269... Val Loss: 0.447888\n",
      "Epoch: 5490/10000... Step: 549000... Loss: 0.165339... Val Loss: 0.447253\n",
      "Epoch: 5500/10000... Step: 550000... Loss: 0.151939... Val Loss: 0.445680\n",
      "Epoch: 5510/10000... Step: 551000... Loss: 0.117244... Val Loss: 0.444458\n",
      "Epoch: 5520/10000... Step: 552000... Loss: 0.137908... Val Loss: 0.440899\n",
      "Epoch: 5530/10000... Step: 553000... Loss: 0.137387... Val Loss: 0.444891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5540/10000... Step: 554000... Loss: 0.116523... Val Loss: 0.441157\n",
      "Epoch: 5550/10000... Step: 555000... Loss: 0.160307... Val Loss: 0.439038\n",
      "Epoch: 5560/10000... Step: 556000... Loss: 0.158199... Val Loss: 0.442586\n",
      "Epoch: 5570/10000... Step: 557000... Loss: 0.119409... Val Loss: 0.441183\n",
      "Epoch: 5580/10000... Step: 558000... Loss: 0.151918... Val Loss: 0.435776\n",
      "Epoch: 5590/10000... Step: 559000... Loss: 0.134933... Val Loss: 0.438197\n",
      "Epoch: 5600/10000... Step: 560000... Loss: 0.132629... Val Loss: 0.445227\n",
      "Epoch: 5610/10000... Step: 561000... Loss: 0.119203... Val Loss: 0.437637\n",
      "Epoch: 5620/10000... Step: 562000... Loss: 0.112045... Val Loss: 0.436527\n",
      "Epoch: 5630/10000... Step: 563000... Loss: 0.144068... Val Loss: 0.436893\n",
      "Epoch: 5640/10000... Step: 564000... Loss: 0.145712... Val Loss: 0.451681\n",
      "Epoch: 5650/10000... Step: 565000... Loss: 0.141561... Val Loss: 0.439080\n",
      "Epoch: 5660/10000... Step: 566000... Loss: 0.131954... Val Loss: 0.443099\n",
      "Epoch: 5670/10000... Step: 567000... Loss: 0.106321... Val Loss: 0.446325\n",
      "Epoch: 5680/10000... Step: 568000... Loss: 0.127071... Val Loss: 0.438264\n",
      "Epoch: 5690/10000... Step: 569000... Loss: 0.110580... Val Loss: 0.445213\n",
      "Epoch: 5700/10000... Step: 570000... Loss: 0.115355... Val Loss: 0.434450\n",
      "Epoch: 5710/10000... Step: 571000... Loss: 0.114011... Val Loss: 0.431774\n",
      "Epoch: 5720/10000... Step: 572000... Loss: 0.154620... Val Loss: 0.447987\n",
      "Epoch: 5730/10000... Step: 573000... Loss: 0.143814... Val Loss: 0.433901\n",
      "Epoch: 5740/10000... Step: 574000... Loss: 0.137642... Val Loss: 0.440660\n",
      "Epoch: 5750/10000... Step: 575000... Loss: 0.114342... Val Loss: 0.439948\n",
      "Epoch: 5760/10000... Step: 576000... Loss: 0.123525... Val Loss: 0.438473\n",
      "Epoch: 5770/10000... Step: 577000... Loss: 0.122578... Val Loss: 0.429842\n",
      "Epoch: 5780/10000... Step: 578000... Loss: 0.124118... Val Loss: 0.444016\n",
      "Epoch: 5790/10000... Step: 579000... Loss: 0.105645... Val Loss: 0.445305\n",
      "Epoch: 5800/10000... Step: 580000... Loss: 0.115368... Val Loss: 0.433538\n",
      "Epoch: 5810/10000... Step: 581000... Loss: 0.137150... Val Loss: 0.431284\n",
      "Epoch: 5820/10000... Step: 582000... Loss: 0.129922... Val Loss: 0.441423\n",
      "Epoch  5825: reducing learning rate of group 0 to 3.9063e-07.\n",
      "Epoch: 5830/10000... Step: 583000... Loss: 0.158597... Val Loss: 0.433059\n",
      "Epoch: 5840/10000... Step: 584000... Loss: 0.126692... Val Loss: 0.442260\n",
      "Epoch: 5850/10000... Step: 585000... Loss: 0.134055... Val Loss: 0.448110\n",
      "Epoch: 5860/10000... Step: 586000... Loss: 0.114187... Val Loss: 0.438335\n",
      "Epoch: 5870/10000... Step: 587000... Loss: 0.129439... Val Loss: 0.445020\n",
      "Epoch: 5880/10000... Step: 588000... Loss: 0.179685... Val Loss: 0.446756\n",
      "Epoch: 5890/10000... Step: 589000... Loss: 0.161322... Val Loss: 0.448693\n",
      "Epoch: 5900/10000... Step: 590000... Loss: 0.138661... Val Loss: 0.435632\n",
      "Epoch: 5910/10000... Step: 591000... Loss: 0.134962... Val Loss: 0.454010\n",
      "Epoch: 5920/10000... Step: 592000... Loss: 0.140190... Val Loss: 0.446259\n",
      "Epoch: 5930/10000... Step: 593000... Loss: 0.128165... Val Loss: 0.440670\n",
      "Epoch: 5940/10000... Step: 594000... Loss: 0.132232... Val Loss: 0.453219\n",
      "Epoch: 5950/10000... Step: 595000... Loss: 0.139041... Val Loss: 0.439761\n",
      "Epoch: 5960/10000... Step: 596000... Loss: 0.147694... Val Loss: 0.436118\n",
      "Epoch: 5970/10000... Step: 597000... Loss: 0.130292... Val Loss: 0.446132\n",
      "Epoch: 5980/10000... Step: 598000... Loss: 0.143024... Val Loss: 0.436640\n",
      "Epoch: 5990/10000... Step: 599000... Loss: 0.119123... Val Loss: 0.446306\n",
      "Epoch: 6000/10000... Step: 600000... Loss: 0.132542... Val Loss: 0.442894\n",
      "Epoch: 6010/10000... Step: 601000... Loss: 0.138652... Val Loss: 0.447100\n",
      "Epoch: 6020/10000... Step: 602000... Loss: 0.135960... Val Loss: 0.448923\n",
      "Epoch: 6030/10000... Step: 603000... Loss: 0.113772... Val Loss: 0.439960\n",
      "Epoch: 6040/10000... Step: 604000... Loss: 0.147659... Val Loss: 0.434412\n",
      "Epoch: 6050/10000... Step: 605000... Loss: 0.138529... Val Loss: 0.435756\n",
      "Epoch: 6060/10000... Step: 606000... Loss: 0.125369... Val Loss: 0.457821\n",
      "Epoch: 6070/10000... Step: 607000... Loss: 0.133308... Val Loss: 0.430912\n",
      "Epoch: 6080/10000... Step: 608000... Loss: 0.119812... Val Loss: 0.446798\n",
      "Epoch: 6090/10000... Step: 609000... Loss: 0.122868... Val Loss: 0.443265\n",
      "Epoch: 6100/10000... Step: 610000... Loss: 0.147879... Val Loss: 0.429709\n",
      "Epoch: 6110/10000... Step: 611000... Loss: 0.146280... Val Loss: 0.444719\n",
      "Epoch: 6120/10000... Step: 612000... Loss: 0.124543... Val Loss: 0.444316\n",
      "Epoch: 6130/10000... Step: 613000... Loss: 0.147761... Val Loss: 0.442805\n",
      "Epoch: 6140/10000... Step: 614000... Loss: 0.123971... Val Loss: 0.452337\n",
      "Epoch: 6150/10000... Step: 615000... Loss: 0.127790... Val Loss: 0.443328\n",
      "Epoch: 6160/10000... Step: 616000... Loss: 0.130479... Val Loss: 0.447795\n",
      "Epoch: 6170/10000... Step: 617000... Loss: 0.158372... Val Loss: 0.436366\n",
      "Epoch: 6180/10000... Step: 618000... Loss: 0.134416... Val Loss: 0.449167\n",
      "Epoch: 6190/10000... Step: 619000... Loss: 0.141918... Val Loss: 0.442957\n",
      "Epoch: 6200/10000... Step: 620000... Loss: 0.145677... Val Loss: 0.439135\n",
      "Epoch: 6210/10000... Step: 621000... Loss: 0.141702... Val Loss: 0.450103\n",
      "Epoch: 6220/10000... Step: 622000... Loss: 0.130915... Val Loss: 0.448274\n",
      "Epoch: 6230/10000... Step: 623000... Loss: 0.109968... Val Loss: 0.440539\n",
      "Epoch: 6240/10000... Step: 624000... Loss: 0.124390... Val Loss: 0.447448\n",
      "Epoch: 6250/10000... Step: 625000... Loss: 0.129469... Val Loss: 0.438508\n",
      "Epoch: 6260/10000... Step: 626000... Loss: 0.132493... Val Loss: 0.456944\n",
      "Epoch: 6270/10000... Step: 627000... Loss: 0.143462... Val Loss: 0.440516\n",
      "Epoch: 6280/10000... Step: 628000... Loss: 0.115927... Val Loss: 0.441872\n",
      "Epoch: 6290/10000... Step: 629000... Loss: 0.127604... Val Loss: 0.444328\n",
      "Epoch: 6300/10000... Step: 630000... Loss: 0.137062... Val Loss: 0.445717\n",
      "Epoch: 6310/10000... Step: 631000... Loss: 0.124401... Val Loss: 0.448361\n",
      "Epoch: 6320/10000... Step: 632000... Loss: 0.142062... Val Loss: 0.434305\n",
      "Epoch  6326: reducing learning rate of group 0 to 1.9531e-07.\n",
      "Epoch: 6330/10000... Step: 633000... Loss: 0.136085... Val Loss: 0.447350\n",
      "Epoch: 6340/10000... Step: 634000... Loss: 0.127458... Val Loss: 0.447043\n",
      "Epoch: 6350/10000... Step: 635000... Loss: 0.146533... Val Loss: 0.440484\n",
      "Epoch: 6370/10000... Step: 637000... Loss: 0.123934... Val Loss: 0.436655\n",
      "Epoch: 6380/10000... Step: 638000... Loss: 0.117331... Val Loss: 0.447554\n",
      "Epoch: 6390/10000... Step: 639000... Loss: 0.125257... Val Loss: 0.444905\n",
      "Epoch: 6400/10000... Step: 640000... Loss: 0.128672... Val Loss: 0.439812\n",
      "Epoch: 6410/10000... Step: 641000... Loss: 0.141720... Val Loss: 0.449028\n",
      "Epoch: 6420/10000... Step: 642000... Loss: 0.123683... Val Loss: 0.436936\n",
      "Epoch: 6430/10000... Step: 643000... Loss: 0.117965... Val Loss: 0.456021\n",
      "Epoch: 6440/10000... Step: 644000... Loss: 0.146928... Val Loss: 0.441569\n",
      "Epoch: 6450/10000... Step: 645000... Loss: 0.119575... Val Loss: 0.452216\n",
      "Epoch: 6460/10000... Step: 646000... Loss: 0.151894... Val Loss: 0.450432\n",
      "Epoch: 6470/10000... Step: 647000... Loss: 0.142645... Val Loss: 0.436271\n",
      "Epoch: 6480/10000... Step: 648000... Loss: 0.147931... Val Loss: 0.446402\n",
      "Epoch: 6490/10000... Step: 649000... Loss: 0.127050... Val Loss: 0.441177\n",
      "Epoch: 6500/10000... Step: 650000... Loss: 0.121109... Val Loss: 0.441399\n",
      "Epoch: 6510/10000... Step: 651000... Loss: 0.129991... Val Loss: 0.446885\n",
      "Epoch: 6520/10000... Step: 652000... Loss: 0.121544... Val Loss: 0.443130\n",
      "Epoch: 6530/10000... Step: 653000... Loss: 0.134070... Val Loss: 0.436427\n",
      "Epoch: 6540/10000... Step: 654000... Loss: 0.120090... Val Loss: 0.431954\n",
      "Epoch: 6550/10000... Step: 655000... Loss: 0.119023... Val Loss: 0.442914\n",
      "Epoch: 6560/10000... Step: 656000... Loss: 0.129721... Val Loss: 0.450017\n",
      "Epoch: 6570/10000... Step: 657000... Loss: 0.126228... Val Loss: 0.457276\n",
      "Epoch: 6580/10000... Step: 658000... Loss: 0.142538... Val Loss: 0.449838\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-8dcb56491007>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "counter = 0\n",
    "print_every = 1000\n",
    "clip = .5\n",
    "valid_loss_min = np.Inf\n",
    "val_loss = torch.tensor(np.Inf)\n",
    "model.train()\n",
    "history = pd.DataFrame()\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(train_batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = train_criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            val_h = model.init_hidden(val_batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in val_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp, val_h)\n",
    "                val_loss = val_criterion(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            history.loc[i + 1 / len(train_loader), 'train_loss'] = loss.data.cpu().numpy()\n",
    "            history.loc[i, 'dev_loss'] = val_loss.cpu().detach().numpy()\n",
    "            \n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './model_1face.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)\n",
    "    scheduler.step(val_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7eff0a98a240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(history.plot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>dev_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9.01</th>\n",
       "      <td>0.430252</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.406871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19.01</th>\n",
       "      <td>0.439899</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.423425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.01</th>\n",
       "      <td>0.458890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.373825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39.01</th>\n",
       "      <td>0.462353</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.378933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49.01</th>\n",
       "      <td>0.407788</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.404696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59.01</th>\n",
       "      <td>0.452861</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.463952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69.01</th>\n",
       "      <td>0.443603</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.452389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79.01</th>\n",
       "      <td>0.392853</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.437774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89.01</th>\n",
       "      <td>0.421360</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.380715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.01</th>\n",
       "      <td>0.410814</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.367289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109.01</th>\n",
       "      <td>0.411617</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.339606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119.01</th>\n",
       "      <td>0.374878</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.337838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129.01</th>\n",
       "      <td>0.413377</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.488720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139.01</th>\n",
       "      <td>0.422499</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.337036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149.01</th>\n",
       "      <td>0.385006</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.277921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6439.01</th>\n",
       "      <td>0.146928</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6439.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.428070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6449.01</th>\n",
       "      <td>0.119575</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6449.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.473190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6459.01</th>\n",
       "      <td>0.151894</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6459.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.426806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6469.01</th>\n",
       "      <td>0.142645</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6469.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.591813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6479.01</th>\n",
       "      <td>0.147931</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6479.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.372290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6489.01</th>\n",
       "      <td>0.127050</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6489.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.566121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6499.01</th>\n",
       "      <td>0.121109</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6499.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.326138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6509.01</th>\n",
       "      <td>0.129991</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6509.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.647684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6519.01</th>\n",
       "      <td>0.121544</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6519.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.357384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6529.01</th>\n",
       "      <td>0.134070</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6529.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.426717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6539.01</th>\n",
       "      <td>0.120090</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6539.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.586072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6549.01</th>\n",
       "      <td>0.119023</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6549.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.353158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6559.01</th>\n",
       "      <td>0.129721</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6559.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.322563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569.01</th>\n",
       "      <td>0.126228</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.385827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6579.01</th>\n",
       "      <td>0.142538</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6579.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.720322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1316 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         train_loss  dev_loss\n",
       "9.01       0.430252       NaN\n",
       "9.00            NaN  0.406871\n",
       "19.01      0.439899       NaN\n",
       "19.00           NaN  0.423425\n",
       "29.01      0.458890       NaN\n",
       "29.00           NaN  0.373825\n",
       "39.01      0.462353       NaN\n",
       "39.00           NaN  0.378933\n",
       "49.01      0.407788       NaN\n",
       "49.00           NaN  0.404696\n",
       "59.01      0.452861       NaN\n",
       "59.00           NaN  0.463952\n",
       "69.01      0.443603       NaN\n",
       "69.00           NaN  0.452389\n",
       "79.01      0.392853       NaN\n",
       "79.00           NaN  0.437774\n",
       "89.01      0.421360       NaN\n",
       "89.00           NaN  0.380715\n",
       "99.01      0.410814       NaN\n",
       "99.00           NaN  0.367289\n",
       "109.01     0.411617       NaN\n",
       "109.00          NaN  0.339606\n",
       "119.01     0.374878       NaN\n",
       "119.00          NaN  0.337838\n",
       "129.01     0.413377       NaN\n",
       "129.00          NaN  0.488720\n",
       "139.01     0.422499       NaN\n",
       "139.00          NaN  0.337036\n",
       "149.01     0.385006       NaN\n",
       "149.00          NaN  0.277921\n",
       "...             ...       ...\n",
       "6439.01    0.146928       NaN\n",
       "6439.00         NaN  0.428070\n",
       "6449.01    0.119575       NaN\n",
       "6449.00         NaN  0.473190\n",
       "6459.01    0.151894       NaN\n",
       "6459.00         NaN  0.426806\n",
       "6469.01    0.142645       NaN\n",
       "6469.00         NaN  0.591813\n",
       "6479.01    0.147931       NaN\n",
       "6479.00         NaN  0.372290\n",
       "6489.01    0.127050       NaN\n",
       "6489.00         NaN  0.566121\n",
       "6499.01    0.121109       NaN\n",
       "6499.00         NaN  0.326138\n",
       "6509.01    0.129991       NaN\n",
       "6509.00         NaN  0.647684\n",
       "6519.01    0.121544       NaN\n",
       "6519.00         NaN  0.357384\n",
       "6529.01    0.134070       NaN\n",
       "6529.00         NaN  0.426717\n",
       "6539.01    0.120090       NaN\n",
       "6539.00         NaN  0.586072\n",
       "6549.01    0.119023       NaN\n",
       "6549.00         NaN  0.353158\n",
       "6559.01    0.129721       NaN\n",
       "6559.00         NaN  0.322563\n",
       "6569.01    0.126228       NaN\n",
       "6569.00         NaN  0.385827\n",
       "6579.01    0.142538       NaN\n",
       "6579.00         NaN  0.720322\n",
       "\n",
       "[1316 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fixed history code that got botched above. \n",
    "Testing original with history = history\n",
    "adding all previous fc with (64, 32) and (32,16) all activated with ELU\n",
    "0.9 dropout  << getting aggressive with dropout (looked like we overfitted on the model above. train_loss kept going down while val_loss hovered at .40)\n",
    "0.0001 LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DFDCNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, drop_prob=0.9):\n",
    "        super(DFDCNet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.9)\n",
    "        self.batchnorm = nn.BatchNorm1d(hidden_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.float()\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "#         out = self.batchnorm(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.elu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.elu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.elu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFDCNet(\n",
      "  (lstm): LSTM(512, 128, num_layers=5, batch_first=True, dropout=0.9)\n",
      "  (dropout): Dropout(p=0.9, inplace=False)\n",
      "  (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (elu): ELU(alpha=1.0)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 512\n",
    "output_size = 1\n",
    "hidden_dim = 128\n",
    "n_layers = 5\n",
    "\n",
    "model = DFDCNet(input_size, output_size, hidden_dim, n_layers)\n",
    "model.to(device)\n",
    "train_criterion = nn.BCELoss()\n",
    "val_criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=500, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/10000... Step: 1000... Loss: 0.400138... Val Loss: 0.395108\n",
      "Validation loss decreased (inf --> 0.395108).  Saving model ...\n",
      "Epoch: 20/10000... Step: 2000... Loss: 0.384487... Val Loss: 0.390183\n",
      "Validation loss decreased (0.395108 --> 0.390183).  Saving model ...\n",
      "Epoch: 30/10000... Step: 3000... Loss: 0.388891... Val Loss: 0.399166\n",
      "Epoch: 40/10000... Step: 4000... Loss: 0.400144... Val Loss: 0.396218\n",
      "Epoch: 50/10000... Step: 5000... Loss: 0.390128... Val Loss: 0.389486\n",
      "Validation loss decreased (0.390183 --> 0.389486).  Saving model ...\n",
      "Epoch: 60/10000... Step: 6000... Loss: 0.392917... Val Loss: 0.386507\n",
      "Validation loss decreased (0.389486 --> 0.386507).  Saving model ...\n",
      "Epoch: 90/10000... Step: 9000... Loss: 0.369805... Val Loss: 0.385383\n",
      "Validation loss decreased (0.386507 --> 0.385383).  Saving model ...\n",
      "Epoch: 100/10000... Step: 10000... Loss: 0.343799... Val Loss: 0.382633\n",
      "Validation loss decreased (0.385383 --> 0.382633).  Saving model ...\n",
      "Epoch: 110/10000... Step: 11000... Loss: 0.367438... Val Loss: 0.384163\n",
      "Epoch: 120/10000... Step: 12000... Loss: 0.382679... Val Loss: 0.391991\n",
      "Epoch: 130/10000... Step: 13000... Loss: 0.395799... Val Loss: 0.379662\n",
      "Validation loss decreased (0.382633 --> 0.379662).  Saving model ...\n",
      "Epoch: 140/10000... Step: 14000... Loss: 0.355650... Val Loss: 0.378101\n",
      "Validation loss decreased (0.379662 --> 0.378101).  Saving model ...\n",
      "Epoch: 150/10000... Step: 15000... Loss: 0.355162... Val Loss: 0.384790\n",
      "Epoch: 160/10000... Step: 16000... Loss: 0.401025... Val Loss: 0.377795\n",
      "Validation loss decreased (0.378101 --> 0.377795).  Saving model ...\n",
      "Epoch: 170/10000... Step: 17000... Loss: 0.341144... Val Loss: 0.378510\n",
      "Epoch: 180/10000... Step: 18000... Loss: 0.335074... Val Loss: 0.371999\n",
      "Validation loss decreased (0.377795 --> 0.371999).  Saving model ...\n",
      "Epoch: 190/10000... Step: 19000... Loss: 0.326288... Val Loss: 0.386042\n",
      "Epoch: 200/10000... Step: 20000... Loss: 0.358573... Val Loss: 0.383933\n",
      "Epoch: 210/10000... Step: 21000... Loss: 0.335893... Val Loss: 0.373414\n",
      "Epoch: 220/10000... Step: 22000... Loss: 0.344809... Val Loss: 0.376262\n",
      "Epoch: 230/10000... Step: 23000... Loss: 0.314800... Val Loss: 0.370380\n",
      "Validation loss decreased (0.371999 --> 0.370380).  Saving model ...\n",
      "Epoch: 240/10000... Step: 24000... Loss: 0.356303... Val Loss: 0.376794\n",
      "Epoch: 250/10000... Step: 25000... Loss: 0.319031... Val Loss: 0.374005\n",
      "Epoch: 260/10000... Step: 26000... Loss: 0.306571... Val Loss: 0.374146\n",
      "Epoch: 270/10000... Step: 27000... Loss: 0.323818... Val Loss: 0.374708\n",
      "Epoch: 300/10000... Step: 30000... Loss: 0.352237... Val Loss: 0.363733\n",
      "Validation loss decreased (0.370380 --> 0.363733).  Saving model ...\n",
      "Epoch: 310/10000... Step: 31000... Loss: 0.286039... Val Loss: 0.367252\n",
      "Epoch: 320/10000... Step: 32000... Loss: 0.338686... Val Loss: 0.374730\n",
      "Epoch: 330/10000... Step: 33000... Loss: 0.323454... Val Loss: 0.363866\n",
      "Epoch: 340/10000... Step: 34000... Loss: 0.336008... Val Loss: 0.372150\n",
      "Epoch: 350/10000... Step: 35000... Loss: 0.319928... Val Loss: 0.370358\n",
      "Epoch: 360/10000... Step: 36000... Loss: 0.342411... Val Loss: 0.364200\n",
      "Epoch: 370/10000... Step: 37000... Loss: 0.294467... Val Loss: 0.365067\n",
      "Epoch: 380/10000... Step: 38000... Loss: 0.330688... Val Loss: 0.363696\n",
      "Validation loss decreased (0.363733 --> 0.363696).  Saving model ...\n",
      "Epoch: 390/10000... Step: 39000... Loss: 0.266017... Val Loss: 0.365116\n",
      "Epoch: 400/10000... Step: 40000... Loss: 0.329212... Val Loss: 0.366694\n",
      "Epoch: 410/10000... Step: 41000... Loss: 0.318160... Val Loss: 0.362578\n",
      "Validation loss decreased (0.363696 --> 0.362578).  Saving model ...\n",
      "Epoch: 420/10000... Step: 42000... Loss: 0.298779... Val Loss: 0.367576\n",
      "Epoch: 430/10000... Step: 43000... Loss: 0.322762... Val Loss: 0.360893\n",
      "Validation loss decreased (0.362578 --> 0.360893).  Saving model ...\n",
      "Epoch: 440/10000... Step: 44000... Loss: 0.330710... Val Loss: 0.364491\n",
      "Epoch: 450/10000... Step: 45000... Loss: 0.335177... Val Loss: 0.365572\n",
      "Epoch: 460/10000... Step: 46000... Loss: 0.328484... Val Loss: 0.368030\n",
      "Epoch: 470/10000... Step: 47000... Loss: 0.317573... Val Loss: 0.359407\n",
      "Validation loss decreased (0.360893 --> 0.359407).  Saving model ...\n",
      "Epoch: 480/10000... Step: 48000... Loss: 0.317871... Val Loss: 0.365839\n",
      "Epoch: 510/10000... Step: 51000... Loss: 0.367327... Val Loss: 0.363995\n",
      "Epoch: 520/10000... Step: 52000... Loss: 0.338426... Val Loss: 0.360058\n",
      "Epoch: 530/10000... Step: 53000... Loss: 0.319514... Val Loss: 0.360544\n",
      "Epoch: 540/10000... Step: 54000... Loss: 0.301739... Val Loss: 0.365156\n",
      "Epoch: 550/10000... Step: 55000... Loss: 0.298274... Val Loss: 0.349444\n",
      "Validation loss decreased (0.354662 --> 0.349444).  Saving model ...\n",
      "Epoch: 560/10000... Step: 56000... Loss: 0.308076... Val Loss: 0.356648\n",
      "Epoch: 570/10000... Step: 57000... Loss: 0.313969... Val Loss: 0.353491\n",
      "Epoch: 580/10000... Step: 58000... Loss: 0.321071... Val Loss: 0.355563\n",
      "Epoch: 590/10000... Step: 59000... Loss: 0.294256... Val Loss: 0.354823\n",
      "Epoch: 600/10000... Step: 60000... Loss: 0.314358... Val Loss: 0.351822\n",
      "Epoch: 610/10000... Step: 61000... Loss: 0.317550... Val Loss: 0.351358\n",
      "Epoch: 620/10000... Step: 62000... Loss: 0.286796... Val Loss: 0.352989\n",
      "Epoch: 630/10000... Step: 63000... Loss: 0.318108... Val Loss: 0.355442\n",
      "Epoch: 640/10000... Step: 64000... Loss: 0.260385... Val Loss: 0.345907\n",
      "Validation loss decreased (0.349444 --> 0.345907).  Saving model ...\n",
      "Epoch: 650/10000... Step: 65000... Loss: 0.267909... Val Loss: 0.357867\n",
      "Epoch: 660/10000... Step: 66000... Loss: 0.304209... Val Loss: 0.350708\n",
      "Epoch: 670/10000... Step: 67000... Loss: 0.304783... Val Loss: 0.345592\n",
      "Validation loss decreased (0.345907 --> 0.345592).  Saving model ...\n",
      "Epoch: 680/10000... Step: 68000... Loss: 0.302725... Val Loss: 0.346030\n",
      "Epoch: 690/10000... Step: 69000... Loss: 0.276953... Val Loss: 0.344990\n",
      "Validation loss decreased (0.345592 --> 0.344990).  Saving model ...\n",
      "Epoch: 700/10000... Step: 70000... Loss: 0.310156... Val Loss: 0.349088\n",
      "Epoch: 710/10000... Step: 71000... Loss: 0.295884... Val Loss: 0.352107\n",
      "Epoch: 720/10000... Step: 72000... Loss: 0.284912... Val Loss: 0.342030\n",
      "Validation loss decreased (0.344990 --> 0.342030).  Saving model ...\n",
      "Epoch: 730/10000... Step: 73000... Loss: 0.302689... Val Loss: 0.348737\n",
      "Epoch: 740/10000... Step: 74000... Loss: 0.277853... Val Loss: 0.341484\n",
      "Validation loss decreased (0.342030 --> 0.341484).  Saving model ...\n",
      "Epoch: 750/10000... Step: 75000... Loss: 0.298326... Val Loss: 0.347560\n",
      "Epoch: 760/10000... Step: 76000... Loss: 0.331768... Val Loss: 0.343047\n",
      "Epoch: 770/10000... Step: 77000... Loss: 0.237962... Val Loss: 0.342852\n",
      "Epoch: 780/10000... Step: 78000... Loss: 0.263829... Val Loss: 0.342820\n",
      "Epoch: 790/10000... Step: 79000... Loss: 0.284089... Val Loss: 0.344142\n",
      "Epoch: 800/10000... Step: 80000... Loss: 0.269672... Val Loss: 0.340178\n",
      "Validation loss decreased (0.341484 --> 0.340178).  Saving model ...\n",
      "Epoch: 810/10000... Step: 81000... Loss: 0.280192... Val Loss: 0.337163\n",
      "Validation loss decreased (0.340178 --> 0.337163).  Saving model ...\n",
      "Epoch: 820/10000... Step: 82000... Loss: 0.274201... Val Loss: 0.345116\n",
      "Epoch: 830/10000... Step: 83000... Loss: 0.281568... Val Loss: 0.341630\n",
      "Epoch: 840/10000... Step: 84000... Loss: 0.246296... Val Loss: 0.339855\n",
      "Epoch: 850/10000... Step: 85000... Loss: 0.277870... Val Loss: 0.335705\n",
      "Validation loss decreased (0.337163 --> 0.335705).  Saving model ...\n",
      "Epoch: 860/10000... Step: 86000... Loss: 0.281861... Val Loss: 0.345199\n",
      "Epoch: 870/10000... Step: 87000... Loss: 0.245904... Val Loss: 0.336115\n",
      "Epoch: 880/10000... Step: 88000... Loss: 0.274962... Val Loss: 0.344469\n",
      "Epoch: 890/10000... Step: 89000... Loss: 0.266113... Val Loss: 0.338227\n",
      "Epoch: 900/10000... Step: 90000... Loss: 0.282110... Val Loss: 0.336910\n",
      "Epoch: 910/10000... Step: 91000... Loss: 0.245165... Val Loss: 0.342793\n",
      "Epoch: 920/10000... Step: 92000... Loss: 0.266095... Val Loss: 0.342732\n",
      "Epoch: 930/10000... Step: 93000... Loss: 0.267620... Val Loss: 0.342730\n",
      "Epoch: 940/10000... Step: 94000... Loss: 0.243798... Val Loss: 0.339891\n",
      "Epoch: 950/10000... Step: 95000... Loss: 0.249421... Val Loss: 0.346579\n",
      "Epoch: 960/10000... Step: 96000... Loss: 0.247500... Val Loss: 0.341724\n",
      "Epoch: 970/10000... Step: 97000... Loss: 0.254715... Val Loss: 0.334878\n",
      "Validation loss decreased (0.335705 --> 0.334878).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 980/10000... Step: 98000... Loss: 0.238862... Val Loss: 0.334679\n",
      "Validation loss decreased (0.334878 --> 0.334679).  Saving model ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-c253c1ccbfeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "counter = 0\n",
    "print_every = 1000\n",
    "clip = .5\n",
    "valid_loss_min = np.Inf\n",
    "val_loss = torch.tensor(np.Inf)\n",
    "model.train()\n",
    "history = pd.DataFrame()\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(train_batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = train_criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            val_h = model.init_hidden(val_batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in val_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp, val_h)\n",
    "                val_loss = val_criterion(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            history.loc[i + 1, 'train_loss'] = loss.data.cpu().numpy()\n",
    "            history.loc[i + 1, 'dev_loss'] = val_loss.cpu().detach().numpy()\n",
    "            \n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './model_1face.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)\n",
    "    scheduler.step(val_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7efb3499ef28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(history.plot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>dev_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.400138</td>\n",
       "      <td>0.394252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.384487</td>\n",
       "      <td>0.304138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.388891</td>\n",
       "      <td>0.247592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.400144</td>\n",
       "      <td>0.406792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.390128</td>\n",
       "      <td>0.371141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.392917</td>\n",
       "      <td>0.389180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.363402</td>\n",
       "      <td>0.407598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.378886</td>\n",
       "      <td>0.451834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.369805</td>\n",
       "      <td>0.396285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.343799</td>\n",
       "      <td>0.374847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.367438</td>\n",
       "      <td>0.321101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.382679</td>\n",
       "      <td>0.376651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.395799</td>\n",
       "      <td>0.366119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.355650</td>\n",
       "      <td>0.442288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.355162</td>\n",
       "      <td>0.334022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.401025</td>\n",
       "      <td>0.468624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0.341144</td>\n",
       "      <td>0.389351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.335074</td>\n",
       "      <td>0.390198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.326288</td>\n",
       "      <td>0.236411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.358573</td>\n",
       "      <td>0.398262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0.335893</td>\n",
       "      <td>0.533129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.344809</td>\n",
       "      <td>0.285518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>0.314800</td>\n",
       "      <td>0.395783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.356303</td>\n",
       "      <td>0.434688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.319031</td>\n",
       "      <td>0.376655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>0.306571</td>\n",
       "      <td>0.312868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>0.323818</td>\n",
       "      <td>0.277818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0.363781</td>\n",
       "      <td>0.370465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>0.308360</td>\n",
       "      <td>0.377234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.352237</td>\n",
       "      <td>0.381102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>0.276953</td>\n",
       "      <td>0.363660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.310156</td>\n",
       "      <td>0.320226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>0.295884</td>\n",
       "      <td>0.374930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>0.284912</td>\n",
       "      <td>0.266557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>0.302689</td>\n",
       "      <td>0.229357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>0.277853</td>\n",
       "      <td>0.357613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>0.298326</td>\n",
       "      <td>0.343023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>0.331768</td>\n",
       "      <td>0.332020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>0.237962</td>\n",
       "      <td>0.335888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>0.263829</td>\n",
       "      <td>0.290134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>0.284089</td>\n",
       "      <td>0.378199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.269672</td>\n",
       "      <td>0.239881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>0.280192</td>\n",
       "      <td>0.356807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>0.274201</td>\n",
       "      <td>0.366048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>0.281568</td>\n",
       "      <td>0.288088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>0.246296</td>\n",
       "      <td>0.293150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>0.277870</td>\n",
       "      <td>0.287793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0.281861</td>\n",
       "      <td>0.414253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>0.245904</td>\n",
       "      <td>0.303039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>0.274962</td>\n",
       "      <td>0.251378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0.266113</td>\n",
       "      <td>0.289102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>0.282110</td>\n",
       "      <td>0.437843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>0.245165</td>\n",
       "      <td>0.380129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>0.266095</td>\n",
       "      <td>0.373772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>0.267620</td>\n",
       "      <td>0.268744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>0.243798</td>\n",
       "      <td>0.217449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>0.249421</td>\n",
       "      <td>0.323130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>0.247500</td>\n",
       "      <td>0.403566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>0.254715</td>\n",
       "      <td>0.361974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>0.238862</td>\n",
       "      <td>0.252841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train_loss  dev_loss\n",
       "10     0.400138  0.394252\n",
       "20     0.384487  0.304138\n",
       "30     0.388891  0.247592\n",
       "40     0.400144  0.406792\n",
       "50     0.390128  0.371141\n",
       "60     0.392917  0.389180\n",
       "70     0.363402  0.407598\n",
       "80     0.378886  0.451834\n",
       "90     0.369805  0.396285\n",
       "100    0.343799  0.374847\n",
       "110    0.367438  0.321101\n",
       "120    0.382679  0.376651\n",
       "130    0.395799  0.366119\n",
       "140    0.355650  0.442288\n",
       "150    0.355162  0.334022\n",
       "160    0.401025  0.468624\n",
       "170    0.341144  0.389351\n",
       "180    0.335074  0.390198\n",
       "190    0.326288  0.236411\n",
       "200    0.358573  0.398262\n",
       "210    0.335893  0.533129\n",
       "220    0.344809  0.285518\n",
       "230    0.314800  0.395783\n",
       "240    0.356303  0.434688\n",
       "250    0.319031  0.376655\n",
       "260    0.306571  0.312868\n",
       "270    0.323818  0.277818\n",
       "280    0.363781  0.370465\n",
       "290    0.308360  0.377234\n",
       "300    0.352237  0.381102\n",
       "..          ...       ...\n",
       "690    0.276953  0.363660\n",
       "700    0.310156  0.320226\n",
       "710    0.295884  0.374930\n",
       "720    0.284912  0.266557\n",
       "730    0.302689  0.229357\n",
       "740    0.277853  0.357613\n",
       "750    0.298326  0.343023\n",
       "760    0.331768  0.332020\n",
       "770    0.237962  0.335888\n",
       "780    0.263829  0.290134\n",
       "790    0.284089  0.378199\n",
       "800    0.269672  0.239881\n",
       "810    0.280192  0.356807\n",
       "820    0.274201  0.366048\n",
       "830    0.281568  0.288088\n",
       "840    0.246296  0.293150\n",
       "850    0.277870  0.287793\n",
       "860    0.281861  0.414253\n",
       "870    0.245904  0.303039\n",
       "880    0.274962  0.251378\n",
       "890    0.266113  0.289102\n",
       "900    0.282110  0.437843\n",
       "910    0.245165  0.380129\n",
       "920    0.266095  0.373772\n",
       "930    0.267620  0.268744\n",
       "940    0.243798  0.217449\n",
       "950    0.249421  0.323130\n",
       "960    0.247500  0.403566\n",
       "970    0.254715  0.361974\n",
       "980    0.238862  0.252841\n",
       "\n",
       "[98 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
