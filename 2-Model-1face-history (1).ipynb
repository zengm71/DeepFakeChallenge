{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f32140ccbf0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:31<00:00,  1.97s/it]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for p in tqdm(np.arange(0, 50), total = 50):\n",
    "    X_p = torch.load('data_processed/1face_X_part' + str(p) + '.pt', map_location = device)\n",
    "    Y_p = torch.load('data_processed/1face_Y_part' + str(p) + '.pt', map_location = device)\n",
    "    X = X + X_p\n",
    "    Y = Y + Y_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104343"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104343"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.stack(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([104343, 30, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data_tomodel/X.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-89d9b861827d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./data_tomodel/X.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./data_tomodel/Y.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data_tomodel/X.pt'"
     ]
    }
   ],
   "source": [
    "torch.save(X, './data_tomodel/X.pt')\n",
    "torch.save(Y, './data_tomodel/Y.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "dataset = TensorDataset(X[0:104300], torch.from_numpy(np.array(Y[0:104300])))\n",
    "train_data, val_data = torch.utils.data.random_split(dataset, [100000, 4300])\n",
    "train_batch_size = 2000\n",
    "val_batch_size = 100\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=val_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DFDCNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(DFDCNet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.75)\n",
    "        self.batchnorm = nn.BatchNorm1d(hidden_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.fc1 = nn.Linear(hidden_dim, 32)\n",
    "#         self.fc2 = nn.Linear(64, 32)\n",
    "#         self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(32, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.float()\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "#         out = self.batchnorm(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.elu(out)\n",
    "#         out = self.fc2(out)\n",
    "#         out = self.elu(out)\n",
    "#         out = self.fc3(out)\n",
    "#         out = self.elu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFDCNet(\n",
      "  (lstm): LSTM(512, 64, num_layers=5, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.75, inplace=False)\n",
      "  (batchnorm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (elu): ELU(alpha=1.0)\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 512\n",
    "output_size = 1\n",
    "hidden_dim = 128\n",
    "n_layers = 5\n",
    "\n",
    "model = DFDCNet(input_size, output_size, hidden_dim, n_layers)\n",
    "model.to(device)\n",
    "train_criterion = nn.BCELoss()\n",
    "val_criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=500, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/10000... Step: 1000... Loss: 0.531787... Val Loss: 0.431874\n",
      "Validation loss decreased (inf --> 0.431874).  Saving model ...\n",
      "Epoch: 20/10000... Step: 2000... Loss: 0.472329... Val Loss: 0.431393\n",
      "Validation loss decreased (0.431874 --> 0.431393).  Saving model ...\n",
      "Epoch: 30/10000... Step: 3000... Loss: 0.449641... Val Loss: 0.431291\n",
      "Validation loss decreased (0.431393 --> 0.431291).  Saving model ...\n",
      "Epoch: 40/10000... Step: 4000... Loss: 0.437786... Val Loss: 0.430938\n",
      "Validation loss decreased (0.431291 --> 0.430938).  Saving model ...\n",
      "Epoch: 50/10000... Step: 5000... Loss: 0.443349... Val Loss: 0.431201\n",
      "Epoch: 60/10000... Step: 6000... Loss: 0.466028... Val Loss: 0.430974\n",
      "Epoch: 70/10000... Step: 7000... Loss: 0.462511... Val Loss: 0.430990\n",
      "Epoch: 80/10000... Step: 8000... Loss: 0.465358... Val Loss: 0.431100\n",
      "Epoch: 90/10000... Step: 9000... Loss: 0.467207... Val Loss: 0.431350\n",
      "Epoch: 100/10000... Step: 10000... Loss: 0.455970... Val Loss: 0.431349\n",
      "Epoch: 110/10000... Step: 11000... Loss: 0.457632... Val Loss: 0.431261\n",
      "Epoch: 120/10000... Step: 12000... Loss: 0.454191... Val Loss: 0.431336\n",
      "Epoch: 130/10000... Step: 13000... Loss: 0.457278... Val Loss: 0.431589\n",
      "Epoch: 140/10000... Step: 14000... Loss: 0.441668... Val Loss: 0.431142\n",
      "Epoch: 150/10000... Step: 15000... Loss: 0.452922... Val Loss: 0.431018\n",
      "Epoch: 160/10000... Step: 16000... Loss: 0.501180... Val Loss: 0.430780\n",
      "Validation loss decreased (0.430938 --> 0.430780).  Saving model ...\n",
      "Epoch: 170/10000... Step: 17000... Loss: 0.452665... Val Loss: 0.431126\n",
      "Epoch: 180/10000... Step: 18000... Loss: 0.435249... Val Loss: 0.431215\n",
      "Epoch: 190/10000... Step: 19000... Loss: 0.435412... Val Loss: 0.431546\n",
      "Epoch: 200/10000... Step: 20000... Loss: 0.455950... Val Loss: 0.431593\n",
      "Epoch: 210/10000... Step: 21000... Loss: 0.490311... Val Loss: 0.430924\n",
      "Epoch: 220/10000... Step: 22000... Loss: 0.465992... Val Loss: 0.431115\n",
      "Epoch: 230/10000... Step: 23000... Loss: 0.436535... Val Loss: 0.430913\n",
      "Epoch: 240/10000... Step: 24000... Loss: 0.492195... Val Loss: 0.431761\n",
      "Epoch: 250/10000... Step: 25000... Loss: 0.426555... Val Loss: 0.425136\n",
      "Validation loss decreased (0.430780 --> 0.425136).  Saving model ...\n",
      "Epoch: 260/10000... Step: 26000... Loss: 0.412694... Val Loss: 0.407532\n",
      "Validation loss decreased (0.425136 --> 0.407532).  Saving model ...\n",
      "Epoch: 270/10000... Step: 27000... Loss: 0.394946... Val Loss: 0.401915\n",
      "Validation loss decreased (0.407532 --> 0.401915).  Saving model ...\n",
      "Epoch: 280/10000... Step: 28000... Loss: 0.395257... Val Loss: 0.391669\n",
      "Validation loss decreased (0.401915 --> 0.391669).  Saving model ...\n",
      "Epoch: 290/10000... Step: 29000... Loss: 0.387796... Val Loss: 0.381405\n",
      "Validation loss decreased (0.391669 --> 0.381405).  Saving model ...\n",
      "Epoch: 300/10000... Step: 30000... Loss: 0.394108... Val Loss: 0.379386\n",
      "Validation loss decreased (0.381405 --> 0.379386).  Saving model ...\n",
      "Epoch: 310/10000... Step: 31000... Loss: 0.394702... Val Loss: 0.369911\n",
      "Validation loss decreased (0.379386 --> 0.369911).  Saving model ...\n",
      "Epoch: 320/10000... Step: 32000... Loss: 0.366856... Val Loss: 0.364915\n",
      "Validation loss decreased (0.369911 --> 0.364915).  Saving model ...\n",
      "Epoch: 330/10000... Step: 33000... Loss: 0.368662... Val Loss: 0.363449\n",
      "Validation loss decreased (0.364915 --> 0.363449).  Saving model ...\n",
      "Epoch: 340/10000... Step: 34000... Loss: 0.348882... Val Loss: 0.356082\n",
      "Validation loss decreased (0.363449 --> 0.356082).  Saving model ...\n",
      "Epoch: 350/10000... Step: 35000... Loss: 0.353864... Val Loss: 0.353985\n",
      "Validation loss decreased (0.356082 --> 0.353985).  Saving model ...\n",
      "Epoch: 360/10000... Step: 36000... Loss: 0.331982... Val Loss: 0.354707\n",
      "Epoch: 370/10000... Step: 37000... Loss: 0.307452... Val Loss: 0.340589\n",
      "Validation loss decreased (0.353985 --> 0.340589).  Saving model ...\n",
      "Epoch: 380/10000... Step: 38000... Loss: 0.306288... Val Loss: 0.336129\n",
      "Validation loss decreased (0.340589 --> 0.336129).  Saving model ...\n",
      "Epoch: 390/10000... Step: 39000... Loss: 0.318658... Val Loss: 0.338284\n",
      "Epoch: 400/10000... Step: 40000... Loss: 0.313316... Val Loss: 0.334955\n",
      "Validation loss decreased (0.336129 --> 0.334955).  Saving model ...\n",
      "Epoch: 410/10000... Step: 41000... Loss: 0.335133... Val Loss: 0.334931\n",
      "Validation loss decreased (0.334955 --> 0.334931).  Saving model ...\n",
      "Epoch: 420/10000... Step: 42000... Loss: 0.270853... Val Loss: 0.332716\n",
      "Validation loss decreased (0.334931 --> 0.332716).  Saving model ...\n",
      "Epoch: 430/10000... Step: 43000... Loss: 0.284182... Val Loss: 0.333650\n",
      "Epoch: 440/10000... Step: 44000... Loss: 0.287038... Val Loss: 0.338938\n",
      "Epoch: 450/10000... Step: 45000... Loss: 0.270831... Val Loss: 0.333599\n",
      "Epoch: 460/10000... Step: 46000... Loss: 0.262602... Val Loss: 0.338987\n",
      "Epoch: 470/10000... Step: 47000... Loss: 0.293131... Val Loss: 0.331634\n",
      "Validation loss decreased (0.332716 --> 0.331634).  Saving model ...\n",
      "Epoch: 480/10000... Step: 48000... Loss: 0.301701... Val Loss: 0.324947\n",
      "Validation loss decreased (0.331634 --> 0.324947).  Saving model ...\n",
      "Epoch: 490/10000... Step: 49000... Loss: 0.252277... Val Loss: 0.332261\n",
      "Epoch: 500/10000... Step: 50000... Loss: 0.272591... Val Loss: 0.322272\n",
      "Validation loss decreased (0.324947 --> 0.322272).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "counter = 0\n",
    "print_every = 1000\n",
    "clip = .5\n",
    "valid_loss_min = np.Inf\n",
    "val_loss = torch.tensor(np.Inf)\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(train_batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = train_criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            val_h = model.init_hidden(val_batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in val_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp, val_h)\n",
    "                val_loss = val_criterion(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './model_1face.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)\n",
    "    scheduler.step(val_loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "* https://github.com/gabrielloye/LSTM_Sentiment-Analysis/blob/master/main.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implemented history code to show train_loss and dev_loss (val_loss)\n",
    "Testing original with history = history\n",
    "adding all previous fc with (64, 32) and (32,16) all activated with ELU\n",
    "0.9 dropout  << getting aggressive with dropout (looked like we overfitted on the model above. train_loss kept going down while val_loss hovered at .40)\n",
    "0.0001 LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DFDCNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, drop_prob=0.9):\n",
    "        super(DFDCNet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.9)\n",
    "        self.batchnorm = nn.BatchNorm1d(hidden_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.float()\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "#         out = self.batchnorm(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.elu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.elu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.elu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFDCNet(\n",
      "  (lstm): LSTM(512, 128, num_layers=5, batch_first=True, dropout=0.9)\n",
      "  (dropout): Dropout(p=0.9, inplace=False)\n",
      "  (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (elu): ELU(alpha=1.0)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 512\n",
    "output_size = 1\n",
    "hidden_dim = 128\n",
    "n_layers = 5\n",
    "\n",
    "model = DFDCNet(input_size, output_size, hidden_dim, n_layers)\n",
    "model.to(device)\n",
    "train_criterion = nn.BCELoss()\n",
    "val_criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=500, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/10000... Step: 1000... Loss: 0.447421... Val Loss: 0.447476\n",
      "Validation loss decreased (inf --> 0.447476).  Saving model ...\n",
      "Epoch: 40/10000... Step: 2000... Loss: 0.436513... Val Loss: 0.434345\n",
      "Validation loss decreased (0.447476 --> 0.434345).  Saving model ...\n",
      "Epoch: 60/10000... Step: 3000... Loss: 0.435026... Val Loss: 0.427582\n",
      "Validation loss decreased (0.434345 --> 0.427582).  Saving model ...\n",
      "Epoch: 80/10000... Step: 4000... Loss: 0.430853... Val Loss: 0.424114\n",
      "Validation loss decreased (0.427582 --> 0.424114).  Saving model ...\n",
      "Epoch: 100/10000... Step: 5000... Loss: 0.433941... Val Loss: 0.423616\n",
      "Validation loss decreased (0.424114 --> 0.423616).  Saving model ...\n",
      "Epoch: 120/10000... Step: 6000... Loss: 0.439497... Val Loss: 0.417461\n",
      "Validation loss decreased (0.423616 --> 0.417461).  Saving model ...\n",
      "Epoch: 140/10000... Step: 7000... Loss: 0.419833... Val Loss: 0.417426\n",
      "Validation loss decreased (0.417461 --> 0.417426).  Saving model ...\n",
      "Epoch: 160/10000... Step: 8000... Loss: 0.428127... Val Loss: 0.413456\n",
      "Validation loss decreased (0.417426 --> 0.413456).  Saving model ...\n",
      "Epoch: 180/10000... Step: 9000... Loss: 0.417480... Val Loss: 0.407976\n",
      "Validation loss decreased (0.413456 --> 0.407976).  Saving model ...\n",
      "Epoch: 200/10000... Step: 10000... Loss: 0.405655... Val Loss: 0.403983\n",
      "Validation loss decreased (0.407976 --> 0.403983).  Saving model ...\n",
      "Epoch: 220/10000... Step: 11000... Loss: 0.397994... Val Loss: 0.405668\n",
      "Epoch: 240/10000... Step: 12000... Loss: 0.378346... Val Loss: 0.403272\n",
      "Validation loss decreased (0.403983 --> 0.403272).  Saving model ...\n",
      "Epoch: 280/10000... Step: 14000... Loss: 0.384608... Val Loss: 0.394262\n",
      "Validation loss decreased (0.401951 --> 0.394262).  Saving model ...\n",
      "Epoch: 300/10000... Step: 15000... Loss: 0.411088... Val Loss: 0.394259\n",
      "Validation loss decreased (0.394262 --> 0.394259).  Saving model ...\n",
      "Epoch: 320/10000... Step: 16000... Loss: 0.381833... Val Loss: 0.391811\n",
      "Validation loss decreased (0.394259 --> 0.391811).  Saving model ...\n",
      "Epoch: 340/10000... Step: 17000... Loss: 0.386430... Val Loss: 0.388319\n",
      "Validation loss decreased (0.391811 --> 0.388319).  Saving model ...\n",
      "Epoch: 360/10000... Step: 18000... Loss: 0.349267... Val Loss: 0.388659\n",
      "Epoch: 380/10000... Step: 19000... Loss: 0.409762... Val Loss: 0.383132\n",
      "Validation loss decreased (0.388319 --> 0.383132).  Saving model ...\n",
      "Epoch: 400/10000... Step: 20000... Loss: 0.372670... Val Loss: 0.382950\n",
      "Validation loss decreased (0.383132 --> 0.382950).  Saving model ...\n",
      "Epoch: 420/10000... Step: 21000... Loss: 0.353272... Val Loss: 0.384626\n",
      "Epoch: 440/10000... Step: 22000... Loss: 0.373615... Val Loss: 0.383239\n",
      "Epoch: 460/10000... Step: 23000... Loss: 0.356815... Val Loss: 0.377523\n",
      "Validation loss decreased (0.382950 --> 0.377523).  Saving model ...\n",
      "Epoch: 480/10000... Step: 24000... Loss: 0.367266... Val Loss: 0.384147\n",
      "Epoch: 520/10000... Step: 26000... Loss: 0.358414... Val Loss: 0.374150\n",
      "Validation loss decreased (0.377523 --> 0.374150).  Saving model ...\n",
      "Epoch: 540/10000... Step: 27000... Loss: 0.359729... Val Loss: 0.379151\n",
      "Epoch: 560/10000... Step: 28000... Loss: 0.369503... Val Loss: 0.376569\n",
      "Epoch: 580/10000... Step: 29000... Loss: 0.369540... Val Loss: 0.378365\n",
      "Epoch: 600/10000... Step: 30000... Loss: 0.360199... Val Loss: 0.368177\n",
      "Validation loss decreased (0.374150 --> 0.368177).  Saving model ...\n",
      "Epoch: 620/10000... Step: 31000... Loss: 0.338828... Val Loss: 0.371171\n",
      "Epoch: 640/10000... Step: 32000... Loss: 0.343346... Val Loss: 0.368149\n",
      "Validation loss decreased (0.368177 --> 0.368149).  Saving model ...\n",
      "Epoch: 660/10000... Step: 33000... Loss: 0.348013... Val Loss: 0.370132\n",
      "Epoch: 680/10000... Step: 34000... Loss: 0.350476... Val Loss: 0.370109\n",
      "Epoch: 700/10000... Step: 35000... Loss: 0.344650... Val Loss: 0.370027\n",
      "Epoch: 720/10000... Step: 36000... Loss: 0.349046... Val Loss: 0.363309\n",
      "Validation loss decreased (0.368149 --> 0.363309).  Saving model ...\n",
      "Epoch: 740/10000... Step: 37000... Loss: 0.339868... Val Loss: 0.365100\n",
      "Epoch: 760/10000... Step: 38000... Loss: 0.328055... Val Loss: 0.363481\n",
      "Epoch: 780/10000... Step: 39000... Loss: 0.347520... Val Loss: 0.358058\n",
      "Validation loss decreased (0.363309 --> 0.358058).  Saving model ...\n",
      "Epoch: 800/10000... Step: 40000... Loss: 0.360996... Val Loss: 0.371365\n",
      "Epoch: 820/10000... Step: 41000... Loss: 0.333201... Val Loss: 0.362956\n",
      "Epoch: 840/10000... Step: 42000... Loss: 0.351135... Val Loss: 0.361761\n",
      "Epoch: 860/10000... Step: 43000... Loss: 0.319984... Val Loss: 0.364481\n",
      "Epoch: 900/10000... Step: 45000... Loss: 0.329273... Val Loss: 0.370577\n",
      "Epoch: 920/10000... Step: 46000... Loss: 0.340107... Val Loss: 0.361182\n",
      "Epoch: 940/10000... Step: 47000... Loss: 0.314007... Val Loss: 0.361917\n",
      "Epoch: 960/10000... Step: 48000... Loss: 0.346263... Val Loss: 0.359416\n",
      "Epoch: 980/10000... Step: 49000... Loss: 0.325151... Val Loss: 0.354183\n",
      "Validation loss decreased (0.358058 --> 0.354183).  Saving model ...\n",
      "Epoch: 1000/10000... Step: 50000... Loss: 0.317549... Val Loss: 0.359898\n",
      "Epoch: 1020/10000... Step: 51000... Loss: 0.345908... Val Loss: 0.355900\n",
      "Epoch: 1040/10000... Step: 52000... Loss: 0.326049... Val Loss: 0.354927\n",
      "Epoch: 1060/10000... Step: 53000... Loss: 0.314253... Val Loss: 0.358781\n",
      "Epoch: 1080/10000... Step: 54000... Loss: 0.287560... Val Loss: 0.365036\n",
      "Epoch: 1100/10000... Step: 55000... Loss: 0.320054... Val Loss: 0.355513\n",
      "Epoch: 1120/10000... Step: 56000... Loss: 0.311111... Val Loss: 0.356936\n",
      "Epoch: 1140/10000... Step: 57000... Loss: 0.278423... Val Loss: 0.354245\n",
      "Epoch: 1160/10000... Step: 58000... Loss: 0.322684... Val Loss: 0.359867\n",
      "Epoch: 1180/10000... Step: 59000... Loss: 0.267658... Val Loss: 0.349610\n",
      "Validation loss decreased (0.354183 --> 0.349610).  Saving model ...\n",
      "Epoch: 1220/10000... Step: 61000... Loss: 0.303137... Val Loss: 0.368063\n",
      "Epoch: 1240/10000... Step: 62000... Loss: 0.308068... Val Loss: 0.349458\n",
      "Validation loss decreased (0.349610 --> 0.349458).  Saving model ...\n",
      "Epoch: 1260/10000... Step: 63000... Loss: 0.290385... Val Loss: 0.349247\n",
      "Validation loss decreased (0.349458 --> 0.349247).  Saving model ...\n",
      "Epoch: 1280/10000... Step: 64000... Loss: 0.299261... Val Loss: 0.346861\n",
      "Validation loss decreased (0.349247 --> 0.346861).  Saving model ...\n",
      "Epoch: 1300/10000... Step: 65000... Loss: 0.298623... Val Loss: 0.344374\n",
      "Validation loss decreased (0.346861 --> 0.344374).  Saving model ...\n",
      "Epoch: 1320/10000... Step: 66000... Loss: 0.319245... Val Loss: 0.342301\n",
      "Validation loss decreased (0.344374 --> 0.342301).  Saving model ...\n",
      "Epoch: 1340/10000... Step: 67000... Loss: 0.287112... Val Loss: 0.345063\n",
      "Epoch: 1360/10000... Step: 68000... Loss: 0.301723... Val Loss: 0.345667\n",
      "Epoch: 1380/10000... Step: 69000... Loss: 0.312642... Val Loss: 0.350199\n",
      "Epoch: 1400/10000... Step: 70000... Loss: 0.308705... Val Loss: 0.346022\n",
      "Epoch: 1420/10000... Step: 71000... Loss: 0.292624... Val Loss: 0.345600\n",
      "Epoch  1421: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch: 1440/10000... Step: 72000... Loss: 0.262874... Val Loss: 0.339976\n",
      "Validation loss decreased (0.342301 --> 0.339976).  Saving model ...\n",
      "Epoch: 1460/10000... Step: 73000... Loss: 0.308120... Val Loss: 0.340719\n",
      "Epoch: 1480/10000... Step: 74000... Loss: 0.288612... Val Loss: 0.344892\n",
      "Epoch: 1500/10000... Step: 75000... Loss: 0.285598... Val Loss: 0.339899\n",
      "Validation loss decreased (0.339976 --> 0.339899).  Saving model ...\n",
      "Epoch: 1520/10000... Step: 76000... Loss: 0.277192... Val Loss: 0.339226\n",
      "Validation loss decreased (0.339899 --> 0.339226).  Saving model ...\n",
      "Epoch: 1540/10000... Step: 77000... Loss: 0.280897... Val Loss: 0.348073\n",
      "Epoch: 1560/10000... Step: 78000... Loss: 0.265210... Val Loss: 0.343849\n",
      "Epoch: 1580/10000... Step: 79000... Loss: 0.279594... Val Loss: 0.342821\n",
      "Epoch: 1600/10000... Step: 80000... Loss: 0.291438... Val Loss: 0.342945\n",
      "Epoch: 1620/10000... Step: 81000... Loss: 0.274226... Val Loss: 0.348057\n",
      "Epoch: 1640/10000... Step: 82000... Loss: 0.287142... Val Loss: 0.339465\n",
      "Epoch: 1660/10000... Step: 83000... Loss: 0.304095... Val Loss: 0.357254\n",
      "Epoch: 1680/10000... Step: 84000... Loss: 0.295064... Val Loss: 0.341653\n",
      "Epoch: 1700/10000... Step: 85000... Loss: 0.272414... Val Loss: 0.345013\n",
      "Epoch: 1720/10000... Step: 86000... Loss: 0.283581... Val Loss: 0.340395\n",
      "Epoch: 1740/10000... Step: 87000... Loss: 0.285419... Val Loss: 0.342111\n",
      "Epoch: 1760/10000... Step: 88000... Loss: 0.270365... Val Loss: 0.347354\n",
      "Epoch: 1780/10000... Step: 89000... Loss: 0.282214... Val Loss: 0.350013\n",
      "Epoch: 1800/10000... Step: 90000... Loss: 0.275097... Val Loss: 0.343206\n",
      "Epoch: 1820/10000... Step: 91000... Loss: 0.299222... Val Loss: 0.346570\n",
      "Epoch: 1840/10000... Step: 92000... Loss: 0.263724... Val Loss: 0.337690\n",
      "Validation loss decreased (0.339226 --> 0.337690).  Saving model ...\n",
      "Epoch: 1860/10000... Step: 93000... Loss: 0.262590... Val Loss: 0.348566\n",
      "Epoch: 1880/10000... Step: 94000... Loss: 0.268951... Val Loss: 0.344586\n",
      "Epoch: 1900/10000... Step: 95000... Loss: 0.288077... Val Loss: 0.344977\n",
      "Epoch: 1920/10000... Step: 96000... Loss: 0.276101... Val Loss: 0.342596\n",
      "Epoch  1922: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch: 1940/10000... Step: 97000... Loss: 0.247985... Val Loss: 0.343848\n",
      "Epoch: 1960/10000... Step: 98000... Loss: 0.268182... Val Loss: 0.338571\n",
      "Epoch: 1980/10000... Step: 99000... Loss: 0.284816... Val Loss: 0.337479\n",
      "Validation loss decreased (0.337690 --> 0.337479).  Saving model ...\n",
      "Epoch: 2000/10000... Step: 100000... Loss: 0.266459... Val Loss: 0.341238\n",
      "Epoch: 2020/10000... Step: 101000... Loss: 0.266713... Val Loss: 0.341457\n",
      "Epoch: 2040/10000... Step: 102000... Loss: 0.251180... Val Loss: 0.340969\n",
      "Epoch: 2060/10000... Step: 103000... Loss: 0.278999... Val Loss: 0.344877\n",
      "Epoch: 2080/10000... Step: 104000... Loss: 0.277450... Val Loss: 0.342410\n",
      "Epoch: 2100/10000... Step: 105000... Loss: 0.268496... Val Loss: 0.336942\n",
      "Validation loss decreased (0.337479 --> 0.336942).  Saving model ...\n",
      "Epoch: 2160/10000... Step: 108000... Loss: 0.291019... Val Loss: 0.348093\n",
      "Epoch: 2180/10000... Step: 109000... Loss: 0.279398... Val Loss: 0.341316\n",
      "Epoch: 2200/10000... Step: 110000... Loss: 0.285295... Val Loss: 0.347497\n",
      "Epoch: 2220/10000... Step: 111000... Loss: 0.269440... Val Loss: 0.334181\n",
      "Validation loss decreased (0.336942 --> 0.334181).  Saving model ...\n",
      "Epoch: 2240/10000... Step: 112000... Loss: 0.261155... Val Loss: 0.336024\n",
      "Epoch: 2260/10000... Step: 113000... Loss: 0.255096... Val Loss: 0.346352\n",
      "Epoch: 2280/10000... Step: 114000... Loss: 0.263665... Val Loss: 0.338872\n",
      "Epoch: 2300/10000... Step: 115000... Loss: 0.270986... Val Loss: 0.341011\n",
      "Epoch: 2320/10000... Step: 116000... Loss: 0.272162... Val Loss: 0.344136\n",
      "Epoch: 2340/10000... Step: 117000... Loss: 0.257712... Val Loss: 0.333235\n",
      "Validation loss decreased (0.334181 --> 0.333235).  Saving model ...\n",
      "Epoch: 2360/10000... Step: 118000... Loss: 0.245996... Val Loss: 0.337028\n",
      "Epoch: 2380/10000... Step: 119000... Loss: 0.264301... Val Loss: 0.342057\n",
      "Epoch: 2400/10000... Step: 120000... Loss: 0.237066... Val Loss: 0.334411\n",
      "Epoch: 2420/10000... Step: 121000... Loss: 0.223155... Val Loss: 0.343234\n",
      "Epoch: 2440/10000... Step: 122000... Loss: 0.235049... Val Loss: 0.338652\n",
      "Epoch: 2460/10000... Step: 123000... Loss: 0.284622... Val Loss: 0.340792\n",
      "Epoch: 2480/10000... Step: 124000... Loss: 0.268077... Val Loss: 0.344233\n",
      "Epoch: 2500/10000... Step: 125000... Loss: 0.229340... Val Loss: 0.337356\n",
      "Epoch: 2520/10000... Step: 126000... Loss: 0.248207... Val Loss: 0.335738\n",
      "Epoch: 2540/10000... Step: 127000... Loss: 0.286182... Val Loss: 0.337599\n",
      "Epoch: 2560/10000... Step: 128000... Loss: 0.277867... Val Loss: 0.341009\n",
      "Epoch  2561: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch: 2580/10000... Step: 129000... Loss: 0.256494... Val Loss: 0.336560\n",
      "Epoch: 2600/10000... Step: 130000... Loss: 0.252093... Val Loss: 0.337376\n",
      "Epoch: 2620/10000... Step: 131000... Loss: 0.243666... Val Loss: 0.340888\n",
      "Epoch: 2640/10000... Step: 132000... Loss: 0.245487... Val Loss: 0.341182\n",
      "Epoch: 2680/10000... Step: 134000... Loss: 0.234367... Val Loss: 0.337972\n",
      "Epoch: 2700/10000... Step: 135000... Loss: 0.282628... Val Loss: 0.337308\n",
      "Epoch: 2720/10000... Step: 136000... Loss: 0.244845... Val Loss: 0.333553\n",
      "Epoch: 2740/10000... Step: 137000... Loss: 0.247033... Val Loss: 0.337965\n",
      "Epoch: 2760/10000... Step: 138000... Loss: 0.264359... Val Loss: 0.339728\n",
      "Epoch: 2780/10000... Step: 139000... Loss: 0.268725... Val Loss: 0.339674\n",
      "Epoch: 2800/10000... Step: 140000... Loss: 0.231133... Val Loss: 0.336325\n",
      "Epoch: 2820/10000... Step: 141000... Loss: 0.251920... Val Loss: 0.340429\n",
      "Epoch: 2840/10000... Step: 142000... Loss: 0.270012... Val Loss: 0.335248\n",
      "Epoch: 2860/10000... Step: 143000... Loss: 0.269813... Val Loss: 0.336240\n",
      "Epoch: 2880/10000... Step: 144000... Loss: 0.251789... Val Loss: 0.341346\n",
      "Epoch: 2900/10000... Step: 145000... Loss: 0.247989... Val Loss: 0.342293\n",
      "Epoch: 2920/10000... Step: 146000... Loss: 0.276263... Val Loss: 0.339624\n",
      "Epoch: 2940/10000... Step: 147000... Loss: 0.235090... Val Loss: 0.340344\n",
      "Epoch: 2960/10000... Step: 148000... Loss: 0.270180... Val Loss: 0.339766\n",
      "Epoch: 2980/10000... Step: 149000... Loss: 0.243370... Val Loss: 0.334124\n",
      "Epoch: 3000/10000... Step: 150000... Loss: 0.262506... Val Loss: 0.352696\n",
      "Epoch: 3020/10000... Step: 151000... Loss: 0.247521... Val Loss: 0.342166\n",
      "Epoch: 3040/10000... Step: 152000... Loss: 0.252251... Val Loss: 0.340951\n",
      "Epoch: 3060/10000... Step: 153000... Loss: 0.259744... Val Loss: 0.340168\n",
      "Epoch: 3080/10000... Step: 154000... Loss: 0.261782... Val Loss: 0.331650\n",
      "Validation loss decreased (0.333235 --> 0.331650).  Saving model ...\n",
      "Epoch: 3100/10000... Step: 155000... Loss: 0.276885... Val Loss: 0.340578\n",
      "Epoch: 3120/10000... Step: 156000... Loss: 0.283609... Val Loss: 0.330791\n",
      "Validation loss decreased (0.331650 --> 0.330791).  Saving model ...\n",
      "Epoch: 3140/10000... Step: 157000... Loss: 0.247733... Val Loss: 0.343713\n",
      "Epoch: 3160/10000... Step: 158000... Loss: 0.253250... Val Loss: 0.337798\n",
      "Epoch: 3180/10000... Step: 159000... Loss: 0.233357... Val Loss: 0.333985\n",
      "Epoch: 3200/10000... Step: 160000... Loss: 0.255858... Val Loss: 0.335636\n",
      "Epoch: 3220/10000... Step: 161000... Loss: 0.246746... Val Loss: 0.340858\n",
      "Epoch: 3240/10000... Step: 162000... Loss: 0.254880... Val Loss: 0.333920\n",
      "Epoch: 3260/10000... Step: 163000... Loss: 0.239313... Val Loss: 0.333216\n",
      "Epoch: 3280/10000... Step: 164000... Loss: 0.252604... Val Loss: 0.338469\n",
      "Epoch: 3300/10000... Step: 165000... Loss: 0.260653... Val Loss: 0.342604\n",
      "Epoch: 3320/10000... Step: 166000... Loss: 0.257013... Val Loss: 0.341988\n",
      "Epoch: 3340/10000... Step: 167000... Loss: 0.261989... Val Loss: 0.334899\n",
      "Epoch: 3360/10000... Step: 168000... Loss: 0.227490... Val Loss: 0.343830\n",
      "Epoch: 3380/10000... Step: 169000... Loss: 0.271131... Val Loss: 0.344059\n",
      "Epoch: 3400/10000... Step: 170000... Loss: 0.261220... Val Loss: 0.339377\n",
      "Epoch: 3420/10000... Step: 171000... Loss: 0.251993... Val Loss: 0.339145\n",
      "Epoch: 3440/10000... Step: 172000... Loss: 0.262318... Val Loss: 0.340635\n",
      "Epoch: 3460/10000... Step: 173000... Loss: 0.236600... Val Loss: 0.336276\n",
      "Epoch: 3500/10000... Step: 175000... Loss: 0.234474... Val Loss: 0.347014\n",
      "Epoch: 3520/10000... Step: 176000... Loss: 0.271212... Val Loss: 0.334316\n",
      "Epoch: 3540/10000... Step: 177000... Loss: 0.238573... Val Loss: 0.343914\n",
      "Epoch: 3560/10000... Step: 178000... Loss: 0.260510... Val Loss: 0.336204\n",
      "Epoch: 3580/10000... Step: 179000... Loss: 0.258911... Val Loss: 0.345473\n",
      "Epoch: 3600/10000... Step: 180000... Loss: 0.256710... Val Loss: 0.334915\n",
      "Epoch: 3620/10000... Step: 181000... Loss: 0.272997... Val Loss: 0.334817\n",
      "Epoch: 3640/10000... Step: 182000... Loss: 0.253373... Val Loss: 0.331203\n",
      "Epoch: 3660/10000... Step: 183000... Loss: 0.241775... Val Loss: 0.337381\n",
      "Epoch: 3680/10000... Step: 184000... Loss: 0.224678... Val Loss: 0.341049\n",
      "Epoch: 3700/10000... Step: 185000... Loss: 0.243147... Val Loss: 0.333174\n",
      "Epoch: 3720/10000... Step: 186000... Loss: 0.271335... Val Loss: 0.336350\n",
      "Epoch: 3740/10000... Step: 187000... Loss: 0.262992... Val Loss: 0.336439\n",
      "Epoch: 3760/10000... Step: 188000... Loss: 0.233605... Val Loss: 0.341780\n",
      "Epoch: 3780/10000... Step: 189000... Loss: 0.255266... Val Loss: 0.340582\n",
      "Epoch  3781: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch: 3800/10000... Step: 190000... Loss: 0.241039... Val Loss: 0.338448\n",
      "Epoch: 3820/10000... Step: 191000... Loss: 0.260838... Val Loss: 0.342393\n",
      "Epoch: 3840/10000... Step: 192000... Loss: 0.263152... Val Loss: 0.336100\n",
      "Epoch: 3860/10000... Step: 193000... Loss: 0.271590... Val Loss: 0.337500\n",
      "Epoch: 3880/10000... Step: 194000... Loss: 0.252452... Val Loss: 0.331814\n",
      "Epoch: 3900/10000... Step: 195000... Loss: 0.278686... Val Loss: 0.336932\n",
      "Epoch: 3920/10000... Step: 196000... Loss: 0.250210... Val Loss: 0.334801\n",
      "Epoch: 3940/10000... Step: 197000... Loss: 0.240781... Val Loss: 0.333574\n",
      "Epoch: 3960/10000... Step: 198000... Loss: 0.275387... Val Loss: 0.335027\n",
      "Epoch: 3980/10000... Step: 199000... Loss: 0.241006... Val Loss: 0.338580\n",
      "Epoch: 4000/10000... Step: 200000... Loss: 0.270588... Val Loss: 0.341178\n",
      "Epoch: 4020/10000... Step: 201000... Loss: 0.263812... Val Loss: 0.335592\n",
      "Epoch: 4040/10000... Step: 202000... Loss: 0.263804... Val Loss: 0.336292\n",
      "Epoch: 4060/10000... Step: 203000... Loss: 0.269315... Val Loss: 0.335392\n",
      "Epoch: 4080/10000... Step: 204000... Loss: 0.243883... Val Loss: 0.339119\n",
      "Epoch: 4100/10000... Step: 205000... Loss: 0.232922... Val Loss: 0.338981\n",
      "Epoch: 4120/10000... Step: 206000... Loss: 0.245113... Val Loss: 0.337035\n",
      "Epoch: 4140/10000... Step: 207000... Loss: 0.257800... Val Loss: 0.336682\n",
      "Epoch: 4160/10000... Step: 208000... Loss: 0.253389... Val Loss: 0.336984\n",
      "Epoch: 4180/10000... Step: 209000... Loss: 0.248267... Val Loss: 0.336369\n",
      "Epoch: 4200/10000... Step: 210000... Loss: 0.256524... Val Loss: 0.340940\n",
      "Epoch: 4220/10000... Step: 211000... Loss: 0.240074... Val Loss: 0.336698\n",
      "Epoch: 4240/10000... Step: 212000... Loss: 0.248691... Val Loss: 0.334546\n",
      "Epoch: 4280/10000... Step: 214000... Loss: 0.230018... Val Loss: 0.338710\n",
      "Epoch  4282: reducing learning rate of group 0 to 3.1250e-06.\n",
      "Epoch: 4300/10000... Step: 215000... Loss: 0.245212... Val Loss: 0.335269\n",
      "Epoch: 4320/10000... Step: 216000... Loss: 0.238405... Val Loss: 0.338281\n",
      "Epoch: 4340/10000... Step: 217000... Loss: 0.243610... Val Loss: 0.335900\n",
      "Epoch: 4360/10000... Step: 218000... Loss: 0.246538... Val Loss: 0.338056\n",
      "Epoch: 4380/10000... Step: 219000... Loss: 0.267128... Val Loss: 0.340570\n",
      "Epoch: 4400/10000... Step: 220000... Loss: 0.256234... Val Loss: 0.341822\n",
      "Epoch: 4420/10000... Step: 221000... Loss: 0.239505... Val Loss: 0.336499\n",
      "Epoch: 4440/10000... Step: 222000... Loss: 0.255679... Val Loss: 0.337970\n",
      "Epoch: 4460/10000... Step: 223000... Loss: 0.226141... Val Loss: 0.337954\n",
      "Epoch: 4480/10000... Step: 224000... Loss: 0.247235... Val Loss: 0.337545\n",
      "Epoch: 4500/10000... Step: 225000... Loss: 0.243334... Val Loss: 0.340894\n",
      "Epoch: 4520/10000... Step: 226000... Loss: 0.240039... Val Loss: 0.338622\n",
      "Epoch: 4540/10000... Step: 227000... Loss: 0.244909... Val Loss: 0.343217\n",
      "Epoch: 4560/10000... Step: 228000... Loss: 0.270102... Val Loss: 0.339364\n",
      "Epoch: 4580/10000... Step: 229000... Loss: 0.234834... Val Loss: 0.334668\n",
      "Epoch: 4600/10000... Step: 230000... Loss: 0.250362... Val Loss: 0.337246\n",
      "Epoch: 4620/10000... Step: 231000... Loss: 0.255519... Val Loss: 0.336917\n",
      "Epoch: 4640/10000... Step: 232000... Loss: 0.240935... Val Loss: 0.343594\n",
      "Epoch: 4660/10000... Step: 233000... Loss: 0.220894... Val Loss: 0.335060\n",
      "Epoch: 4680/10000... Step: 234000... Loss: 0.250268... Val Loss: 0.341825\n",
      "Epoch: 4700/10000... Step: 235000... Loss: 0.235936... Val Loss: 0.345654\n",
      "Epoch: 4720/10000... Step: 236000... Loss: 0.238015... Val Loss: 0.337482\n",
      "Epoch: 4740/10000... Step: 237000... Loss: 0.263804... Val Loss: 0.344673\n",
      "Epoch: 4760/10000... Step: 238000... Loss: 0.265392... Val Loss: 0.339849\n",
      "Epoch: 4780/10000... Step: 239000... Loss: 0.248155... Val Loss: 0.333388\n",
      "Epoch  4783: reducing learning rate of group 0 to 1.5625e-06.\n",
      "Epoch: 4800/10000... Step: 240000... Loss: 0.239828... Val Loss: 0.336616\n",
      "Epoch: 4820/10000... Step: 241000... Loss: 0.238612... Val Loss: 0.335898\n",
      "Epoch: 4840/10000... Step: 242000... Loss: 0.250757... Val Loss: 0.337740\n",
      "Epoch: 4860/10000... Step: 243000... Loss: 0.218359... Val Loss: 0.337862\n",
      "Epoch: 4880/10000... Step: 244000... Loss: 0.243570... Val Loss: 0.333688\n",
      "Epoch: 4900/10000... Step: 245000... Loss: 0.247454... Val Loss: 0.333689\n",
      "Epoch: 4920/10000... Step: 246000... Loss: 0.262718... Val Loss: 0.338274\n",
      "Epoch: 4940/10000... Step: 247000... Loss: 0.265675... Val Loss: 0.337574\n",
      "Epoch: 4960/10000... Step: 248000... Loss: 0.215997... Val Loss: 0.343386\n",
      "Epoch: 4980/10000... Step: 249000... Loss: 0.249942... Val Loss: 0.333196\n",
      "Epoch: 5000/10000... Step: 250000... Loss: 0.266573... Val Loss: 0.337249\n",
      "Epoch: 5020/10000... Step: 251000... Loss: 0.253487... Val Loss: 0.334519\n",
      "Epoch: 5040/10000... Step: 252000... Loss: 0.242802... Val Loss: 0.334966\n",
      "Epoch: 5060/10000... Step: 253000... Loss: 0.226277... Val Loss: 0.337695\n",
      "Epoch: 5080/10000... Step: 254000... Loss: 0.251087... Val Loss: 0.331816\n",
      "Epoch: 5100/10000... Step: 255000... Loss: 0.239325... Val Loss: 0.335289\n",
      "Epoch: 5120/10000... Step: 256000... Loss: 0.254708... Val Loss: 0.334042\n",
      "Epoch: 5140/10000... Step: 257000... Loss: 0.242725... Val Loss: 0.341812\n",
      "Epoch: 5160/10000... Step: 258000... Loss: 0.254557... Val Loss: 0.340046\n",
      "Epoch: 5180/10000... Step: 259000... Loss: 0.253700... Val Loss: 0.334474\n",
      "Epoch: 5200/10000... Step: 260000... Loss: 0.249304... Val Loss: 0.341553\n",
      "Epoch: 5220/10000... Step: 261000... Loss: 0.246160... Val Loss: 0.335306\n",
      "Epoch: 5240/10000... Step: 262000... Loss: 0.239547... Val Loss: 0.338519\n",
      "Epoch: 5260/10000... Step: 263000... Loss: 0.270837... Val Loss: 0.339807\n",
      "Epoch: 5280/10000... Step: 264000... Loss: 0.244807... Val Loss: 0.331659\n",
      "Epoch  5284: reducing learning rate of group 0 to 7.8125e-07.\n",
      "Epoch: 5300/10000... Step: 265000... Loss: 0.249092... Val Loss: 0.340125\n",
      "Epoch: 5320/10000... Step: 266000... Loss: 0.249860... Val Loss: 0.332977\n",
      "Epoch: 5340/10000... Step: 267000... Loss: 0.242510... Val Loss: 0.335815\n",
      "Epoch: 5360/10000... Step: 268000... Loss: 0.208723... Val Loss: 0.340330\n",
      "Epoch: 5380/10000... Step: 269000... Loss: 0.229279... Val Loss: 0.339182\n",
      "Epoch: 5400/10000... Step: 270000... Loss: 0.277865... Val Loss: 0.335580\n",
      "Epoch: 5420/10000... Step: 271000... Loss: 0.238861... Val Loss: 0.339082\n",
      "Epoch: 5440/10000... Step: 272000... Loss: 0.264931... Val Loss: 0.335632\n",
      "Epoch: 5460/10000... Step: 273000... Loss: 0.240897... Val Loss: 0.338208\n",
      "Epoch: 5480/10000... Step: 274000... Loss: 0.238999... Val Loss: 0.337001\n",
      "Epoch: 5500/10000... Step: 275000... Loss: 0.250999... Val Loss: 0.344759\n",
      "Epoch: 5520/10000... Step: 276000... Loss: 0.264774... Val Loss: 0.344339\n",
      "Epoch: 5540/10000... Step: 277000... Loss: 0.252981... Val Loss: 0.339321\n",
      "Epoch: 5560/10000... Step: 278000... Loss: 0.242273... Val Loss: 0.336273\n",
      "Epoch: 5580/10000... Step: 279000... Loss: 0.249766... Val Loss: 0.336909\n",
      "Epoch: 5600/10000... Step: 280000... Loss: 0.249380... Val Loss: 0.338082\n",
      "Epoch: 5620/10000... Step: 281000... Loss: 0.248475... Val Loss: 0.340987\n",
      "Epoch: 5660/10000... Step: 283000... Loss: 0.249527... Val Loss: 0.336381\n",
      "Epoch: 5680/10000... Step: 284000... Loss: 0.250471... Val Loss: 0.341091\n",
      "Epoch: 5700/10000... Step: 285000... Loss: 0.224443... Val Loss: 0.341955\n",
      "Epoch: 5720/10000... Step: 286000... Loss: 0.254445... Val Loss: 0.337932\n",
      "Epoch: 5740/10000... Step: 287000... Loss: 0.245975... Val Loss: 0.334367\n",
      "Epoch: 5760/10000... Step: 288000... Loss: 0.230178... Val Loss: 0.340290\n",
      "Epoch: 5780/10000... Step: 289000... Loss: 0.284260... Val Loss: 0.330082\n",
      "Validation loss decreased (0.330791 --> 0.330082).  Saving model ...\n",
      "Epoch  5785: reducing learning rate of group 0 to 3.9063e-07.\n",
      "Epoch: 5800/10000... Step: 290000... Loss: 0.263725... Val Loss: 0.335466\n",
      "Epoch: 5820/10000... Step: 291000... Loss: 0.241534... Val Loss: 0.343174\n",
      "Epoch: 5840/10000... Step: 292000... Loss: 0.259902... Val Loss: 0.334737\n",
      "Epoch: 5860/10000... Step: 293000... Loss: 0.242231... Val Loss: 0.337023\n",
      "Epoch: 5880/10000... Step: 294000... Loss: 0.231872... Val Loss: 0.338930\n",
      "Epoch: 5900/10000... Step: 295000... Loss: 0.228718... Val Loss: 0.338255\n",
      "Epoch: 5920/10000... Step: 296000... Loss: 0.251634... Val Loss: 0.337962\n",
      "Epoch: 5940/10000... Step: 297000... Loss: 0.258949... Val Loss: 0.337419\n",
      "Epoch: 5960/10000... Step: 298000... Loss: 0.255917... Val Loss: 0.340010\n",
      "Epoch: 5980/10000... Step: 299000... Loss: 0.257802... Val Loss: 0.335392\n",
      "Epoch: 6000/10000... Step: 300000... Loss: 0.222228... Val Loss: 0.339367\n",
      "Epoch: 6020/10000... Step: 301000... Loss: 0.266546... Val Loss: 0.343917\n",
      "Epoch: 6040/10000... Step: 302000... Loss: 0.250940... Val Loss: 0.341848\n",
      "Epoch: 6060/10000... Step: 303000... Loss: 0.224555... Val Loss: 0.344236\n",
      "Epoch: 6080/10000... Step: 304000... Loss: 0.253099... Val Loss: 0.334651\n",
      "Epoch: 6100/10000... Step: 305000... Loss: 0.244213... Val Loss: 0.338671\n",
      "Epoch: 6120/10000... Step: 306000... Loss: 0.233404... Val Loss: 0.342952\n",
      "Epoch: 6140/10000... Step: 307000... Loss: 0.250777... Val Loss: 0.338630\n",
      "Epoch: 6160/10000... Step: 308000... Loss: 0.238342... Val Loss: 0.336358\n",
      "Epoch: 6180/10000... Step: 309000... Loss: 0.238216... Val Loss: 0.342204\n",
      "Epoch: 6200/10000... Step: 310000... Loss: 0.275995... Val Loss: 0.339310\n",
      "Epoch: 6220/10000... Step: 311000... Loss: 0.270371... Val Loss: 0.331875\n",
      "Epoch: 6240/10000... Step: 312000... Loss: 0.235877... Val Loss: 0.333001\n",
      "Epoch: 6260/10000... Step: 313000... Loss: 0.269526... Val Loss: 0.341114\n",
      "Epoch: 6280/10000... Step: 314000... Loss: 0.257857... Val Loss: 0.341231\n",
      "Epoch  6286: reducing learning rate of group 0 to 1.9531e-07.\n",
      "Epoch: 6300/10000... Step: 315000... Loss: 0.230049... Val Loss: 0.335815\n",
      "Epoch: 6320/10000... Step: 316000... Loss: 0.278395... Val Loss: 0.339784\n",
      "Epoch: 6340/10000... Step: 317000... Loss: 0.243096... Val Loss: 0.338335\n",
      "Epoch: 6360/10000... Step: 318000... Loss: 0.251703... Val Loss: 0.340942\n",
      "Epoch: 6380/10000... Step: 319000... Loss: 0.259616... Val Loss: 0.343105\n",
      "Epoch: 6400/10000... Step: 320000... Loss: 0.213736... Val Loss: 0.340148\n",
      "Epoch: 6420/10000... Step: 321000... Loss: 0.254134... Val Loss: 0.330907\n",
      "Epoch: 6440/10000... Step: 322000... Loss: 0.219393... Val Loss: 0.345578\n",
      "Epoch: 6460/10000... Step: 323000... Loss: 0.231786... Val Loss: 0.339625\n",
      "Epoch: 6480/10000... Step: 324000... Loss: 0.251788... Val Loss: 0.334880\n",
      "Epoch: 6500/10000... Step: 325000... Loss: 0.267247... Val Loss: 0.340314\n",
      "Epoch: 6520/10000... Step: 326000... Loss: 0.256544... Val Loss: 0.337130\n",
      "Epoch: 6540/10000... Step: 327000... Loss: 0.235189... Val Loss: 0.341878\n",
      "Epoch: 6560/10000... Step: 328000... Loss: 0.258484... Val Loss: 0.342627\n",
      "Epoch: 6580/10000... Step: 329000... Loss: 0.234849... Val Loss: 0.340999\n",
      "Epoch: 6600/10000... Step: 330000... Loss: 0.250312... Val Loss: 0.336113\n",
      "Epoch: 6620/10000... Step: 331000... Loss: 0.247886... Val Loss: 0.332941\n",
      "Epoch: 6640/10000... Step: 332000... Loss: 0.232680... Val Loss: 0.340100\n",
      "Epoch: 6660/10000... Step: 333000... Loss: 0.253325... Val Loss: 0.345097\n",
      "Epoch: 6680/10000... Step: 334000... Loss: 0.248917... Val Loss: 0.342988\n",
      "Epoch: 6700/10000... Step: 335000... Loss: 0.212677... Val Loss: 0.328296\n",
      "Validation loss decreased (0.330082 --> 0.328296).  Saving model ...\n",
      "Epoch: 6720/10000... Step: 336000... Loss: 0.234158... Val Loss: 0.342656\n",
      "Epoch: 6740/10000... Step: 337000... Loss: 0.238409... Val Loss: 0.339948\n",
      "Epoch: 6760/10000... Step: 338000... Loss: 0.235810... Val Loss: 0.338752\n",
      "Epoch: 6780/10000... Step: 339000... Loss: 0.233443... Val Loss: 0.333095\n",
      "Epoch  6787: reducing learning rate of group 0 to 9.7656e-08.\n",
      "Epoch: 6800/10000... Step: 340000... Loss: 0.243056... Val Loss: 0.333394\n",
      "Epoch: 6820/10000... Step: 341000... Loss: 0.250280... Val Loss: 0.340604\n",
      "Epoch: 6840/10000... Step: 342000... Loss: 0.230788... Val Loss: 0.342995\n",
      "Epoch: 6860/10000... Step: 343000... Loss: 0.235551... Val Loss: 0.337698\n",
      "Epoch: 6880/10000... Step: 344000... Loss: 0.237707... Val Loss: 0.341980\n",
      "Epoch: 6900/10000... Step: 345000... Loss: 0.236668... Val Loss: 0.342058\n",
      "Epoch: 6920/10000... Step: 346000... Loss: 0.257760... Val Loss: 0.336387\n",
      "Epoch: 6940/10000... Step: 347000... Loss: 0.243224... Val Loss: 0.336785\n",
      "Epoch: 6960/10000... Step: 348000... Loss: 0.261407... Val Loss: 0.334743\n",
      "Epoch: 6980/10000... Step: 349000... Loss: 0.244577... Val Loss: 0.339843\n",
      "Epoch: 7000/10000... Step: 350000... Loss: 0.258788... Val Loss: 0.337143\n",
      "Epoch: 7020/10000... Step: 351000... Loss: 0.244380... Val Loss: 0.342936\n",
      "Epoch: 7040/10000... Step: 352000... Loss: 0.249948... Val Loss: 0.335408\n",
      "Epoch: 7060/10000... Step: 353000... Loss: 0.272867... Val Loss: 0.342484\n",
      "Epoch: 7080/10000... Step: 354000... Loss: 0.235612... Val Loss: 0.338529\n",
      "Epoch: 7100/10000... Step: 355000... Loss: 0.245527... Val Loss: 0.341050\n",
      "Epoch: 7120/10000... Step: 356000... Loss: 0.230111... Val Loss: 0.342629\n",
      "Epoch: 7140/10000... Step: 357000... Loss: 0.296480... Val Loss: 0.337763\n",
      "Epoch: 7160/10000... Step: 358000... Loss: 0.226874... Val Loss: 0.342799\n",
      "Epoch: 7180/10000... Step: 359000... Loss: 0.275278... Val Loss: 0.345429\n",
      "Epoch: 7200/10000... Step: 360000... Loss: 0.262343... Val Loss: 0.335138\n",
      "Epoch: 7220/10000... Step: 361000... Loss: 0.241302... Val Loss: 0.345624\n",
      "Epoch: 7240/10000... Step: 362000... Loss: 0.250321... Val Loss: 0.339796\n",
      "Epoch: 7260/10000... Step: 363000... Loss: 0.258949... Val Loss: 0.340956\n",
      "Epoch: 7280/10000... Step: 364000... Loss: 0.256228... Val Loss: 0.335035\n",
      "Epoch  7288: reducing learning rate of group 0 to 4.8828e-08.\n",
      "Epoch: 7300/10000... Step: 365000... Loss: 0.274156... Val Loss: 0.334190\n",
      "Epoch: 7320/10000... Step: 366000... Loss: 0.263669... Val Loss: 0.339238\n",
      "Epoch: 7340/10000... Step: 367000... Loss: 0.230993... Val Loss: 0.337644\n",
      "Epoch: 7360/10000... Step: 368000... Loss: 0.251785... Val Loss: 0.339357\n",
      "Epoch: 7380/10000... Step: 369000... Loss: 0.217519... Val Loss: 0.338266\n",
      "Epoch: 7400/10000... Step: 370000... Loss: 0.238754... Val Loss: 0.343181\n",
      "Epoch: 7420/10000... Step: 371000... Loss: 0.258928... Val Loss: 0.335047\n",
      "Epoch: 7440/10000... Step: 372000... Loss: 0.233912... Val Loss: 0.340575\n",
      "Epoch: 7460/10000... Step: 373000... Loss: 0.273148... Val Loss: 0.343341\n",
      "Epoch: 7480/10000... Step: 374000... Loss: 0.241484... Val Loss: 0.341279\n",
      "Epoch: 7500/10000... Step: 375000... Loss: 0.251584... Val Loss: 0.336977\n",
      "Epoch: 7520/10000... Step: 376000... Loss: 0.249728... Val Loss: 0.341833\n",
      "Epoch: 7540/10000... Step: 377000... Loss: 0.242184... Val Loss: 0.341488\n",
      "Epoch: 7560/10000... Step: 378000... Loss: 0.251264... Val Loss: 0.339951\n",
      "Epoch: 7580/10000... Step: 379000... Loss: 0.233350... Val Loss: 0.337970\n",
      "Epoch: 7600/10000... Step: 380000... Loss: 0.261025... Val Loss: 0.332325\n",
      "Epoch: 7620/10000... Step: 381000... Loss: 0.229944... Val Loss: 0.335994\n",
      "Epoch: 7640/10000... Step: 382000... Loss: 0.238420... Val Loss: 0.337295\n",
      "Epoch: 7660/10000... Step: 383000... Loss: 0.248425... Val Loss: 0.341676\n",
      "Epoch: 7680/10000... Step: 384000... Loss: 0.255644... Val Loss: 0.340233\n",
      "Epoch: 7700/10000... Step: 385000... Loss: 0.242276... Val Loss: 0.342605\n",
      "Epoch: 7720/10000... Step: 386000... Loss: 0.229915... Val Loss: 0.338294\n",
      "Epoch: 7740/10000... Step: 387000... Loss: 0.237093... Val Loss: 0.335138\n",
      "Epoch: 7760/10000... Step: 388000... Loss: 0.228017... Val Loss: 0.338641\n",
      "Epoch: 7780/10000... Step: 389000... Loss: 0.243395... Val Loss: 0.337856\n",
      "Epoch  7789: reducing learning rate of group 0 to 2.4414e-08.\n",
      "Epoch: 7800/10000... Step: 390000... Loss: 0.236605... Val Loss: 0.340557\n",
      "Epoch: 7820/10000... Step: 391000... Loss: 0.248792... Val Loss: 0.341690\n",
      "Epoch: 7840/10000... Step: 392000... Loss: 0.228838... Val Loss: 0.332558\n",
      "Epoch: 7860/10000... Step: 393000... Loss: 0.273360... Val Loss: 0.340848\n",
      "Epoch: 7880/10000... Step: 394000... Loss: 0.248333... Val Loss: 0.334667\n",
      "Epoch: 7900/10000... Step: 395000... Loss: 0.265884... Val Loss: 0.343590\n",
      "Epoch: 7920/10000... Step: 396000... Loss: 0.263768... Val Loss: 0.337350\n",
      "Epoch: 7940/10000... Step: 397000... Loss: 0.221773... Val Loss: 0.335725\n",
      "Epoch: 7960/10000... Step: 398000... Loss: 0.251348... Val Loss: 0.336606\n",
      "Epoch: 7980/10000... Step: 399000... Loss: 0.254137... Val Loss: 0.339061\n",
      "Epoch: 8000/10000... Step: 400000... Loss: 0.239857... Val Loss: 0.339848\n",
      "Epoch: 8020/10000... Step: 401000... Loss: 0.246843... Val Loss: 0.339699\n",
      "Epoch: 8040/10000... Step: 402000... Loss: 0.267568... Val Loss: 0.344425\n",
      "Epoch: 8060/10000... Step: 403000... Loss: 0.239797... Val Loss: 0.338343\n",
      "Epoch: 8080/10000... Step: 404000... Loss: 0.242729... Val Loss: 0.334018\n",
      "Epoch: 8100/10000... Step: 405000... Loss: 0.250107... Val Loss: 0.339669\n",
      "Epoch: 8120/10000... Step: 406000... Loss: 0.257139... Val Loss: 0.337410\n",
      "Epoch: 8140/10000... Step: 407000... Loss: 0.252553... Val Loss: 0.334677\n",
      "Epoch: 8160/10000... Step: 408000... Loss: 0.255587... Val Loss: 0.339525\n",
      "Epoch: 8180/10000... Step: 409000... Loss: 0.231957... Val Loss: 0.342608\n",
      "Epoch: 8200/10000... Step: 410000... Loss: 0.259347... Val Loss: 0.343193\n",
      "Epoch: 8220/10000... Step: 411000... Loss: 0.269328... Val Loss: 0.339492\n",
      "Epoch: 8240/10000... Step: 412000... Loss: 0.246705... Val Loss: 0.331127\n",
      "Epoch: 8260/10000... Step: 413000... Loss: 0.216289... Val Loss: 0.336807\n",
      "Epoch: 8280/10000... Step: 414000... Loss: 0.229895... Val Loss: 0.336636\n",
      "Epoch  8290: reducing learning rate of group 0 to 1.2207e-08.\n",
      "Epoch: 8300/10000... Step: 415000... Loss: 0.259575... Val Loss: 0.339588\n",
      "Epoch: 8320/10000... Step: 416000... Loss: 0.232114... Val Loss: 0.338656\n",
      "Epoch: 8340/10000... Step: 417000... Loss: 0.234675... Val Loss: 0.340276\n",
      "Epoch: 8360/10000... Step: 418000... Loss: 0.254214... Val Loss: 0.335170\n",
      "Epoch: 8380/10000... Step: 419000... Loss: 0.245551... Val Loss: 0.343922\n",
      "Epoch: 8400/10000... Step: 420000... Loss: 0.246275... Val Loss: 0.333385\n",
      "Epoch: 8440/10000... Step: 422000... Loss: 0.271443... Val Loss: 0.337681\n",
      "Epoch: 8460/10000... Step: 423000... Loss: 0.255468... Val Loss: 0.339272\n",
      "Epoch: 8480/10000... Step: 424000... Loss: 0.251545... Val Loss: 0.336662\n",
      "Epoch: 8500/10000... Step: 425000... Loss: 0.236799... Val Loss: 0.340411\n",
      "Epoch: 8520/10000... Step: 426000... Loss: 0.252043... Val Loss: 0.338164\n",
      "Epoch: 8540/10000... Step: 427000... Loss: 0.267260... Val Loss: 0.341335\n",
      "Epoch: 8560/10000... Step: 428000... Loss: 0.239717... Val Loss: 0.338839\n",
      "Epoch: 8580/10000... Step: 429000... Loss: 0.239232... Val Loss: 0.339511\n",
      "Epoch: 8600/10000... Step: 430000... Loss: 0.239308... Val Loss: 0.339734\n",
      "Epoch: 8620/10000... Step: 431000... Loss: 0.250739... Val Loss: 0.340203\n",
      "Epoch: 8640/10000... Step: 432000... Loss: 0.264194... Val Loss: 0.339758\n",
      "Epoch: 8660/10000... Step: 433000... Loss: 0.288060... Val Loss: 0.343949\n",
      "Epoch: 8680/10000... Step: 434000... Loss: 0.230677... Val Loss: 0.340108\n",
      "Epoch: 8700/10000... Step: 435000... Loss: 0.251585... Val Loss: 0.335562\n",
      "Epoch: 8720/10000... Step: 436000... Loss: 0.261392... Val Loss: 0.339914\n",
      "Epoch: 8740/10000... Step: 437000... Loss: 0.255049... Val Loss: 0.334577\n",
      "Epoch: 8760/10000... Step: 438000... Loss: 0.239552... Val Loss: 0.344169\n",
      "Epoch: 8780/10000... Step: 439000... Loss: 0.238306... Val Loss: 0.342529\n",
      "Epoch: 8800/10000... Step: 440000... Loss: 0.247612... Val Loss: 0.343360\n",
      "Epoch: 8820/10000... Step: 441000... Loss: 0.234903... Val Loss: 0.337874\n",
      "Epoch: 8840/10000... Step: 442000... Loss: 0.249837... Val Loss: 0.335886\n",
      "Epoch: 8860/10000... Step: 443000... Loss: 0.256707... Val Loss: 0.340846\n",
      "Epoch: 8880/10000... Step: 444000... Loss: 0.236424... Val Loss: 0.339005\n",
      "Epoch: 8900/10000... Step: 445000... Loss: 0.230288... Val Loss: 0.339514\n",
      "Epoch: 8920/10000... Step: 446000... Loss: 0.231469... Val Loss: 0.339935\n",
      "Epoch: 8940/10000... Step: 447000... Loss: 0.241283... Val Loss: 0.331706\n",
      "Epoch: 8960/10000... Step: 448000... Loss: 0.265257... Val Loss: 0.336082\n",
      "Epoch: 8980/10000... Step: 449000... Loss: 0.244137... Val Loss: 0.334290\n",
      "Epoch: 9000/10000... Step: 450000... Loss: 0.262266... Val Loss: 0.337283\n",
      "Epoch: 9020/10000... Step: 451000... Loss: 0.241338... Val Loss: 0.343120\n",
      "Epoch: 9040/10000... Step: 452000... Loss: 0.257076... Val Loss: 0.338022\n",
      "Epoch: 9060/10000... Step: 453000... Loss: 0.256951... Val Loss: 0.340290\n",
      "Epoch: 9080/10000... Step: 454000... Loss: 0.213974... Val Loss: 0.340099\n",
      "Epoch: 9100/10000... Step: 455000... Loss: 0.242196... Val Loss: 0.338997\n",
      "Epoch: 9120/10000... Step: 456000... Loss: 0.254713... Val Loss: 0.335911\n",
      "Epoch: 9140/10000... Step: 457000... Loss: 0.250957... Val Loss: 0.333508\n",
      "Epoch: 9160/10000... Step: 458000... Loss: 0.251100... Val Loss: 0.338355\n",
      "Epoch: 9180/10000... Step: 459000... Loss: 0.239847... Val Loss: 0.336568\n",
      "Epoch: 9200/10000... Step: 460000... Loss: 0.227367... Val Loss: 0.332979\n",
      "Epoch: 9220/10000... Step: 461000... Loss: 0.245460... Val Loss: 0.341647\n",
      "Epoch: 9240/10000... Step: 462000... Loss: 0.250755... Val Loss: 0.339055\n",
      "Epoch: 9260/10000... Step: 463000... Loss: 0.262211... Val Loss: 0.334393\n",
      "Epoch: 9280/10000... Step: 464000... Loss: 0.234960... Val Loss: 0.337983\n",
      "Epoch: 9300/10000... Step: 465000... Loss: 0.213851... Val Loss: 0.338925\n",
      "Epoch: 9320/10000... Step: 466000... Loss: 0.224079... Val Loss: 0.335329\n",
      "Epoch: 9340/10000... Step: 467000... Loss: 0.248460... Val Loss: 0.334035\n",
      "Epoch: 9360/10000... Step: 468000... Loss: 0.246998... Val Loss: 0.337349\n",
      "Epoch: 9380/10000... Step: 469000... Loss: 0.259460... Val Loss: 0.339686\n",
      "Epoch: 9400/10000... Step: 470000... Loss: 0.237049... Val Loss: 0.337935\n",
      "Epoch: 9420/10000... Step: 471000... Loss: 0.249328... Val Loss: 0.340841\n",
      "Epoch: 9440/10000... Step: 472000... Loss: 0.238025... Val Loss: 0.336897\n",
      "Epoch: 9460/10000... Step: 473000... Loss: 0.252063... Val Loss: 0.342105\n",
      "Epoch: 9480/10000... Step: 474000... Loss: 0.225902... Val Loss: 0.338912\n",
      "Epoch: 9500/10000... Step: 475000... Loss: 0.254438... Val Loss: 0.340137\n",
      "Epoch: 9520/10000... Step: 476000... Loss: 0.243922... Val Loss: 0.337442\n",
      "Epoch: 9540/10000... Step: 477000... Loss: 0.258780... Val Loss: 0.335758\n",
      "Epoch: 9560/10000... Step: 478000... Loss: 0.268960... Val Loss: 0.333950\n",
      "Epoch: 9580/10000... Step: 479000... Loss: 0.266903... Val Loss: 0.340911\n",
      "Epoch: 9600/10000... Step: 480000... Loss: 0.270907... Val Loss: 0.339922\n",
      "Epoch: 9620/10000... Step: 481000... Loss: 0.240891... Val Loss: 0.339099\n",
      "Epoch: 9640/10000... Step: 482000... Loss: 0.247124... Val Loss: 0.338483\n",
      "Epoch: 9660/10000... Step: 483000... Loss: 0.225028... Val Loss: 0.338119\n",
      "Epoch: 9680/10000... Step: 484000... Loss: 0.249650... Val Loss: 0.340741\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c253c1ccbfeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2076\u001b[0m     return torch._C._nn.binary_cross_entropy(\n\u001b[0;32m-> 2077\u001b[0;31m         input, target, weight, reduction_enum)\n\u001b[0m\u001b[1;32m   2078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "counter = 0\n",
    "print_every = 1000\n",
    "clip = .5\n",
    "valid_loss_min = np.Inf\n",
    "val_loss = torch.tensor(np.Inf)\n",
    "model.train()\n",
    "history = pd.DataFrame()\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(train_batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = train_criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            val_h = model.init_hidden(val_batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in val_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp, val_h)\n",
    "                val_loss = val_criterion(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            history.loc[i + 1, 'train_loss'] = loss.data.cpu().numpy()\n",
    "            history.loc[i + 1, 'dev_loss'] = val_loss.cpu().detach().numpy()\n",
    "            \n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './model_1face.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)\n",
    "    scheduler.step(val_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f320202ac50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(history.plot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
